{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m2nnljFefVXn"
      ],
      "mount_file_id": "1iJWrRidKrMSn5-o_qi6IHjV95y5V1Ax8",
      "authorship_tag": "ABX9TyPvXEdV50WwY8bwVUMTbefd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-2/notebooks/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Installation"
      ],
      "metadata": {
        "id": "3pSpAemJtoV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mddkJ2qFtWul",
        "outputId": "aa38b8f8-7ac6-4f40-c08e-96170c41abc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=90e29d51db783940b0dcea4e552cd6ff0790040b250bad76bc148b79e9571736\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1 pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark ipython-autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "java --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eykSj3wuIF_",
        "outputId": "26f3bd62-c865-4a91-8338-d5275f95223b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.21 2023-10-17\n",
            "OpenJDK Runtime Environment (build 11.0.21+9-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.21+9-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "JE3wgJzgt8bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zo-ThnUt7QG",
        "outputId": "935bbf73-50c6-4d8e-b02b-46175773bef9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 276 µs (started: 2024-02-25 08:15:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DSCI553/hw2\")\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "y4trPSXvueXZ",
        "outputId": "797dd6e5-9a83-44c4-dcaf-4ea2399b0854"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/DSCI553/hw2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 172 ms (started: 2024-02-25 08:15:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dREq8V4Fuq7F",
        "outputId": "56804282-791b-4691-f9d4-739d5dd09150"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment_2.ipynb  small2.csv\t      t1-s2-c1.txt\tt1-s2-c2.txt\t\t       task1_ref.py\n",
            "small1.csv\t    t1-s2-c1-ref.txt  t1-s2-c2-ref.txt\tta_feng_all_months_merged.csv\n",
            "time: 306 ms (started: 2024-02-25 08:15:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "FNIJpYFWuSmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_files(file1_path, file2_path):\n",
        "    # Read the contents of the first file\n",
        "    with open(file1_path, 'r') as file1:\n",
        "        file1_contents = file1.read().strip()\n",
        "\n",
        "    # Read the contents of the second file\n",
        "    with open(file2_path, 'r') as file2:\n",
        "        file2_contents = file2.read().strip()\n",
        "\n",
        "    # Compare the contents of the files\n",
        "    if file1_contents == file2_contents:\n",
        "        print(\"The contents of the files are the same.\")\n",
        "    else:\n",
        "        print(\"The contents of the files are different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-cRQA61jr_z",
        "outputId": "bb10415e-b362-48b2-bc85-0d2cb8e42d6b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 878 µs (started: 2024-02-25 08:53:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1"
      ],
      "metadata": {
        "id": "xxNkrnCHuPKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from collections import Counter\n",
        "from itertools import combinations, chain\n",
        "\n",
        "\n",
        "def generate_baskets(data, case):\n",
        "    \"\"\"\n",
        "    Generate baskets based on the specified case.\n",
        "    \"\"\"\n",
        "    if case == 1:\n",
        "        # Group Business\n",
        "        baskets = data.map(lambda line: line.strip().split(\",\")).map(lambda x: (x[0], [x[1]]))\n",
        "    elif case == 2:\n",
        "        # Group User\n",
        "        baskets = data.map(lambda line: line.strip().split(\",\")).map(lambda x: (x[1], [x[0]]))\n",
        "    return baskets.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "\n",
        "def hash_function(item1, item2, num_buckets):\n",
        "    \"\"\"\n",
        "    Simple hash function to distribute items into different buckets.\n",
        "    \"\"\"\n",
        "    hash_val = int(item1) ^ int(item2)\n",
        "    return hash_val % num_buckets\n",
        "\n",
        "\n",
        "def PCY(baskets, total_num_baskets, support, num_buckets):\n",
        "    \"\"\"\n",
        "    Run the PCY algorithm to find candidates\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    baskets = list(baskets)\n",
        "\n",
        "    # Calculate the support ratio based on the number of baskets in the current partition and the total number of baskets\n",
        "    support_ratio = len(baskets) / total_num_baskets\n",
        "\n",
        "    # Calculate the partition support threshold by multiplying the support ratio with the desired support\n",
        "    partition_support_threshold = support_ratio * support\n",
        "\n",
        "    # First pass:\n",
        "    # Initialize counters to keep track of item counts, and hashes\n",
        "    item_counts = Counter()\n",
        "    hash_counts = Counter()\n",
        "\n",
        "    # Initialize a bitmap list with zeros, representing whether each hash bucket meets the support threshold\n",
        "    bitmap = [0] * num_buckets\n",
        "\n",
        "    for basket in baskets:\n",
        "        # Count item pairs\n",
        "        item_counts.update(basket)\n",
        "\n",
        "        pairs = list(combinations(basket, 2))\n",
        "\n",
        "        # Hash pairs and count the number of occurence every hash\n",
        "        hashes = [hash_function(item[0], item[1], num_buckets) for item in pairs]\n",
        "        hash_counts.update(hashes)\n",
        "\n",
        "    # Update the bitmap if count exceeds support threshold\n",
        "    for h, count in hash_counts.items():\n",
        "        if count >= partition_support_threshold:\n",
        "            bitmap[h] = 1\n",
        "\n",
        "    # Second Pass:\n",
        "    frequent_singles = []\n",
        "    for item, count in item_counts.items():\n",
        "        if count >= partition_support_threshold:\n",
        "            frequent_singles.append(item)\n",
        "            candidates.append((tuple([item]), 1))\n",
        "\n",
        "    frequent_pairs = []\n",
        "    for item in combinations(frequent_singles, 2):\n",
        "        hash_val = hash_function(item[0], item[1], num_buckets)\n",
        "        if bitmap[hash_val] == 1:\n",
        "            frequent_pairs.append(item)\n",
        "\n",
        "    if len(frequent_pairs) == 0:\n",
        "        return []\n",
        "\n",
        "    # Filter items from basket that are not frequent\n",
        "    baskets = [[item for item in basket if item in frequent_singles] for basket in baskets]\n",
        "\n",
        "    # Initialize candidate dictionary\n",
        "    candidate_dict = {pair: 0 for pair in frequent_pairs}\n",
        "\n",
        "    # Count occurrences of frequent pairs in baskets\n",
        "    for basket in baskets:\n",
        "        for pair in frequent_pairs:\n",
        "            if set(pair).issubset(basket):\n",
        "                candidate_dict[pair] += 1\n",
        "\n",
        "    # Filter the pair that do not meet the support threshold\n",
        "    candidate_dict = {\n",
        "        pair: count for pair, count in candidate_dict.items() if count >= partition_support_threshold\n",
        "    }\n",
        "\n",
        "    # Append new candidates to the candidates list\n",
        "    candidates += [(pair, 1) for pair in candidate_dict.keys()]\n",
        "\n",
        "    # Valid Candidates\n",
        "    val_candidates = candidate_dict.keys()\n",
        "\n",
        "    # Check for frequent pairs in larger sets\n",
        "    k = 3\n",
        "    while len(val_candidates) > 0:\n",
        "        unq_singles = set(chain.from_iterable(val_candidates))\n",
        "        itemsets = list(combinations(unq_singles, k))\n",
        "\n",
        "        # Initialize candidate dictionary\n",
        "        candidate_dict = {itemset: 0 for itemset in itemsets}\n",
        "\n",
        "        # Count occurrences of frequent pairs in baskets\n",
        "        for basket in baskets:\n",
        "            for itemset in itemsets:\n",
        "                # Check if the candidate itemset is a subset of the current basket\n",
        "                if set(itemset).issubset(basket):\n",
        "                    candidate_dict[itemset] += 1\n",
        "\n",
        "        # Filter the pair that do not meet the support threshold\n",
        "        candidate_dict = {\n",
        "            itemset: count for itemset, count in candidate_dict.items() if count >= partition_support_threshold\n",
        "        }\n",
        "\n",
        "        # Append new candidates to the candidates list\n",
        "        candidates += [(itemset, 1) for itemset in candidate_dict.keys()]\n",
        "\n",
        "        # Update Valid Candidates\n",
        "        val_candidates = candidate_dict.keys()\n",
        "\n",
        "        # Update Order of larger itemset for next iteration\n",
        "        k += 1\n",
        "\n",
        "    return candidates\n",
        "\n",
        "\n",
        "def check_itemsets_in_basket(baskets, candidates):\n",
        "    \"\"\"\n",
        "    Check if candidate itemsets are frequent in the given baskets.\n",
        "\n",
        "    Parameters:\n",
        "        baskets (iterable): An iterable containing baskets of items.\n",
        "        candidates (iterable): An iterable containing candidate itemsets.\n",
        "\n",
        "    Returns:\n",
        "        frequent_itemsets (list): A list of frequent itemsets found in the baskets.\n",
        "    \"\"\"\n",
        "    candidates = list(candidates)\n",
        "    frequent_itemsets = []\n",
        "\n",
        "    for basket in baskets:\n",
        "        for itemset in candidates:\n",
        "            # Check if the candidate itemset is a subset of the current basket\n",
        "            if set(itemset).issubset(basket):\n",
        "                frequent_itemsets.append(itemset)\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def SON(baskets, support, num_buckets=1000):\n",
        "    \"\"\"\n",
        "    Perform SON algorithm to find frequent itemsets.\n",
        "\n",
        "    Args:\n",
        "        baskets (RDD): RDD containing baskets of items.\n",
        "        support (int): Minimum support threshold.\n",
        "        num_buckets (int, optional): Number of buckets for the PCY algorithm. Defaults to 1000.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two dictionaries:\n",
        "            - candidates_map: A dictionary mapping length of itemsets to the list of frequent candidates.\n",
        "            - frequent_itemsets: A dictionary mapping length of frequent itemsets to the list of frequent itemsets.\n",
        "    \"\"\"\n",
        "    # Count the number of baskets\n",
        "    num_baskets = baskets.count()\n",
        "\n",
        "    # Extract individual items from baskets\n",
        "    items = baskets.values()\n",
        "\n",
        "    # Stage 1: Finding Frequent Itemsets using PCY\n",
        "    candidates = (\n",
        "        items.mapPartitions(lambda chunk: PCY(chunk, num_baskets, support, num_buckets))\n",
        "        .reduceByKey(lambda x, y: x + y)\n",
        "        .map(lambda x: tuple(sorted(x[0])))\n",
        "        .distinct()\n",
        "        .sortBy(lambda x: (len(x), x))\n",
        "    )\n",
        "\n",
        "    # Group candidates by length and collect as a map\n",
        "    candidates_map = (\n",
        "        candidates\n",
        "        .groupBy(lambda x: len(x))\n",
        "        .mapValues(list)  # Convert grouped values to a list\n",
        "        .sortByKey()\n",
        "        .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # Retrieve frequent candidates as a list\n",
        "    candidates = candidates.collect()\n",
        "\n",
        "    # Stage 2: Candidate Pruning\n",
        "    frequent_itemsets = (\n",
        "        items.mapPartitions(lambda chunk: check_itemsets_in_basket(chunk, candidates))\n",
        "        .map(lambda itemset: (itemset, 1))\n",
        "        .reduceByKey(lambda x, y: x + y)\n",
        "        .filter(lambda x: x[1] >= support)\n",
        "        .map(lambda x: tuple(sorted(x[0])))\n",
        "        .sortBy(lambda x: (len(x), x))\n",
        "        .groupBy(lambda x: len(x))\n",
        "        .mapValues(list)  # Convert grouped values to a list\n",
        "        .sortByKey()\n",
        "        .collectAsMap()\n",
        "    )\n",
        "    return candidates_map, frequent_itemsets\n",
        "\n",
        "\n",
        "def task1(case_number, support, input_file_path, output_file_path):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 1\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        # Read the input data\n",
        "        data = spark.textFile(input_file_path)\n",
        "\n",
        "        # Drop the header row\n",
        "        data = data.filter(lambda row: row != 'user_id,business_id')\n",
        "\n",
        "        # Generate baskets based on the specified case\n",
        "        baskets = generate_baskets(data, case_number).cache()\n",
        "\n",
        "        # Run the SON Algorithm\n",
        "        candidates, frequent_itemsets = SON(baskets, support, num_buckets=1000)\n",
        "\n",
        "        # Output the results\n",
        "        with open(output_file_path, \"w\") as f:\n",
        "            f.write(f\"Candidates:\\n\")\n",
        "            for _, itemsets in candidates.items():\n",
        "                itemsets = \",\".join(map(str, itemsets)).replace(\",)\", \")\")\n",
        "                f.write(f\"{itemsets}\\n\\n\")\n",
        "            f.write(\"Frequent Itemsets:\\n\")\n",
        "            for line, itemsets in frequent_itemsets.items():\n",
        "                itemsets = \",\".join(map(str, itemsets)).replace(\",)\", \")\")\n",
        "                f.write(f\"{itemsets}\")\n",
        "                if line < len(frequent_itemsets):\n",
        "                    f.write(\"\\n\\n\")\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if len(sys.argv) != 5:\n",
        "#         print(\n",
        "#             \"Usage: spark-submit --executor-memory 4G --driver-memory 4G \"\n",
        "#             \"task1.py <case_number> <support> <input_file_path> <output_file_path>\"\n",
        "#         )\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Read input parameters\n",
        "#     case_number = int(sys.argv[1])\n",
        "#     support = int(sys.argv[2])\n",
        "#     input_file_path = sys.argv[3]\n",
        "#     output_file_path = sys.argv[4]\n",
        "\n",
        "#     task1(case_number, support, input_file_path, output_file_path)\n",
        "\n",
        "\n",
        "task1(2, 9, \"small2.csv\", \"t1-s2-c2.txt\")\n",
        "task1(1, 4, \"small2.csv\", \"t1-s2-c1.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBabaHjksK9w",
        "outputId": "b8840040-eb0a-4bad-bf48-3dfcfd8da17e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 22.17356586456299\n",
            "\n",
            "Duration: 33.39101719856262\n",
            "\n",
            "time: 57.7 s (started: 2024-02-25 10:54:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ref"
      ],
      "metadata": {
        "id": "m2nnljFefVXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Reference Code Task 1 { vertical-output: true, form-width: \"50%\" }\n",
        "%%writefile task1_ref.py\n",
        "from pyspark import SparkContext\n",
        "from itertools import combinations as comb\n",
        "import time\n",
        "import sys\n",
        "import copy\n",
        "\n",
        "def hash(x, y, n_bucket):\n",
        "  return (x ^ y) % n_bucket\n",
        "\n",
        "def group_bus(lines):\n",
        "    return lines.map(lambda row: (row[0], row[1])).groupByKey().mapValues(set).map(lambda row: (row[0], list(row[1])))\n",
        "\n",
        "def group_user(lines):\n",
        "    return lines.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set).map(lambda row: (row[0], list(row[1])))\n",
        "\n",
        "def PCY(baskets, total_n, support, n_bucket):\n",
        "    baskets = list(baskets)\n",
        "    item_cnt = {}\n",
        "    hash_cnt = {}\n",
        "    bitmap=[0 for x in range(0, n_bucket)]\n",
        "    candidates = []\n",
        "    p = len(baskets) / total_n\n",
        "    t = p * support\n",
        "\n",
        "    #PCY pass1\n",
        "    for basket in baskets:\n",
        "        for item in basket:\n",
        "            if item not in item_cnt.keys():\n",
        "                item_cnt[item] = 1\n",
        "            else:\n",
        "                item_cnt[item] += 1\n",
        "        for i in range(0, len(basket) - 1):\n",
        "            for j in range(i + 1, len(basket)):\n",
        "                idx = hash(int(basket[i]), int(basket[j]), n_bucket)\n",
        "                if idx not in hash_cnt.keys():\n",
        "                    hash_cnt[idx] = 1\n",
        "                else:\n",
        "                    hash_cnt[idx] += 1\n",
        "        for key, value in hash_cnt.items():\n",
        "            if value >= t:\n",
        "                bitmap[key] = 1\n",
        "\n",
        "    #PCY pass2\n",
        "    single_freq = []\n",
        "    for key, value in item_cnt.items():\n",
        "        if value >= t:\n",
        "            single_freq.append(key)\n",
        "            candidates.append((tuple([key]), 1))\n",
        "    #single_freq = sorted(single_freq)\n",
        "    #print(candidates)\n",
        "    pair_freq = []\n",
        "    for i in range(0, len(single_freq) - 1):\n",
        "        for j in range(i + 1, len(single_freq)):\n",
        "            pair_freq.append((single_freq[i], single_freq[j]))\n",
        "    for pair in pair_freq:\n",
        "        if bitmap[hash(int(pair[0]), int(pair[1]), n_bucket)] != 1:\n",
        "            pair_freq.remove(pair)\n",
        "\n",
        "    #print(candidates)\n",
        "    if len(pair_freq) == 0:\n",
        "        return []\n",
        "    else:\n",
        "        baskets_temp = []\n",
        "        for basket in baskets:\n",
        "            basket_1 = copy.deepcopy(basket)\n",
        "            for item in basket:\n",
        "                if item not in single_freq:\n",
        "                    basket_1.remove(item)\n",
        "            baskets_temp.append(basket_1)\n",
        "        baskets = baskets_temp\n",
        "        #pair_freq = sorted(pair_freq)\n",
        "        #print(pair_freq)\n",
        "        cnt_dict = dict(zip(pair_freq, [0] * len(pair_freq)))\n",
        "        for basket in baskets:\n",
        "            for key in cnt_dict.keys():\n",
        "                if set(list(key)) <= set(basket):\n",
        "                    cnt_dict[key] += 1\n",
        "        for key, value in cnt_dict.copy().items():\n",
        "            if value < t:\n",
        "                del cnt_dict[key]\n",
        "            else:\n",
        "                candidates.append((tuple(key), 1))\n",
        "        candidate = cnt_dict.keys()\n",
        "        #print(candidate)\n",
        "        k = 3\n",
        "        while(True):\n",
        "            #candidate = sorted(candidate)\n",
        "            temp = list(comb(set(a for b in candidate for a in b), k))\n",
        "            cnt_dict = dict(zip(temp, [0] * len(temp)))\n",
        "            for basket in baskets:\n",
        "                for key in cnt_dict.keys():\n",
        "                    #print(set(list(key)), set(basket))\n",
        "                    if set(list(key)) <= set(basket):\n",
        "                        cnt_dict[key] += 1\n",
        "            #print(cnt_dict, t)\n",
        "            for key, value in cnt_dict.copy().items():\n",
        "                if value < t:\n",
        "                    del cnt_dict[key]\n",
        "                else:\n",
        "                    candidates.append((tuple(key), 1))\n",
        "            candidate = cnt_dict.keys()\n",
        "            if len(candidate) > 0:\n",
        "                k += 1\n",
        "            else:\n",
        "                break\n",
        "        #print(candidates)\n",
        "        return candidates\n",
        "\n",
        "def SON(baskets, candidates):\n",
        "    baskets = list(baskets)\n",
        "    result = {}\n",
        "    for basket in baskets:\n",
        "        for items in candidates:\n",
        "            flag = 0\n",
        "            for item in items:\n",
        "                if item not in basket:\n",
        "                    flag = 1\n",
        "                    break\n",
        "            if flag == 0:\n",
        "                idx = tuple(items)\n",
        "                if idx not in result.keys():\n",
        "                    result[idx] = 1\n",
        "                else:\n",
        "                    result[idx] += 1\n",
        "    return result.items()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    case_n = sys.argv[1]\n",
        "    support = sys.argv[2]\n",
        "    input_path = sys.argv[3]\n",
        "    output_path = sys.argv[4]\n",
        "\n",
        "    n_bucket = 1000\n",
        "    spark = SparkContext(appName= \"task1\")\n",
        "    t_s = time.time()\n",
        "    #spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
        "    #lines = spark.sparkContext.textFile(input_path)\n",
        "    lines = spark.textFile(input_path)\n",
        "    #print(lines.collect())\n",
        "    lines = lines.filter(lambda row: row != 'user_id,business_id').map(lambda row: row.split(','))\n",
        "\n",
        "    #Case1 or 2\n",
        "    if int(case_n) == 1:\n",
        "        baskets = group_bus(lines)\n",
        "    elif int(case_n) == 2:\n",
        "        baskets = group_user(lines)\n",
        "\n",
        "    #SON algorithm stage #1 -> PCY\n",
        "    #n_partitions = 5\n",
        "    total_n = baskets.count()\n",
        "    #baskets = baskets.partitionBy(n_partitions)\n",
        "    p = baskets.getNumPartitions()\n",
        "    items = baskets.values()\n",
        "    a = items.collect()\n",
        "    candidates = items.mapPartitions(lambda baskets: PCY(baskets, total_n, float(support), n_bucket)).reduceByKey(lambda x, y: x + y).collect()\n",
        "    #print(candidates)\n",
        "    result_temp = []\n",
        "    for i in range(0, len(candidates)):\n",
        "        result_temp.append(sorted(candidates[i][0]))\n",
        "    result_temp = sorted(result_temp)\n",
        "    result_temp = sorted(result_temp, key = lambda x: len(x))\n",
        "    last = result_temp[-1]\n",
        "    for i in range(len(result_temp) - 2, -1, -1):\n",
        "        if last == result_temp[i]:\n",
        "            del result_temp[i]\n",
        "        else:\n",
        "            last = result_temp[i]\n",
        "    #print(result_temp)\n",
        "    #Output candidates\n",
        "    result = 'Candidates:\\n'\n",
        "    l = 1\n",
        "    for i in range(0, len(result_temp)):\n",
        "        if len(result_temp[i]) == 1:\n",
        "            result = result + '(' + str(result_temp[i])[1:-1] + '),'\n",
        "        else:\n",
        "            if len(result_temp[i]) != l:\n",
        "                result = result[:-1] + \"\\n\\n\" + '(' + str(result_temp[i])[1:-1] + \"),\"\n",
        "                l = len(result_temp[i])\n",
        "            else:\n",
        "                result = result + '(' + str(result_temp[i])[1:-1] + \"),\"\n",
        "    result = result[:-1] + '\\n\\n'\n",
        "    #print(result)\n",
        "\n",
        "    #SON algorithm stage #2\n",
        "    freq_items = items.mapPartitions(lambda baskets: SON(baskets, result_temp)).reduceByKey(lambda x, y: x + y).filter(lambda x : x[1] >= int(support)).collect()\n",
        "    result_temp = []\n",
        "    for i in range(0, len(freq_items)):\n",
        "        result_temp.append(sorted(freq_items[i][0]))\n",
        "    result_temp = sorted(result_temp)\n",
        "    result_temp = sorted(result_temp, key = lambda x: len(x))\n",
        "    last = result_temp[-1]\n",
        "    for i in range(len(result_temp) - 2, -1, -1):\n",
        "        if last == result_temp[i]:\n",
        "            del result_temp[i]\n",
        "        else:\n",
        "            last = result_temp[i]\n",
        "    #print(result_temp)\n",
        "    result += 'Frequent Itemsets:\\n'\n",
        "    l = 1\n",
        "    for i in range(0, len(result_temp)):\n",
        "        if len(result_temp[i]) == 1:\n",
        "            result = result + '(' + str(result_temp[i])[1:-1] + '),'\n",
        "        else:\n",
        "            if len(result_temp[i]) != l:\n",
        "                result = result[:-1] + \"\\n\\n\" + '(' + str(result_temp[i])[1:-1] + \"),\"\n",
        "                l = len(result_temp[i])\n",
        "            else:\n",
        "                result = result + '(' + str(result_temp[i])[1:-1] + \"),\"\n",
        "    result = result[:-1]\n",
        "\n",
        "    with open(output_path, 'w+') as f:\n",
        "        f.write(result)\n",
        "\n",
        "    t_e = time.time()\n",
        "\n",
        "    print(\"Duration: \", t_e - t_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "cO51qhxleKpa",
        "outputId": "9c07d546-818c-410a-8c68-b1c053b59d2a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1_ref.py\n",
            "time: 16.9 ms (started: 2024-02-25 04:28:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit task1_ref.py 2 9 small2.csv t1-s2-c2-ref.txt --executor-memory 4G --driver-memory 4G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-BgO3xJeUTY",
        "outputId": "4af8a1a8-c0af-49af-d3b9-123f841a838b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/02/25 04:36:04 INFO SparkContext: Running Spark version 3.5.0\n",
            "24/02/25 04:36:04 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "24/02/25 04:36:04 INFO SparkContext: Java version 11.0.21\n",
            "24/02/25 04:36:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/02/25 04:36:05 INFO ResourceUtils: ==============================================================\n",
            "24/02/25 04:36:05 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/02/25 04:36:05 INFO ResourceUtils: ==============================================================\n",
            "24/02/25 04:36:05 INFO SparkContext: Submitted application: task1\n",
            "24/02/25 04:36:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/02/25 04:36:05 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/02/25 04:36:05 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/02/25 04:36:05 INFO SecurityManager: Changing view acls to: root\n",
            "24/02/25 04:36:05 INFO SecurityManager: Changing modify acls to: root\n",
            "24/02/25 04:36:05 INFO SecurityManager: Changing view acls groups to: \n",
            "24/02/25 04:36:05 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/02/25 04:36:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/02/25 04:36:06 INFO Utils: Successfully started service 'sparkDriver' on port 44387.\n",
            "24/02/25 04:36:06 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/02/25 04:36:06 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/02/25 04:36:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/02/25 04:36:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/02/25 04:36:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/02/25 04:36:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ba3e851f-16e8-4084-a954-6d82870ceb33\n",
            "24/02/25 04:36:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/02/25 04:36:06 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/02/25 04:36:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/02/25 04:36:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/02/25 04:36:07 INFO Executor: Starting executor ID driver on host c2d0b6a37d90\n",
            "24/02/25 04:36:07 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "24/02/25 04:36:07 INFO Executor: Java version 11.0.21\n",
            "24/02/25 04:36:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/02/25 04:36:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@13a5759d for default.\n",
            "24/02/25 04:36:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42599.\n",
            "24/02/25 04:36:07 INFO NettyBlockTransferService: Server created on c2d0b6a37d90:42599\n",
            "24/02/25 04:36:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/02/25 04:36:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c2d0b6a37d90, 42599, None)\n",
            "24/02/25 04:36:07 INFO BlockManagerMasterEndpoint: Registering block manager c2d0b6a37d90:42599 with 434.4 MiB RAM, BlockManagerId(driver, c2d0b6a37d90, 42599, None)\n",
            "24/02/25 04:36:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c2d0b6a37d90, 42599, None)\n",
            "24/02/25 04:36:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c2d0b6a37d90, 42599, None)\n",
            "24/02/25 04:36:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
            "24/02/25 04:36:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
            "24/02/25 04:36:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c2d0b6a37d90:42599 (size: 32.6 KiB, free: 434.4 MiB)\n",
            "24/02/25 04:36:10 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/02/25 04:36:11 INFO FileInputFormat: Total input files to process : 1\n",
            "24/02/25 04:36:11 INFO SparkContext: Starting job: count at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:150\n",
            "24/02/25 04:36:11 INFO DAGScheduler: Registering RDD 3 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:14) as input to shuffle 0\n",
            "24/02/25 04:36:12 INFO DAGScheduler: Got job 0 (count at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:150) with 2 output partitions\n",
            "24/02/25 04:36:12 INFO DAGScheduler: Final stage: ResultStage 1 (count at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:150)\n",
            "24/02/25 04:36:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "24/02/25 04:36:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "24/02/25 04:36:12 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:14), which has no missing parents\n",
            "24/02/25 04:36:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.4 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c2d0b6a37d90:42599 (size: 7.9 KiB, free: 434.4 MiB)\n",
            "24/02/25 04:36:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
            "24/02/25 04:36:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:14) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/02/25 04:36:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "24/02/25 04:36:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c2d0b6a37d90, executor driver, partition 0, PROCESS_LOCAL, 7686 bytes) \n",
            "24/02/25 04:36:12 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (c2d0b6a37d90, executor driver, partition 1, PROCESS_LOCAL, 7686 bytes) \n",
            "24/02/25 04:36:12 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/02/25 04:36:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/02/25 04:36:13 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/small2.csv:1731+1731\n",
            "24/02/25 04:36:13 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/small2.csv:0+1731\n",
            "24/02/25 04:36:15 INFO PythonRunner: Times: total = 1286, boot = 946, init = 338, finish = 2\n",
            "24/02/25 04:36:15 INFO PythonRunner: Times: total = 1257, boot = 940, init = 312, finish = 5\n",
            "24/02/25 04:36:15 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1667 bytes result sent to driver\n",
            "24/02/25 04:36:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1667 bytes result sent to driver\n",
            "24/02/25 04:36:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2512 ms on c2d0b6a37d90 (executor driver) (1/2)\n",
            "24/02/25 04:36:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2593 ms on c2d0b6a37d90 (executor driver) (2/2)\n",
            "24/02/25 04:36:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/02/25 04:36:15 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50973\n",
            "24/02/25 04:36:15 INFO DAGScheduler: ShuffleMapStage 0 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:14) finished in 3.132 s\n",
            "24/02/25 04:36:15 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/02/25 04:36:15 INFO DAGScheduler: running: Set()\n",
            "24/02/25 04:36:15 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "24/02/25 04:36:15 INFO DAGScheduler: failed: Set()\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at count at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:150), which has no missing parents\n",
            "24/02/25 04:36:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.9 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c2d0b6a37d90:42599 (size: 7.1 KiB, free: 434.4 MiB)\n",
            "24/02/25 04:36:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at count at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:150) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/02/25 04:36:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "24/02/25 04:36:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (c2d0b6a37d90, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:15 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (c2d0b6a37d90, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "24/02/25 04:36:15 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Getting 2 (935.0 B) non-empty blocks including 2 (935.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Getting 2 (980.0 B) non-empty blocks including 2 (980.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
            "24/02/25 04:36:15 INFO PythonRunner: Times: total = 253, boot = -813, init = 1066, finish = 0\n",
            "24/02/25 04:36:15 INFO PythonRunner: Times: total = 257, boot = -829, init = 1085, finish = 1\n",
            "24/02/25 04:36:15 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2097 bytes result sent to driver\n",
            "24/02/25 04:36:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2097 bytes result sent to driver\n",
            "24/02/25 04:36:15 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 428 ms on c2d0b6a37d90 (executor driver) (1/2)\n",
            "24/02/25 04:36:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 447 ms on c2d0b6a37d90 (executor driver) (2/2)\n",
            "24/02/25 04:36:15 INFO DAGScheduler: ResultStage 1 (count at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:150) finished in 0.478 s\n",
            "24/02/25 04:36:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/02/25 04:36:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/02/25 04:36:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Job 0 finished: count at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:150, took 4.037635 s\n",
            "24/02/25 04:36:15 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:154\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Got job 1 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:154) with 2 output partitions\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:154)\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Missing parents: List()\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:154), which has no missing parents\n",
            "24/02/25 04:36:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.9 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c2d0b6a37d90:42599 (size: 6.7 KiB, free: 434.3 MiB)\n",
            "24/02/25 04:36:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
            "24/02/25 04:36:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:154) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/02/25 04:36:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
            "24/02/25 04:36:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (c2d0b6a37d90, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:15 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (c2d0b6a37d90, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)\n",
            "24/02/25 04:36:15 INFO Executor: Running task 1.0 in stage 3.0 (TID 5)\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Getting 2 (980.0 B) non-empty blocks including 2 (980.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Getting 2 (935.0 B) non-empty blocks including 2 (935.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "24/02/25 04:36:16 INFO PythonRunner: Times: total = 215, boot = -147, init = 361, finish = 1\n",
            "24/02/25 04:36:16 INFO PythonRunner: Times: total = 218, boot = -132, init = 350, finish = 0\n",
            "24/02/25 04:36:16 INFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 3251 bytes result sent to driver\n",
            "24/02/25 04:36:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 3357 bytes result sent to driver\n",
            "24/02/25 04:36:16 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 279 ms on c2d0b6a37d90 (executor driver) (1/2)\n",
            "24/02/25 04:36:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 281 ms on c2d0b6a37d90 (executor driver) (2/2)\n",
            "24/02/25 04:36:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "24/02/25 04:36:16 INFO DAGScheduler: ResultStage 3 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:154) finished in 0.302 s\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/02/25 04:36:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Job 1 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:154, took 0.315617 s\n",
            "24/02/25 04:36:16 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Registering RDD 9 (reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155) as input to shuffle 1\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Got job 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155) with 2 output partitions\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155)\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155), which has no missing parents\n",
            "24/02/25 04:36:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.4 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c2d0b6a37d90:42599 (size: 9.4 KiB, free: 434.3 MiB)\n",
            "24/02/25 04:36:16 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
            "24/02/25 04:36:16 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/02/25 04:36:16 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
            "24/02/25 04:36:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (c2d0b6a37d90, executor driver, partition 0, NODE_LOCAL, 7422 bytes) \n",
            "24/02/25 04:36:16 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (c2d0b6a37d90, executor driver, partition 1, NODE_LOCAL, 7422 bytes) \n",
            "24/02/25 04:36:16 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)\n",
            "24/02/25 04:36:16 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)\n",
            "24/02/25 04:36:16 INFO ShuffleBlockFetcherIterator: Getting 2 (980.0 B) non-empty blocks including 2 (980.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:16 INFO ShuffleBlockFetcherIterator: Getting 2 (935.0 B) non-empty blocks including 2 (935.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/02/25 04:36:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "24/02/25 04:36:16 INFO PythonRunner: Times: total = 389, boot = -105, init = 427, finish = 67\n",
            "24/02/25 04:36:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2269 bytes result sent to driver\n",
            "24/02/25 04:36:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 477 ms on c2d0b6a37d90 (executor driver) (1/2)\n",
            "24/02/25 04:36:50 INFO PythonRunner: Times: total = 33721, boot = -116, init = 410, finish = 33427\n",
            "24/02/25 04:36:50 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2269 bytes result sent to driver\n",
            "24/02/25 04:36:50 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 33801 ms on c2d0b6a37d90 (executor driver) (2/2)\n",
            "24/02/25 04:36:50 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "24/02/25 04:36:50 INFO DAGScheduler: ShuffleMapStage 5 (reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155) finished in 33.825 s\n",
            "24/02/25 04:36:50 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/02/25 04:36:50 INFO DAGScheduler: running: Set()\n",
            "24/02/25 04:36:50 INFO DAGScheduler: waiting: Set(ResultStage 6)\n",
            "24/02/25 04:36:50 INFO DAGScheduler: failed: Set()\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155), which has no missing parents\n",
            "24/02/25 04:36:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.8 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
            "24/02/25 04:36:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c2d0b6a37d90:42599 (size: 5.9 KiB, free: 434.3 MiB)\n",
            "24/02/25 04:36:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/02/25 04:36:50 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
            "24/02/25 04:36:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (c2d0b6a37d90, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:50 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (c2d0b6a37d90, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:50 INFO Executor: Running task 1.0 in stage 6.0 (TID 9)\n",
            "24/02/25 04:36:50 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Getting 2 (11.6 KiB) non-empty blocks including 2 (11.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Getting 2 (12.6 KiB) non-empty blocks including 2 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "24/02/25 04:36:50 INFO PythonRunner: Times: total = 222, boot = -79, init = 294, finish = 7\n",
            "24/02/25 04:36:50 INFO PythonRunner: Times: total = 240, boot = -33351, init = 33585, finish = 6\n",
            "24/02/25 04:36:50 INFO Executor: Finished task 1.0 in stage 6.0 (TID 9). 51761 bytes result sent to driver\n",
            "24/02/25 04:36:50 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 51121 bytes result sent to driver\n",
            "24/02/25 04:36:50 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 286 ms on c2d0b6a37d90 (executor driver) (1/2)\n",
            "24/02/25 04:36:50 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 287 ms on c2d0b6a37d90 (executor driver) (2/2)\n",
            "24/02/25 04:36:50 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "24/02/25 04:36:50 INFO DAGScheduler: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155) finished in 0.306 s\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/02/25 04:36:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Job 2 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:155, took 34.150134 s\n",
            "24/02/25 04:36:50 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Registering RDD 14 (reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185) as input to shuffle 2\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Got job 3 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185) with 2 output partitions\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Final stage: ResultStage 9 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185)\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Submitting ShuffleMapStage 8 (PairwiseRDD[14] at reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185), which has no missing parents\n",
            "24/02/25 04:36:50 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 119.3 KiB, free 433.9 MiB)\n",
            "24/02/25 04:36:50 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 41.5 KiB, free 433.9 MiB)\n",
            "24/02/25 04:36:50 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c2d0b6a37d90:42599 (size: 41.5 KiB, free: 434.3 MiB)\n",
            "24/02/25 04:36:50 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
            "24/02/25 04:36:50 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (PairwiseRDD[14] at reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/02/25 04:36:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0\n",
            "24/02/25 04:36:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10) (c2d0b6a37d90, executor driver, partition 0, NODE_LOCAL, 7422 bytes) \n",
            "24/02/25 04:36:50 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 11) (c2d0b6a37d90, executor driver, partition 1, NODE_LOCAL, 7422 bytes) \n",
            "24/02/25 04:36:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)\n",
            "24/02/25 04:36:50 INFO Executor: Running task 1.0 in stage 8.0 (TID 11)\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Getting 2 (980.0 B) non-empty blocks including 2 (980.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Getting 2 (935.0 B) non-empty blocks including 2 (935.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "24/02/25 04:36:51 INFO PythonRunner: Times: total = 570, boot = -163, init = 481, finish = 252\n",
            "24/02/25 04:36:51 INFO Executor: Finished task 1.0 in stage 8.0 (TID 11). 2269 bytes result sent to driver\n",
            "24/02/25 04:36:51 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 11) in 638 ms on c2d0b6a37d90 (executor driver) (1/2)\n",
            "24/02/25 04:36:51 INFO PythonRunner: Times: total = 608, boot = -196, init = 629, finish = 175\n",
            "24/02/25 04:36:51 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 2269 bytes result sent to driver\n",
            "24/02/25 04:36:51 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 701 ms on c2d0b6a37d90 (executor driver) (2/2)\n",
            "24/02/25 04:36:51 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "24/02/25 04:36:51 INFO DAGScheduler: ShuffleMapStage 8 (reduceByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185) finished in 0.733 s\n",
            "24/02/25 04:36:51 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/02/25 04:36:51 INFO DAGScheduler: running: Set()\n",
            "24/02/25 04:36:51 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
            "24/02/25 04:36:51 INFO DAGScheduler: failed: Set()\n",
            "24/02/25 04:36:51 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[17] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185), which has no missing parents\n",
            "24/02/25 04:36:51 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.5 KiB, free 433.9 MiB)\n",
            "24/02/25 04:36:51 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 433.9 MiB)\n",
            "24/02/25 04:36:51 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on c2d0b6a37d90:42599 (size: 6.2 KiB, free: 434.3 MiB)\n",
            "24/02/25 04:36:51 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
            "24/02/25 04:36:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (PythonRDD[17] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/02/25 04:36:51 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
            "24/02/25 04:36:51 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 12) (c2d0b6a37d90, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:51 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 13) (c2d0b6a37d90, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/02/25 04:36:51 INFO Executor: Running task 0.0 in stage 9.0 (TID 12)\n",
            "24/02/25 04:36:51 INFO Executor: Running task 1.0 in stage 9.0 (TID 13)\n",
            "24/02/25 04:36:51 INFO ShuffleBlockFetcherIterator: Getting 2 (21.7 KiB) non-empty blocks including 2 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "24/02/25 04:36:51 INFO ShuffleBlockFetcherIterator: Getting 2 (21.7 KiB) non-empty blocks including 2 (21.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/02/25 04:36:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
            "24/02/25 04:36:51 INFO PythonRunner: Times: total = 278, boot = -117, init = 385, finish = 10\n",
            "24/02/25 04:36:51 INFO Executor: Finished task 1.0 in stage 9.0 (TID 13). 2959 bytes result sent to driver\n",
            "24/02/25 04:36:51 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 13) in 318 ms on c2d0b6a37d90 (executor driver) (1/2)\n",
            "24/02/25 04:36:51 INFO PythonRunner: Times: total = 319, boot = -50, init = 364, finish = 5\n",
            "24/02/25 04:36:51 INFO Executor: Finished task 0.0 in stage 9.0 (TID 12). 2960 bytes result sent to driver\n",
            "24/02/25 04:36:51 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 12) in 360 ms on c2d0b6a37d90 (executor driver) (2/2)\n",
            "24/02/25 04:36:51 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "24/02/25 04:36:51 INFO DAGScheduler: ResultStage 9 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185) finished in 0.388 s\n",
            "24/02/25 04:36:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/02/25 04:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "24/02/25 04:36:51 INFO DAGScheduler: Job 3 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw2/task1_ref.py:185, took 1.140090 s\n",
            "Duration:  43.10707116127014\n",
            "24/02/25 04:36:51 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/02/25 04:36:51 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/02/25 04:36:51 INFO SparkUI: Stopped Spark web UI at http://c2d0b6a37d90:4040\n",
            "24/02/25 04:36:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/02/25 04:36:52 INFO MemoryStore: MemoryStore cleared\n",
            "24/02/25 04:36:52 INFO BlockManager: BlockManager stopped\n",
            "24/02/25 04:36:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/02/25 04:36:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/02/25 04:36:52 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/02/25 04:36:52 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/25 04:36:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-e490c699-76ae-41e3-8159-e118e2e6a24e/pyspark-35a5a594-888f-40e3-8a13-1c52ee0aeffa\n",
            "24/02/25 04:36:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-bb018301-4b3e-467f-be87-4101fa096a46\n",
            "24/02/25 04:36:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-e490c699-76ae-41e3-8159-e118e2e6a24e\n",
            "time: 55.9 s (started: 2024-02-25 04:35:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare files"
      ],
      "metadata": {
        "id": "UKCdp73Wdzft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_files(\"t1-s2-c2-ref.txt\", \"t1-s2-c2.txt\")\n",
        "compare_files(\"t1-s2-c1-ref.txt\", \"t1-s2-c1.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sutSRWj2jnF0",
        "outputId": "a326d023-74d8-413d-bb12-968898134567"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The contents of the files are the same.\n",
            "The contents of the files are the same.\n",
            "time: 18.5 ms (started: 2024-02-25 10:55:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "5_TFh4MNuVGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from collections import Counter\n",
        "from itertools import combinations, chain\n",
        "\n",
        "\n",
        "def generate_baskets(data, filter_threshold):\n",
        "    \"\"\"\n",
        "    Generate baskets based on the specified case.\n",
        "    \"\"\"\n",
        "    baskets = (\n",
        "        data.map(lambda x: (x[0], [x[1]]))\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "        .filter(lambda x: len(x[1]) > filter_threshold)\n",
        "    )\n",
        "    return baskets\n",
        "\n",
        "\n",
        "def hash_function(item1, item2, num_buckets):\n",
        "    \"\"\"\n",
        "    Simple hash function to distribute items into different buckets.\n",
        "    \"\"\"\n",
        "    hash_val = int(item1) ^ int(item2)\n",
        "    return hash_val % num_buckets\n",
        "\n",
        "\n",
        "def PCY(baskets, total_num_baskets, support, num_buckets):\n",
        "    \"\"\"\n",
        "    Run the PCY algorithm to find candidates\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    baskets = list(baskets)\n",
        "\n",
        "    # Calculate the support ratio based on the number of baskets in the current partition and the total number of baskets\n",
        "    support_ratio = len(baskets) / total_num_baskets\n",
        "\n",
        "    # Calculate the partition support threshold by multiplying the support ratio with the desired support\n",
        "    partition_support_threshold = support_ratio * support\n",
        "\n",
        "    # First pass:\n",
        "    # Initialize counters to keep track of item counts, and hashes\n",
        "    item_counts = Counter()\n",
        "    hash_counts = Counter()\n",
        "\n",
        "    # Initialize a bitmap list with zeros, representing whether each hash bucket meets the support threshold\n",
        "    bitmap = [0] * num_buckets\n",
        "\n",
        "    for basket in baskets:\n",
        "        # Count item pairs\n",
        "        item_counts.update(basket)\n",
        "\n",
        "        pairs = list(combinations(basket, 2))\n",
        "\n",
        "        # Hash pairs and count the number of occurence every hash\n",
        "        hashes = [hash_function(item[0], item[1], num_buckets) for item in pairs]\n",
        "        hash_counts.update(hashes)\n",
        "\n",
        "    # Update the bitmap if count exceeds support threshold\n",
        "    for h, count in hash_counts.items():\n",
        "        if count >= partition_support_threshold:\n",
        "            bitmap[h] = 1\n",
        "\n",
        "    # Second Pass:\n",
        "    frequent_singles = []\n",
        "    for item, count in item_counts.items():\n",
        "        if count >= partition_support_threshold:\n",
        "            frequent_singles.append(item)\n",
        "            candidates.append((tuple([item]), 1))\n",
        "\n",
        "    frequent_pairs = []\n",
        "    for item in combinations(frequent_singles, 2):\n",
        "        hash_val = hash_function(item[0], item[1], num_buckets)\n",
        "        if bitmap[hash_val] == 1:\n",
        "            frequent_pairs.append(item)\n",
        "\n",
        "    if len(frequent_pairs) == 0:\n",
        "        return []\n",
        "\n",
        "    # Filter items from basket that are not frequent\n",
        "    baskets = [[item for item in basket if item in frequent_singles] for basket in baskets]\n",
        "\n",
        "    # Initialize candidate dictionary\n",
        "    candidate_dict = {pair: 0 for pair in frequent_pairs}\n",
        "\n",
        "    # Count occurrences of frequent pairs in baskets\n",
        "    for basket in baskets:\n",
        "        for pair in frequent_pairs:\n",
        "            if set(pair).issubset(basket):\n",
        "                candidate_dict[pair] += 1\n",
        "\n",
        "    # Filter the pair that do not meet the support threshold\n",
        "    candidate_dict = {\n",
        "        pair: count for pair, count in candidate_dict.items() if count >= partition_support_threshold\n",
        "    }\n",
        "\n",
        "    # Append new candidates to the candidates list\n",
        "    candidates += [(pair, 1) for pair in candidate_dict.keys()]\n",
        "\n",
        "    # Valid Candidates\n",
        "    val_candidates = candidate_dict.keys()\n",
        "\n",
        "    # Check for frequent pairs in larger sets\n",
        "    k = 3\n",
        "    while len(val_candidates) > 0:\n",
        "        unq_singles = set(chain.from_iterable(val_candidates))\n",
        "        itemsets = list(combinations(unq_singles, k))\n",
        "\n",
        "        # Initialize candidate dictionary\n",
        "        candidate_dict = {itemset: 0 for itemset in itemsets}\n",
        "\n",
        "        # Count occurrences of frequent pairs in baskets\n",
        "        for basket in baskets:\n",
        "            for itemset in itemsets:\n",
        "                # Check if the candidate itemset is a subset of the current basket\n",
        "                if set(itemset).issubset(basket):\n",
        "                    candidate_dict[itemset] += 1\n",
        "\n",
        "        # Filter the pair that do not meet the support threshold\n",
        "        candidate_dict = {\n",
        "            itemset: count for itemset, count in candidate_dict.items() if count >= partition_support_threshold\n",
        "        }\n",
        "\n",
        "        # Append new candidates to the candidates list\n",
        "        candidates += [(itemset, 1) for itemset in candidate_dict.keys()]\n",
        "\n",
        "        # Update Valid Candidates\n",
        "        val_candidates = candidate_dict.keys()\n",
        "\n",
        "        # Update Order of larger itemset for next iteration\n",
        "        k += 1\n",
        "\n",
        "    return candidates\n",
        "\n",
        "\n",
        "def check_itemsets_in_basket(baskets, candidates):\n",
        "    \"\"\"\n",
        "    Check if candidate itemsets are frequent in the given baskets.\n",
        "\n",
        "    Parameters:\n",
        "        baskets (iterable): An iterable containing baskets of items.\n",
        "        candidates (iterable): An iterable containing candidate itemsets.\n",
        "\n",
        "    Returns:\n",
        "        frequent_itemsets (list): A list of frequent itemsets found in the baskets.\n",
        "    \"\"\"\n",
        "    candidates = list(candidates)\n",
        "    frequent_itemsets = []\n",
        "\n",
        "    for basket in baskets:\n",
        "        for itemset in candidates:\n",
        "            # Check if the candidate itemset is a subset of the current basket\n",
        "            if set(itemset).issubset(basket):\n",
        "                frequent_itemsets.append(itemset)\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def SON(baskets, support, num_buckets=1000):\n",
        "    \"\"\"\n",
        "    Perform SON algorithm to find frequent itemsets.\n",
        "\n",
        "    Args:\n",
        "        baskets (RDD): RDD containing baskets of items.\n",
        "        support (int): Minimum support threshold.\n",
        "        num_buckets (int, optional): Number of buckets for the PCY algorithm. Defaults to 1000.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two dictionaries:\n",
        "            - candidates_map: A dictionary mapping length of itemsets to the list of frequent candidates.\n",
        "            - frequent_itemsets: A dictionary mapping length of frequent itemsets to the list of frequent itemsets.\n",
        "    \"\"\"\n",
        "    # Count the number of baskets\n",
        "    num_baskets = baskets.count()\n",
        "\n",
        "    # Extract individual items from baskets\n",
        "    items = baskets.values()\n",
        "\n",
        "    # Stage 1: Finding Frequent Itemsets using PCY\n",
        "    candidates = (\n",
        "        items.mapPartitions(lambda chunk: PCY(chunk, num_baskets, support, num_buckets))\n",
        "        .reduceByKey(lambda x, y: x + y)\n",
        "        .map(lambda x: tuple(sorted(x[0])))\n",
        "        .distinct()\n",
        "        .sortBy(lambda x: (len(x), x))\n",
        "    )\n",
        "\n",
        "    # Group candidates by length and collect as a map\n",
        "    candidates_map = (\n",
        "        candidates\n",
        "        .groupBy(lambda x: len(x))\n",
        "        .mapValues(list)  # Convert grouped values to a list\n",
        "        .sortByKey()\n",
        "        .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # Retrieve frequent candidates as a list\n",
        "    candidates = candidates.collect()\n",
        "\n",
        "    # Stage 2: Candidate Pruning\n",
        "    frequent_itemsets = (\n",
        "        items.mapPartitions(lambda chunk: check_itemsets_in_basket(chunk, candidates))\n",
        "        .map(lambda itemset: (itemset, 1))\n",
        "        .reduceByKey(lambda x, y: x + y)\n",
        "        .filter(lambda x: x[1] >= support)\n",
        "        .map(lambda x: tuple(sorted(x[0])))\n",
        "        .sortBy(lambda x: (len(x), x))\n",
        "        .groupBy(lambda x: len(x))\n",
        "        .mapValues(list)  # Convert grouped values to a list\n",
        "        .sortByKey()\n",
        "        .collectAsMap()\n",
        "    )\n",
        "    return candidates_map, frequent_itemsets\n",
        "\n",
        "\n",
        "def create_date_customer_id(date, customer_id):\n",
        "    date = date.replace('\"',\"\").split(\"/\")\n",
        "    date = f\"{date[0]}/{date[1]}/{date[2][2:]}\"\n",
        "    customer_id = int(customer_id.replace('\"',\"\"))\n",
        "    return f\"{date}-{customer_id}\"\n",
        "\n",
        "\n",
        "def process_prod_id(prod_id):\n",
        "    prod_id = int(prod_id.replace('\"',\"\"))\n",
        "    return f\"{prod_id}\"\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Drop the header row\n",
        "    data_header = data.first()\n",
        "    data = (\n",
        "        data.filter(lambda row: row != data_header)\n",
        "        .map(lambda row: row.split(\",\"))\n",
        "        .map(lambda row: [create_date_customer_id(row[0], row[1]), process_prod_id(row[5])])\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "def task2(filter_threshold, support, input_file_path, output_file_path):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 2\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Read the input data\n",
        "        data = spark.textFile(input_file_path)\n",
        "        data = preprocess_data(data)\n",
        "\n",
        "        # Generate baskets based on the specified filter threshold\n",
        "        baskets = generate_baskets(data, filter_threshold).cache()\n",
        "\n",
        "        # Run the SON Algorithm\n",
        "        candidates, frequent_itemsets = SON(baskets, support, num_buckets=1000)\n",
        "\n",
        "        # Output the results\n",
        "        with open(output_file_path, \"w\") as f:\n",
        "            f.write(f\"Candidates:\\n\")\n",
        "            for _, itemsets in candidates.items():\n",
        "                itemsets = \",\".join(map(str, itemsets)).replace(\",)\", \")\")\n",
        "                f.write(f\"{itemsets}\\n\\n\")\n",
        "            f.write(\"Frequent Itemsets:\\n\")\n",
        "            for line, itemsets in frequent_itemsets.items():\n",
        "                itemsets = \",\".join(map(str, itemsets)).replace(\",)\", \")\")\n",
        "                f.write(f\"{itemsets}\")\n",
        "                if line < len(frequent_itemsets):\n",
        "                    f.write(\"\\n\\n\")\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if len(sys.argv) != 5:\n",
        "#         print(\n",
        "#             \"Usage: spark-submit --executor-memory 4G --driver-memory 4G \"\n",
        "#             \"task2.py <filter_threshold> <support> <input_file_path> <output_file_path>\"\n",
        "#         )\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Read input parameters\n",
        "#     filter_threshold = int(sys.argv[1])\n",
        "#     support = int(sys.argv[2])\n",
        "#     input_file_path = sys.argv[3]\n",
        "#     output_file_path = sys.argv[4]\n",
        "\n",
        "#     task2(filter_threshold, support, input_file_path, output_file_path)\n",
        "\n",
        "\n",
        "task2(20, 5, \"ta_feng_all_months_merged.csv\", \"t2.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL7Seel1eH6K",
        "outputId": "f5f73430-1503-4b7f-a7bb-4a9dae49236e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 8.921281814575195\n",
            "\n",
            "time: 10.3 s (started: 2024-02-25 11:05:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE END"
      ],
      "metadata": {
        "id": "us7AVeCEt_Ym"
      }
    }
  ]
}