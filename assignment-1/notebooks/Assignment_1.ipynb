{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_637458Wh-Lb"
      ],
      "authorship_tag": "ABX9TyM27rN1mvwgA0oyjLbv9q1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-1/notebooks/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "IcfH89qRcvsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8AQe0hxcWYz",
        "outputId": "0948ed91-9a63-45ed-fa3d-03886a35eb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=010f2a8c4a67e6a74f9a5202451cdc2458fab6c2969ad7c761c51ef2c150ef7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1\n",
            "time: 396 µs (started: 2024-02-09 01:21:29 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "java --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDPg6waVc1w7",
        "outputId": "c8568e96-8fa0-4693-a23a-62b16d443497"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.21 2023-10-17\n",
            "OpenJDK Runtime Environment (build 11.0.21+9-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.21+9-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "AqwthQw5iHpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "H8dYAv8JiJ1z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "_637458Wh-Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "def task1(review_file_path, output_file_path):\n",
        "    spark = SparkContext(appName=\"Yelp Data Exploration (Task 1)\").getOrCreate()\n",
        "\n",
        "    review_rdd = spark.textFile(review_file_path).map(json.loads).cache()\n",
        "\n",
        "    result = {\n",
        "        \"n_review\": 0,\n",
        "        \"n_review_2018\": 0,\n",
        "        \"n_user\": 0,\n",
        "        \"top10_user\": [],\n",
        "        \"n_business\": 0,\n",
        "        \"top10_business\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        result[\"n_review\"] = review_rdd.count()\n",
        "\n",
        "        result[\"n_review_2018\"] = review_rdd.filter(\n",
        "            lambda x: datetime.strptime(x[\"date\"], \"%Y-%m-%d %H:%M:%S\").year==2018\n",
        "        ).count()\n",
        "\n",
        "        result[\"n_user\"] = review_rdd.map(lambda x: x[\"user_id\"]).distinct().count()\n",
        "\n",
        "        result[\"top10_user\"] = (\n",
        "            review_rdd.map(lambda x: (x[\"user_id\"], 1))\n",
        "            .reduceByKey(lambda a, b: a + b)\n",
        "            .sortBy(lambda x: (-x[1], x[0]))\n",
        "            .take(10)\n",
        "        )\n",
        "\n",
        "        result[\"n_business\"] = review_rdd.map(lambda x: x[\"business_id\"]).distinct().count()\n",
        "\n",
        "        result[\"top10_business\"] = (\n",
        "            review_rdd.map(lambda x: (x[\"business_id\"], 1))\n",
        "            .reduceByKey(lambda a, b: a + b)\n",
        "            .sortBy(lambda x: (-x[1], x[0]))\n",
        "            .take(10)\n",
        "        )\n",
        "\n",
        "        with open(output_file_path, \"w\") as f:\n",
        "            json.dump(result, f, indent=4)\n",
        "\n",
        "        pprint(result)\n",
        "\n",
        "    finally:\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if len(sys.argv) != 3:\n",
        "#         print(\"Usage: spark-submit --executor-memory 4G --driver-memory 4G task1.py <review_file_path> <output_file_path>\")\n",
        "#         sys.exit(1)\n",
        "#     review_filepath = sys.argv[1]\n",
        "#     output_filepath = sys.argv[2]\n",
        "#     task1(review_filepath, output_filepath)\n",
        "\n",
        "task1(\"test_reviews.jsonl\", \"python_task1.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRequrTCiDzB",
        "outputId": "178da381-1f50-4417-a502-b45d47762b31"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_business': 30,\n",
            " 'n_review': 30,\n",
            " 'n_review_2018': 3,\n",
            " 'n_user': 30,\n",
            " 'top10_business': [('3fw2X5bZYeW9xCz_zGhOHg', 1),\n",
            "                    ('6lj2BJ4tJeu7db5asGHQ4w', 1),\n",
            "                    ('8mIrX_LrOnAqWsB5JrOojQ', 1),\n",
            "                    ('9nTF596jDvBBia2EXXiOOg', 1),\n",
            "                    ('AakkkTuGZA2KBodKi2_u8A', 1),\n",
            "                    ('FQ1wBQb3aNeRMThSQEV0Qg', 1),\n",
            "                    ('FxLfqxdYPA6Z85PFKaqLrg', 1),\n",
            "                    ('Gyrez6K8f1AyR7dzW9fvAw', 1),\n",
            "                    ('I4Nr-MVc26qWr08-S3Q1ow', 1),\n",
            "                    ('LUN6swQYa4xJKaM_UEUOEw', 1)],\n",
            " 'top10_user': [('-mA3-1mN4JIEkqOtdbNXCQ', 1),\n",
            "                ('2mxBNBeFrgDszqGS5tdEHA', 1),\n",
            "                ('3CJUJILq7CLHk_9OrvpvQg', 1),\n",
            "                ('5JVY32_bmTBfIGpCCsnAfw', 1),\n",
            "                ('6Fz_nus_OG4gar721OKgZA', 1),\n",
            "                ('86J5DwcFk4f4In1Vxe2TvA', 1),\n",
            "                ('8NwU4TRsD3S6gIfBqFzDMQ', 1),\n",
            "                ('DzZ7piLBF-WsJxqosfJgtA', 1),\n",
            "                ('FIk4lQQu1eTe2EpzQ4xhBA', 1),\n",
            "                ('GYNnVehQeXjty0xH7-6Fhw', 1)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task1.py\n",
        "import json\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "def task1(review_file_path, output_file_path):\n",
        "    spark = SparkContext(appName=\"Yelp Data Exploration (Task 1)\").getOrCreate()\n",
        "\n",
        "    review_rdd = spark.textFile(review_file_path).map(json.loads).cache()\n",
        "\n",
        "    result = {\n",
        "        \"n_review\": 0,\n",
        "        \"n_review_2018\": 0,\n",
        "        \"n_user\": 0,\n",
        "        \"top10_user\": [],\n",
        "        \"n_business\": 0,\n",
        "        \"top10_business\": [],\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        result[\"n_review\"] = review_rdd.count()\n",
        "\n",
        "        result[\"n_review_2018\"] = review_rdd.filter(\n",
        "            lambda x: datetime.strptime(x[\"date\"], \"%Y-%m-%d %H:%M:%S\").year == 2018\n",
        "        ).count()\n",
        "\n",
        "        result[\"n_user\"] = review_rdd.map(lambda x: x[\"user_id\"]).distinct().count()\n",
        "\n",
        "        result[\"top10_user\"] = (\n",
        "            review_rdd.map(lambda x: (x[\"user_id\"], 1))\n",
        "            .reduceByKey(lambda a, b: a + b)\n",
        "            .sortBy(lambda x: (-x[1], x[0]), ascending=True)\n",
        "            .take(10)\n",
        "        )\n",
        "\n",
        "        result[\"n_business\"] = review_rdd.map(lambda x: x[\"business_id\"]).distinct().count()\n",
        "\n",
        "        result[\"top10_business\"] = (\n",
        "            review_rdd.map(lambda x: (x[\"business_id\"], 1))\n",
        "            .reduceByKey(lambda a, b: a + b)\n",
        "            .sortBy(lambda x: (-x[1], x[0]), ascending=True)\n",
        "            .take(10)\n",
        "        )\n",
        "\n",
        "        with open(output_file_path, \"w\") as f:\n",
        "            json.dump(result, f, indent=4)\n",
        "\n",
        "    finally:\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\n",
        "            \"Usage: spark-submit --executor-memory 4G --driver-memory 4G \"\n",
        "            \"task1.py <review_file_path> <output_file_path>\"\n",
        "            \" --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "\n",
        "    review_filepath = sys.argv[1]\n",
        "    output_filepath = sys.argv[2]\n",
        "    task1(review_filepath, output_filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VScaA56NvEOo",
        "outputId": "09671023-a012-47c4-caf6-260c90394b2a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --executor-memory 4G --driver-memory 4G task1.py test_reviews.jsonl python_task1.json --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NigAFkZqvNxY",
        "outputId": "6c768973-9a5b-4e7a-e6bf-918f10861958"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: spark-submit --executor-memory 4G --driver-memory 4G task1.py <review_file_path> <output_file_path> --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties\n",
            "24/02/09 03:55:06 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/09 03:55:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-15424baa-744a-4a31-8be1-4e9c87aa17ca\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "VFYbbDoz1fNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "from time import time\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "def custom_partition_func(record, n_partition):\n",
        "    return hash(record[0]) % n_partition\n",
        "\n",
        "\n",
        "def task2(review_file_path, output_file_path, n_partition):\n",
        "    spark = SparkContext(appName=\"Yelp Data Exploration (Task 2)\").getOrCreate()\n",
        "\n",
        "    review_rdd = spark.textFile(review_file_path).map(json.loads).cache()\n",
        "\n",
        "    result = {\n",
        "        \"default\": {\"n_partition\": 0, \"n_items\": [], \"exe_time\": 0},\n",
        "        \"customized\": {\"n_partition\": 0, \"n_items\": [], \"exe_time\": 0},\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Default partition function\n",
        "        start = time()\n",
        "        _ = (\n",
        "            review_rdd.map(lambda x: (x[\"business_id\"], 1))\n",
        "            .reduceByKey(lambda x, y: x + y)\n",
        "            .sortBy(lambda x: (-x[1], x[0]), ascending=True)\n",
        "            .take(10)\n",
        "        )\n",
        "        end = time()\n",
        "\n",
        "        result[\"default\"][\"n_partition\"] = review_rdd.getNumPartitions()\n",
        "\n",
        "        result[\"default\"][\"n_items\"] = review_rdd.glom().map(len).collect()\n",
        "\n",
        "        result[\"default\"][\"exe_time\"] = end - start\n",
        "\n",
        "        # Customized partition function\n",
        "        start = time()\n",
        "        partitioned_rdd = (\n",
        "            review_rdd.map(lambda x: (x[\"business_id\"], 1))\n",
        "            .partitionBy(n_partition, lambda record: custom_partition_func(record, n_partition))\n",
        "            .cache()\n",
        "        )\n",
        "\n",
        "        _ = (\n",
        "            partitioned_rdd.reduceByKey(lambda x, y: x + y)\n",
        "            .sortBy(lambda x: (-x[1], x[0]), ascending=True)\n",
        "            .take(10)\n",
        "        )\n",
        "        end = time()\n",
        "\n",
        "        result[\"customized\"][\"n_partition\"] = partitioned_rdd.getNumPartitions()\n",
        "\n",
        "        result[\"customized\"][\"n_items\"] = partitioned_rdd.glom().map(len).collect()\n",
        "\n",
        "        result[\"customized\"][\"exe_time\"] = end - start\n",
        "\n",
        "        pprint(result)\n",
        "\n",
        "        with open(output_file_path, \"w\") as f:\n",
        "            json.dump(result, f, indent=4)\n",
        "\n",
        "    finally:\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "task2(\"review.jsonl\", \"python_task2.json\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtmIoVhn1kPh",
        "outputId": "867cce04-1f95-4015-ce99-2b6dbe2f79a0"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'customized': {'exe_time': 2.0041656494140625,\n",
            "                'n_items': [393, 419, 419],\n",
            "                'n_partition': 3},\n",
            " 'default': {'exe_time': 2.1744067668914795,\n",
            "             'n_items': [484, 747],\n",
            "             'n_partition': 2}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.py\n",
        "import json\n",
        "import sys\n",
        "from time import time\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "def custom_partition_func(record):\n",
        "    return hash(record[0]) % n_partition\n",
        "\n",
        "\n",
        "def task2(review_file_path, output_file_path, n_partition):\n",
        "    spark = SparkContext(appName=\"Yelp Data Exploration (Task 2)\").getOrCreate()\n",
        "\n",
        "    review_rdd = spark.textFile(review_file_path).map(json.loads).cache()\n",
        "\n",
        "    result = {\n",
        "        \"default\": {\"n_partition\": 0, \"n_items\": [], \"exe_time\": 0},\n",
        "        \"customized\": {\"n_partition\": 0, \"n_items\": [], \"exe_time\": 0},\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Default partition function\n",
        "        start = time()\n",
        "        _ = (\n",
        "            review_rdd.map(lambda x: (x[\"business_id\"], 1))\n",
        "            .reduceByKey(lambda x, y: x + y)\n",
        "            .sortBy(lambda x: (-x[1], x[0]), ascending=True)\n",
        "            .take(10)\n",
        "        )\n",
        "        end = time()\n",
        "\n",
        "        result[\"default\"][\"n_partition\"] = review_rdd.getNumPartitions()\n",
        "\n",
        "        result[\"default\"][\"n_items\"] = review_rdd.glom().map(len).collect()\n",
        "\n",
        "        result[\"default\"][\"exe_time\"] = end - start\n",
        "\n",
        "        # Customized partition function\n",
        "        start = time()\n",
        "        partitioned_rdd = (\n",
        "            review_rdd.map(lambda x: (x[\"business_id\"], 1))\n",
        "            .partitionBy(n_partition, custom_partition_func)\n",
        "            .cache()\n",
        "        )\n",
        "\n",
        "        _ = (\n",
        "            partitioned_rdd.reduceByKey(lambda x, y: x + y)\n",
        "            .sortBy(lambda x: (-x[1], x[0]), ascending=True)\n",
        "            .take(10)\n",
        "        )\n",
        "        end = time()\n",
        "\n",
        "        result[\"customized\"][\"n_partition\"] = partitioned_rdd.getNumPartitions()\n",
        "\n",
        "        result[\"customized\"][\"n_items\"] = partitioned_rdd.glom().map(len).collect()\n",
        "\n",
        "        result[\"customized\"][\"exe_time\"] = end - start\n",
        "\n",
        "        with open(output_file_path, \"w\") as f:\n",
        "            json.dump(result, f, indent=4)\n",
        "\n",
        "    finally:\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 4:\n",
        "        print(\n",
        "            \"Usage: spark-submit --executor-memory 4G --driver-memory 4G\"\n",
        "            \" task2.py <review_file_path> <output_file_path> <n_partitions>\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "\n",
        "    review_file_path = sys.argv[1]\n",
        "    output_file_path = sys.argv[2]\n",
        "    n_partition = int(sys.argv[3])\n",
        "\n",
        "    task2(review_file_path, output_file_path, n_partition)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn2Ghgeurb-o",
        "outputId": "7fd1aca5-efcc-4548-a352-2de2541714ad"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --executor-memory 4G --driver-memory 4G task2.py review.jsonl python_task2.json 2 --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji2IcMCXrf0W",
        "outputId": "722af555-2206-49e1-c397-823cc6e589fc"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: spark-submit --executor-memory 4G --driver-memory 4G task2.py <review_file_path> <output_file_path> <n_partitions>\n",
            "24/02/09 07:07:18 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/09 07:07:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-5f1b5ecd-20d4-4abd-84e9-9b0af863b234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "dAAp11xZ1huw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "VdF0HUjEiBGj"
      }
    }
  ]
}