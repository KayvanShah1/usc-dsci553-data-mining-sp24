{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YEa_4DYT1XCN68bpyHrdYSTudm1kFb8Q",
      "authorship_tag": "ABX9TyMRsEkhNeh4bbw+XR2NbTd+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-6/notebooks/HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Setup"
      ],
      "metadata": {
        "id": "IkpE_uHiM2Wb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecrii56Z_ieS",
        "outputId": "3266a427-3613-4fb3-e851-e27feffcdb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e799adc1437f6b12d4d9fc10fa2cd660e2b8e1c55c7a22806a8743c50e2f5351\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1 pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark ipython-autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "O28IHA3dNCIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import math\n",
        "import statistics\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZqlDmatM9Nk",
        "outputId": "ced3a5e9-d6dc-4f53-c03a-38b51acd049a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 550 µs (started: 2024-04-19 02:34:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "g_RSXHmHNFXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DSCI553/hw6\")\n",
        "\n",
        "\n",
        "class Path:\n",
        "    current_dir = os.getcwd()\n",
        "    data_dir = os.path.join(current_dir, \"data\")\n",
        "    input_csv_file = os.path.join(data_dir, \"hw6_clustering.txt\")\n",
        "\n",
        "    output_dir = os.path.join(current_dir, \"output\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    task_output_file = os.path.join(output_dir, \"task_op.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7-ggIedNEaC",
        "outputId": "595aef08-5659-424a-943c-83363bc526ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.43 ms (started: 2024-04-19 02:35:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1:  Bradley-Fayyad-Reina (BFR) algorithm\n",
        "In BFR, there are three sets of points that you need to keep track of:\n",
        "\n",
        "**Discard set (DS), Compression set (CS), Retained set (RS)**\n",
        "\n",
        "For each cluster in the DS and CS, the cluster is summarized by:\n",
        "- `N`: The number of points\n",
        "- `SUM`: the sum of the coordinates of the points\n",
        "- `SUMSQ`: the sum of squares of coordinates\n"
      ],
      "metadata": {
        "id": "WVZPNzxQsa6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "LARGE_K_FACTOR = 5\n",
        "\n",
        "\n",
        "def read_data(path):\n",
        "    data = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            row = line.strip().split(\",\")\n",
        "            data.append([float(value) for value in row])\n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "def split_data(arr: np.array):\n",
        "    \"\"\"\n",
        "    Splits the input array into 5 roughly equal parts after shuffling.\n",
        "    \"\"\"\n",
        "    np.random.shuffle(arr)\n",
        "    return np.array_split(arr, 5)\n",
        "\n",
        "\n",
        "def cluster_points(data: np.array, n_cluster: int):\n",
        "    \"\"\"\n",
        "    Clusters the data points using KMeans algorithm.\n",
        "\n",
        "    Args:\n",
        "        data (np.array): The input data array containing points.\n",
        "        n_cluster (int): The desired number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, List[int]]: A dictionary where keys represent cluster IDs and values contain the indices of points\n",
        "        assigned to each cluster.\n",
        "    \"\"\"\n",
        "    model = KMeans(n_clusters=LARGE_K_FACTOR*n_cluster)\n",
        "    model.fit(data[:, 2:])\n",
        "\n",
        "    clusters = {}\n",
        "    for idx, cid in enumerate(model.labels_):\n",
        "        clusters.setdefault(cid, []).append(idx)\n",
        "    return clusters\n",
        "\n",
        "\n",
        "def update_dict_values(data_dict: dict, data: np.array, cluster_id: int, idx: list):\n",
        "    \"\"\"\n",
        "    Update the values in a dictionary containing information about clusters.\n",
        "\n",
        "    Parameters:\n",
        "        data_dict (dict): The dictionary containing information about clusters.\n",
        "        data (np.array): The input data array.\n",
        "        cluster_id (int): The ID of the cluster being updated.\n",
        "        idx (list): The indices of the points belonging to the cluster.\n",
        "    \"\"\"\n",
        "    features = data[idx, 2:]\n",
        "\n",
        "    n = len(idx)\n",
        "    SUM = np.sum(features, axis=0)\n",
        "    SUMSQ = np.sum(np.square(features), axis=0)\n",
        "    centroid = SUM / n\n",
        "\n",
        "    data_dict[\"params\"][cluster_id] = [n, SUM, SUMSQ]\n",
        "    data_dict[\"points\"][cluster_id] = np.array(data[idx, 0]).astype(int).tolist()\n",
        "    data_dict[\"centroids\"][cluster_id] = centroid\n",
        "    data_dict[\"distances\"][cluster_id] = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "\n",
        "\n",
        "def write_intermediate_results(\n",
        "    output_path: str, round_num: int, ds_params_dict: dict, cs_params_dict: dict, retained_set\n",
        "):\n",
        "    \"\"\"\n",
        "    Write intermediate clustering results to a file.\n",
        "\n",
        "    Parameters:\n",
        "        output_path (str): The path to the output file.\n",
        "        round_num (int): The current round number of clustering.\n",
        "        ds_params_dict (dict): A dictionary containing parameters for the Discard Set clusters.\n",
        "        cs_params_dict (dict): A dictionary containing parameters for the Compression Set clusters.\n",
        "        retained_set (set): A set containing isolated points in the Retained Set.\n",
        "    \"\"\"\n",
        "    num_ds = sum(value[0] for value in ds_params_dict.values())\n",
        "    num_cs = sum(value[0] for value in cs_params_dict.values())\n",
        "    with open(output_path, \"a\") as f:\n",
        "        if round_num == 1:\n",
        "            f.write(\"The intermediate results:\\n\")\n",
        "\n",
        "        result_str = f\"Round {round_num}: {num_ds},{len(cs_params_dict)},{num_cs},{len(retained_set)}\\n\"\n",
        "        f.write(result_str)\n",
        "\n",
        "\n",
        "def BFR(data: np.array, n_cluster: int, output_path: str):\n",
        "    # Step 1: Load 20% of the data randomly\n",
        "    data = split_data(data)\n",
        "    split1 = data[0]\n",
        "\n",
        "    # Step 2. Run K-Means (e.g., from sklearn) with a large K (e.g., 5 times of the number of the input clusters)\n",
        "    # on the data in memory using the Euclidean distance as the similarity measurement\n",
        "    clusters = cluster_points(split1, n_cluster)\n",
        "\n",
        "    # Step 3: In the K-Means result from Step 2, move all the clusters that contain only one point to RS (outliers).\n",
        "    retained_set = {idx[0] for idx in clusters.values() if len(idx) == 1}\n",
        "    discarded_split1 = np.delete(split1, list(retained_set), axis=0)\n",
        "\n",
        "    # Step 4. Run K-Means again to cluster the rest of the data points with K = the number of input clusters.\n",
        "    clusters = cluster_points(discarded_split1, n_cluster)\n",
        "\n",
        "    # Create Dictionaries to keep track of data\n",
        "    discarded_set_dict = {\"params\": {}, \"centroids\": {}, \"distances\": {}, \"points\": {}}\n",
        "    compressed_set_dict = {\"params\": {}, \"centroids\": {}, \"distances\": {}, \"points\": {}}\n",
        "\n",
        "    # Step 5. Use the K-Means result from Step 4 to generate the DS clusters (i.e., discard their points and\n",
        "    # generate statistics).\n",
        "    for cluster_id, idx in clusters.items():\n",
        "        update_dict_values(discarded_set_dict, discarded_split1, cluster_id, idx)\n",
        "\n",
        "    # The initialization of DS has finished, so far, you have K numbers of DS clusters (from Step 5) and some\n",
        "    # numbers of RS (from Step 3).\n",
        "\n",
        "    # Step 6. Run K-Means on the points in the RS with a large K (e.g., 5 times of the number of the input clusters) to\n",
        "    # generate CS (clusters with more than one points) and RS (clusters with only one point).\n",
        "    split1_retained = discarded_split1[list(retained_set), :]\n",
        "    if len(retained_set) >= LARGE_K_FACTOR * n_cluster:\n",
        "        clusters = cluster_points(split1_retained, n_cluster)\n",
        "        retained_set = {idx[0] for idx in clusters.values() if len(idx) == 1}\n",
        "\n",
        "        for cluster_id, idx in clusters.items():\n",
        "            if len(idx) > 1:\n",
        "                update_dict_values(compressed_set_dict, split1_retained, cluster_id, idx)\n",
        "\n",
        "    write_intermediate_results(\n",
        "        output_path, 1, discarded_set_dict[\"params\"], compressed_set_dict[\"params\"], retained_set\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def task(input_path: str, n_cluster: int, output_path: str):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        data = read_data(input_path)\n",
        "\n",
        "        BFR(data, n_cluster, output_path)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Check if correct number of command-line arguments are provided\n",
        "#     if len(sys.argv) != 4:\n",
        "#         print(\"Usage: python task3.py <input_filename> <stream_size> <num_of_asks> <output_filename>\")\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Parse command-line arguments\n",
        "#     input_path = sys.argv[1]\n",
        "#     n_cluster = int(sys.argv[2])\n",
        "#     output_path = sys.argv[3]\n",
        "\n",
        "#     # Call task1 function\n",
        "#     task3(input_path, n_cluster, output_path)\n",
        "\n",
        "task(Path.input_csv_file, 5, Path.task_output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw1kyIN0Na-6",
        "outputId": "aafa2e70-057c-4add-88cd-b430085574dc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 11.409284830093384\n",
            "\n",
            "time: 11.4 s (started: 2024-04-19 09:13:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cat output/task_op.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kak1uJUlrq63",
        "outputId": "607b4e85-07c3-4191-9833-8ce70d9ce5e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The intermediate results:\n",
            "Round 1: 64463,0,0,0\n",
            "time: 122 ms (started: 2024-04-19 09:27:58 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE END"
      ],
      "metadata": {
        "id": "dWoCHjwCsCV5"
      }
    }
  ]
}