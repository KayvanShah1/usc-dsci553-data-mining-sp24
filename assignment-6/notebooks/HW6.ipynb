{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WVZPNzxQsa6M"
      ],
      "mount_file_id": "1YEa_4DYT1XCN68bpyHrdYSTudm1kFb8Q",
      "authorship_tag": "ABX9TyM1+zsy2P/O6HWS1FODRUyn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-6/notebooks/HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Setup"
      ],
      "metadata": {
        "id": "IkpE_uHiM2Wb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecrii56Z_ieS",
        "outputId": "73fc9efb-9e0a-4ee1-ce22-70b87430042d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1\n"
          ]
        }
      ],
      "source": [
        "%pip install ipython-autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "O28IHA3dNCIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import math\n",
        "import statistics\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZqlDmatM9Nk",
        "outputId": "17b4d7be-94b4-401e-c75e-281409966187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 504 Âµs (started: 2024-04-21 06:11:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "g_RSXHmHNFXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DSCI553/hw6\")\n",
        "\n",
        "\n",
        "class Path:\n",
        "    current_dir = os.getcwd()\n",
        "    data_dir = os.path.join(current_dir, \"data\")\n",
        "    input_csv_file = os.path.join(data_dir, \"hw6_clustering.txt\")\n",
        "\n",
        "    output_dir = os.path.join(current_dir, \"output\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    task_output_file = os.path.join(output_dir, \"task_op.txt\")\n",
        "    task_output_ref_file = os.path.join(output_dir, \"task_ref_op.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7-ggIedNEaC",
        "outputId": "e09399c7-1abe-47b3-f305-f75071cc9c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 479 ms (started: 2024-04-21 06:11:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1:  Bradley-Fayyad-Reina (BFR) algorithm\n",
        "In BFR, there are three sets of points that you need to keep track of:\n",
        "\n",
        "**Discard set (DS), Compression set (CS), Retained set (RS)**\n",
        "\n",
        "For each cluster in the DS and CS, the cluster is summarized by:\n",
        "- `N`: The number of points\n",
        "- `SUM`: the sum of the coordinates of the points\n",
        "- `SUMSQ`: the sum of squares of coordinates\n"
      ],
      "metadata": {
        "id": "WVZPNzxQsa6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ref"
      ],
      "metadata": {
        "id": "EA_37wSyqye0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(37)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_path = Path.input_csv_file\n",
        "    n_cluster = 5\n",
        "    output_path = Path.task_output_ref_file\n",
        "    s_t = time.time()\n",
        "\n",
        "    rs = set()\n",
        "    ds_dict = {}\n",
        "    ds_c_dict = {}\n",
        "    ds_d_dict = {}\n",
        "    ds_point_dict = {}\n",
        "    cs_dict = {}\n",
        "    cs_c_dict = {}\n",
        "    cs_d_dict = {}\n",
        "    cs_point_dict = {}\n",
        "    D = 0\n",
        "\n",
        "    with open(input_path, \"r\") as f:\n",
        "        datas = np.array(f.readlines())\n",
        "\n",
        "    npdata = []\n",
        "    for data in datas:\n",
        "        temp = np.array(data.strip('\\n').split(','))\n",
        "        npdata.append(temp)\n",
        "    npdata = np.array(npdata).astype(np.float64)\n",
        "\n",
        "    #step1\n",
        "    np.random.shuffle(npdata)\n",
        "    npdata = np.array_split(npdata, 5)\n",
        "    data1 = npdata[0]\n",
        "\n",
        "    #step2\n",
        "    kmeans1 = KMeans(n_clusters=5 * n_cluster).fit(npdata[0][:, 2:])\n",
        "\n",
        "    #step3\n",
        "    clusters = {}\n",
        "    for idx, cid in enumerate(kmeans1.labels_):\n",
        "        if cid not in clusters.keys():\n",
        "            clusters[cid] = []\n",
        "        clusters[cid].append(idx)\n",
        "    rs = set()\n",
        "    #print(len(data1))\n",
        "    for idx in clusters.values():\n",
        "        if len(idx) == 1:\n",
        "            #print(idx)\n",
        "            rs.add(idx[0])\n",
        "    ds = np.delete(data1, list(rs), axis=0)\n",
        "    #print(len(data1))\n",
        "    #print(len(ds))\n",
        "\n",
        "    #step4\n",
        "    kmeans2 = KMeans(n_clusters=n_cluster).fit(ds[:, 2:])\n",
        "    clusters = {}\n",
        "    for idx, cid in enumerate(kmeans2.labels_):\n",
        "        if cid not in clusters.keys():\n",
        "            clusters[cid] = []\n",
        "        clusters[cid].append(idx)\n",
        "\n",
        "    #step5\n",
        "    for cid, idx in clusters.items():\n",
        "        n = len(idx)\n",
        "        ds_feature = ds[idx, 2:]\n",
        "        SUM = np.sum(ds_feature, axis=0)\n",
        "        SUMSQ = np.sum(np.square(ds_feature), axis=0)\n",
        "        ds_dict[cid] = []\n",
        "        ds_dict[cid].append(n)\n",
        "        ds_dict[cid].append(SUM)\n",
        "        ds_dict[cid].append(SUMSQ)\n",
        "\n",
        "        points = np.array(ds[idx, 0]).astype(int).tolist()\n",
        "        ds_point_dict[cid] = points\n",
        "\n",
        "        centroid = SUM / n\n",
        "        ds_c_dict[cid] = centroid\n",
        "\n",
        "        d = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "        ds_d_dict[cid] = d\n",
        "\n",
        "    #step6\n",
        "    data_rs = data1[list(rs), :]\n",
        "    if len(rs) >= 5 * n_cluster:\n",
        "        kmeans_temp = KMeans(n_clusters=5 * n_cluster).fit(data_rs[:, 2:])\n",
        "        clusters = {}\n",
        "        for idx, cid in enumerate(kmeans_temp.labels_):\n",
        "            if cid not in clusters.keys():\n",
        "                clusters[cid] = []\n",
        "            clusters[cid].append(idx)\n",
        "        rs = set()\n",
        "        #print(len(data1))\n",
        "        for idx in clusters.values():\n",
        "            if len(idx) == 1:\n",
        "                #print(idx)\n",
        "                rs.add(idx[0])\n",
        "\n",
        "        for cid, idx in clusters.items():\n",
        "            if len(idx) > 1:\n",
        "                n = len(idx)\n",
        "                cs_feature = data_rs[idx, 2:]\n",
        "                SUM = np.sum(cs_feature, axis=0)\n",
        "                SUMSQ = np.sum(np.square(cs_feature), axis=0)\n",
        "                cs_dict[cid] = []\n",
        "                cs_dict[cid].append(n)\n",
        "                cs_dict[cid].append(SUM)\n",
        "                cs_dict[cid].append(SUMSQ)\n",
        "\n",
        "                points = np.array(data_rs[idx, 0]).astype(int).tolist()\n",
        "                cs_point_dict[cid] = points\n",
        "\n",
        "                centroid = SUM / n\n",
        "                cs_c_dict[cid] = centroid\n",
        "\n",
        "                d = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "                cs_d_dict[cid] = d\n",
        "    #print(cs_dict)\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.write('The intermediate results:\\n')\n",
        "        num_ds = 0\n",
        "        num_cs = 0\n",
        "        for value in ds_dict.values():\n",
        "            num_ds += value[0]\n",
        "        for value in cs_dict.values():\n",
        "            num_cs += value[0]\n",
        "        result_str = 'Round 1: ' + str(num_ds) + ',' + str(len(cs_dict)) + ',' + str(num_cs) + ',' + str(len(rs)) + '\\n'\n",
        "        f.write(result_str)\n",
        "\n",
        "    #step7-12\n",
        "    D = 2 * np.sqrt(data1.shape[1] - 2)\n",
        "    for i in range(2, 6):\n",
        "        #step7\n",
        "        for idx, value in enumerate(npdata[i - 1]):\n",
        "            data = value[2:]\n",
        "            #step8\n",
        "            maxx = float('inf')\n",
        "            cluster = -1\n",
        "            for cid, values in ds_dict.items():\n",
        "                mahalanobis_dis = np.sqrt(np.sum(np.square(np.divide(np.subtract(data, ds_c_dict[cid]), ds_d_dict[cid])), axis=0))\n",
        "                if mahalanobis_dis < maxx:\n",
        "                    maxx = mahalanobis_dis\n",
        "                    cluster = cid\n",
        "\n",
        "            if maxx < D and cluster != -1:\n",
        "                n = ds_dict[cluster][0] + 1\n",
        "                SUM = np.add(ds_dict[cluster][1], data)\n",
        "                SUMSQ = np.add(ds_dict[cluster][2], np.square(data))\n",
        "                ds_dict[cluster] = []\n",
        "                ds_dict[cluster].append(n)\n",
        "                ds_dict[cluster].append(SUM)\n",
        "                ds_dict[cluster].append(SUMSQ)\n",
        "\n",
        "                centroid = SUM / n\n",
        "                ds_c_dict[cluster] = centroid\n",
        "\n",
        "                d = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "                ds_d_dict[cluster] = d\n",
        "\n",
        "                ds_point_dict[cluster].append(int(value[0]))\n",
        "            else:\n",
        "                #step9\n",
        "                maxx = float('inf')\n",
        "                cluster = -1\n",
        "                for cid, values in cs_dict.items():\n",
        "                    mahalanobis_dis = np.sqrt(np.sum(np.square(np.divide(np.subtract(data, cs_c_dict[cid]), cs_d_dict[cid])), axis=0))\n",
        "                    if mahalanobis_dis < maxx:\n",
        "                        maxx = mahalanobis_dis\n",
        "                        cluster = cid\n",
        "                if maxx < D and cluster != -1:\n",
        "                    n = cs_dict[cluster][0] + 1\n",
        "                    SUM = np.add(cs_dict[cluster][1], data)\n",
        "                    SUMSQ = np.add(cs_dict[cluster][2], np.square(data))\n",
        "                    cs_dict[cluster] = []\n",
        "                    cs_dict[cluster].append(n)\n",
        "                    cs_dict[cluster].append(SUM)\n",
        "                    cs_dict[cluster].append(SUMSQ)\n",
        "\n",
        "                    centroid = SUM / n\n",
        "                    cs_c_dict[cluster] = centroid\n",
        "\n",
        "                    d = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "                    cs_d_dict[cluster] = d\n",
        "\n",
        "                    cs_point_dict[cluster].append(int(value[0]))\n",
        "                else:\n",
        "                    #step10\n",
        "                    rs.add(idx)\n",
        "        #step11\n",
        "        data_rs = npdata[i - 1][list(rs), :]\n",
        "        if len(rs) >= 5 * n_cluster:\n",
        "            kmeans_temp = KMeans(n_clusters=5 * n_cluster).fit(data_rs[:, 2:])\n",
        "            clusters = {}\n",
        "            for idx, cid in enumerate(kmeans_temp.labels_):\n",
        "                if cid not in clusters.keys():\n",
        "                    clusters[cid] = []\n",
        "                    clusters[cid].append(idx)\n",
        "            rs = set()\n",
        "            #print(len(data1))\n",
        "            for idx in clusters.values():\n",
        "                if len(idx) == 1:\n",
        "                    #print(idx)\n",
        "                    rs.add(idx[0])\n",
        "\n",
        "            for cid, idx in clusters.items():\n",
        "                if len(idx) > 1:\n",
        "                    n = len(idx)\n",
        "                    cs_feature = data_rs[idx, 2:]\n",
        "                    SUM = np.sum(cs_feature, axis=0)\n",
        "                    SUMSQ = np.sum(np.square(cs_feature), axis=0)\n",
        "                    cs_dict[cid] = []\n",
        "                    cs_dict[cid].append(n)\n",
        "                    cs_dict[cid].append(SUM)\n",
        "                    cs_dict[cid].append(SUMSQ)\n",
        "\n",
        "                    points = np.array(data_rs[idx, 0]).astype(int).tolist()\n",
        "                    cs_point_dict[cid] = points\n",
        "\n",
        "                    centroid = SUM / n\n",
        "                    cs_c_dict[cid] = centroid\n",
        "\n",
        "                    d = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "                    cs_d_dict[cid] = d\n",
        "\n",
        "        #step12\n",
        "        new_dict = {}\n",
        "        for cid1 in cs_dict.keys():\n",
        "            cluster = -1\n",
        "            for cid2 in cs_dict.keys():\n",
        "                if cid1 != cid2:\n",
        "                    mahalanobis_dis1 = np.sqrt(np.sum(np.square(np.divide(np.subtract(cs_c_dict[cid1], cs_c_dict[cid2]), cs_d_dict[cid2], out=np.zeros_like(np.subtract(cs_c_dict[cid1], cs_c_dict[cid2])), where=cs_d_dict[cid2] != 0)), axis=0))\n",
        "                    mahalanobis_dis2 = np.sqrt(np.sum(np.square(np.divide(np.subtract(cs_c_dict[cid2], cs_c_dict[cid1]), cs_d_dict[cid1], out=np.zeros_like(np.subtract(cs_c_dict[cid2], cs_c_dict[cid1])), where=cs_d_dict[cid1] != 0)), axis=0))\n",
        "                    mahalanobis_dis = min(mahalanobis_dis1, mahalanobis_dis2)\n",
        "                    if mahalanobis_dis < D:\n",
        "                        D = mahalanobis_dis\n",
        "                        cluster = cid2\n",
        "            new_dict[cid1] = cluster\n",
        "        for cid1, cid2 in new_dict.items():\n",
        "            if cid1 in cs_dict and cid2 in cs_dict:\n",
        "                if cid1 != cid2:\n",
        "                    n = cs_dict[cid1][0] + cs_dict[cid2][0]\n",
        "                    SUM = np.add(cs_dict[cid1][1], cs_dict[cid2][1])\n",
        "                    SUMSQ = np.add(cs_dict[cid1][2], cs_dict[cid2][2])\n",
        "                    cs_dict[cid2][0] = n\n",
        "                    cs_dict[cid2][1] = SUM\n",
        "                    cs_dict[cid2][2] = SUMSQ\n",
        "\n",
        "                    centroid = SUM / n\n",
        "                    cs_c_dict[cid2] = centroid\n",
        "\n",
        "                    d = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "                    cs_d_dict[cid2] = d\n",
        "\n",
        "                    cs_point_dict[cid2].extend(cs_point_dict[cid1])\n",
        "\n",
        "                    cs_dict.pop(cid2)\n",
        "                    cs_c_dict.pop(cid2)\n",
        "                    cs_d_dict.pop(cid2)\n",
        "                    cs_point_dict.pop(cid2)\n",
        "        #step13 last iteration\n",
        "        if i == 5:\n",
        "            new_dict = {}\n",
        "            for cid1 in cs_dict.keys():\n",
        "                cluster = -1\n",
        "                for cid2 in ds_dict.keys():\n",
        "                    if cid1 != cid2:\n",
        "                        mahalanobis_dis1 = np.sqrt(np.sum(np.square(np.divide(np.subtract(cs_c_dict[cid1], ds_c_dict[cid2]), ds_d_dict[cid2], out=np.zeros_like(np.subtract(cs_c_dict[cid1], ds_c_dict[cid2])), where=ds_d_dict[cid2] != 0)), axis=0))\n",
        "                        mahalanobis_dis2 = np.sqrt(np.sum(np.square(np.divide(np.subtract(ds_c_dict[cid2], cs_c_dict[cid1]), cs_d_dict[cid1], out=np.zeros_like(np.subtract(ds_c_dict[cid2], cs_c_dict[cid1])), where=cs_d_dict[cid1] != 0)), axis=0))\n",
        "                        mahalanobis_dis = min(mahalanobis_dis1, mahalanobis_dis2)\n",
        "                        if mahalanobis_dis < D:\n",
        "                            D = mahalanobis_dis\n",
        "                            cluster = cid2\n",
        "                new_dict[cid1] = cluster\n",
        "            for cid1, cid2 in new_dict.items():\n",
        "                if cid1 in cs_dict and cid2 in ds_dict:\n",
        "                    if cid1 != cid2:\n",
        "                        n = cs_dict[cid1][0] + ds_dict[cid2][0]\n",
        "                        SUM = np.add(cs_dict[cid1][1], ds_dict[cid2][1])\n",
        "                        SUMSQ = np.add(cs_dict[cid1][2], ds_dict[cid2][2])\n",
        "                        ds_dict[cid2][0] = n\n",
        "                        ds_dict[cid2][1] = SUM\n",
        "                        ds_dict[cid2][2] = SUMSQ\n",
        "\n",
        "                        centroid = SUM / n\n",
        "                        ds_c_dict[cid2] = centroid\n",
        "\n",
        "                        d = np.sqrt(np.subtract(SUMSQ / n, np.square(centroid)))\n",
        "                        ds_d_dict[cid2] = d\n",
        "\n",
        "                        ds_point_dict[cid2].extend(cs_point_dict[cid1])\n",
        "\n",
        "                        cs_dict.pop(cid1)\n",
        "                        cs_c_dict.pop(cid1)\n",
        "                        cs_d_dict.pop(cid1)\n",
        "                        cs_point_dict.pop(cid1)\n",
        "\n",
        "        with open(output_path, \"a\") as f:\n",
        "            num_ds = 0\n",
        "            num_cs = 0\n",
        "            for value in ds_dict.values():\n",
        "                num_ds += value[0]\n",
        "            for value in cs_dict.values():\n",
        "                num_cs += value[0]\n",
        "            result_str = 'Round ' + str(i) + ': ' + str(num_ds) + ',' + str(len(cs_dict)) + ',' + str(num_cs) + ',' + str(len(rs)) + '\\n'\n",
        "            f.write(result_str)\n",
        "    if len(rs) > 0:\n",
        "        data_rs = npdata[4][list(rs), 0]\n",
        "        rs = set([int(n) for n in data_rs])\n",
        "    result = {}\n",
        "    for cid in ds_dict.keys():\n",
        "        for point in ds_point_dict[cid]:\n",
        "            result[point] = cid\n",
        "    for cid in cs_dict.keys():\n",
        "        for point in cs_point_dict[cid]:\n",
        "            result[point] = -1\n",
        "    for point in rs:\n",
        "        result[point] = -1\n",
        "\n",
        "    with open(output_path, \"a\") as f:\n",
        "        f.write('\\n')\n",
        "        f.write('The clustering results:\\n')\n",
        "        order_dict = sorted(result.keys(), key=int)\n",
        "        for point in order_dict:\n",
        "            f.write(str(point) + ',' + str(result[point]) + '\\n')\n",
        "\n",
        "    e_t = time.time()\n",
        "    print('Duration: ', e_t - s_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEaY62tEq0hC",
        "outputId": "e232b851-10c1-4f11-cbd3-500933480022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration:  26.34880781173706\n",
            "time: 26.4 s (started: 2024-04-21 06:18:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cat output/task_ref_op.txt | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNXtz3S3rUmf",
        "outputId": "7cc3999f-af52-491c-a609-edc663b45a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The intermediate results:\n",
            "Round 1: 64462,0,0,1\n",
            "Round 2: 128904,0,0,22\n",
            "Round 3: 193341,0,0,25\n",
            "Round 4: 257793,0,0,25\n",
            "Round 5: 322236,0,0,25\n",
            "\n",
            "The clustering results:\n",
            "0,1\n",
            "1,1\n",
            "2,2\n",
            "3,1\n",
            "4,0\n",
            "5,0\n",
            "6,3\n",
            "7,1\n",
            "8,0\n",
            "9,0\n",
            "10,1\n",
            "11,0\n",
            "time: 107 ms (started: 2024-04-21 06:18:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 4"
      ],
      "metadata": {
        "id": "uG6xX9pGiYq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "LARGE_K_FACTOR = 5\n",
        "\n",
        "\n",
        "def read_data(path):\n",
        "    data = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            row = line.strip().split(\",\")\n",
        "            data.append([float(value) for value in row])\n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "def get_random_splits(arr: np.array):\n",
        "    \"\"\"\n",
        "    Splits the input array into 5 roughly equal parts after shuffling.\n",
        "    \"\"\"\n",
        "    np.random.shuffle(arr)\n",
        "    return np.array_split(arr, 5)\n",
        "\n",
        "\n",
        "def cluster_points(data: np.array, n_cluster: int, factor: int):\n",
        "    \"\"\"\n",
        "    Clusters the data points using KMeans algorithm.\n",
        "\n",
        "    Args:\n",
        "        data (np.array): The input data array containing points.\n",
        "        n_cluster (int): The desired number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, List[int]]: A dictionary where keys represent cluster IDs and values contain the indices of points\n",
        "        assigned to each cluster.\n",
        "    \"\"\"\n",
        "    model = KMeans(n_clusters=factor*n_cluster)\n",
        "    model.fit(data[:, 2:])\n",
        "\n",
        "    clusters = {}\n",
        "    for i, cluster_id in enumerate(model.labels_):\n",
        "        if cluster_id not in clusters.keys():\n",
        "            clusters[cluster_id] = []\n",
        "        clusters[cluster_id].append(i)\n",
        "    return clusters\n",
        "\n",
        "\n",
        "def process_cluster_data(clusters, data, track_set, check_len=False):\n",
        "    for cluster_id, idx in clusters.items():\n",
        "        n = len(idx)\n",
        "        if check_len and n <= 1:\n",
        "            continue\n",
        "\n",
        "        feats = data[idx, 2:]\n",
        "        row_ids = data[idx, 0]\n",
        "\n",
        "        SUM = np.sum(feats, axis=0)\n",
        "        SUMSQ = np.sum(np.square(feats), axis=0)\n",
        "\n",
        "        ctr = SUM / n\n",
        "        pts = row_ids.astype(int).tolist()\n",
        "        var = np.sqrt(np.subtract(SUMSQ / n, np.square(ctr)))\n",
        "\n",
        "        track_set[\"smr\"][cluster_id] = [n, SUM, SUMSQ]\n",
        "        track_set[\"pts\"][cluster_id] = pts\n",
        "        track_set[\"ctr\"][cluster_id] = ctr\n",
        "        track_set[\"var\"][cluster_id] = var\n",
        "\n",
        "    return track_set\n",
        "\n",
        "\n",
        "def m_dist_pt(feat, ctr, var):\n",
        "    diff = feat - ctr\n",
        "    z = np.square(diff / var)\n",
        "    return np.sqrt(np.sum(z))\n",
        "\n",
        "\n",
        "def m_dist_clusters(ctr1, ctr2, var):\n",
        "    diff = ctr1 - ctr2\n",
        "    z = np.square(diff / var)\n",
        "    return np.sqrt(np.sum(z))\n",
        "\n",
        "\n",
        "def reassign_cluster(feat, t_set):\n",
        "    max_dist = np.inf\n",
        "    cluster = -1\n",
        "    for cluster_id, values in t_set[\"smr\"].items():\n",
        "        ctr = t_set[\"ctr\"][cluster_id]\n",
        "        var = t_set[\"var\"][cluster_id]\n",
        "\n",
        "        m_dist = m_dist_pt(feat, ctr, var)\n",
        "        if m_dist < max_dist:\n",
        "            max_dist = m_dist\n",
        "            cluster = cluster_id\n",
        "\n",
        "    return max_dist, cluster\n",
        "\n",
        "\n",
        "def update_cluster_summary(feat, t_set, cluster, row_id):\n",
        "    n = t_set[\"smr\"][cluster][0] + 1\n",
        "    SUM = np.add(t_set[\"smr\"][cluster][1], feat)\n",
        "    SUMSQ = np.add(t_set[\"smr\"][cluster][2], np.square(feat))\n",
        "\n",
        "    ctr = SUM / n\n",
        "    var = np.sqrt(np.subtract(SUMSQ / n, np.square(ctr)))\n",
        "\n",
        "    t_set[\"smr\"][cluster] = [n, SUM, SUMSQ]\n",
        "    t_set[\"ctr\"][cluster] = ctr\n",
        "    t_set[\"var\"][cluster] = var\n",
        "    t_set[\"pts\"][cluster].append(row_id)\n",
        "\n",
        "    return t_set\n",
        "\n",
        "\n",
        "def merge_clusters(t_set1, t_set2, threshold_dist, inverse=False):\n",
        "    new_assignment = {}\n",
        "    for c1 in t_set1[\"smr\"].keys():\n",
        "        cluster = -1\n",
        "        for c2 in t_set2[\"smr\"].keys():\n",
        "            if c1 != c2:\n",
        "                m_dist1 = m_dist_clusters(t_set1[\"ctr\"][c1], t_set2[\"ctr\"][c2], t_set2[\"var\"][c2])\n",
        "                m_dist2 = m_dist_clusters(t_set2[\"ctr\"][c2], t_set1[\"ctr\"][c1], t_set1[\"var\"][c1])\n",
        "                m_dist = min(m_dist1, m_dist2)\n",
        "                if m_dist < threshold_dist:\n",
        "                    threshold_dist = m_dist\n",
        "                    cluster = c2\n",
        "        new_assignment[c1] = cluster\n",
        "\n",
        "    for c1, c2 in new_assignment.items():\n",
        "        if c1 in t_set1[\"smr\"] and c2 in t_set2[\"smr\"]:\n",
        "            if c1 != c2:\n",
        "                n = t_set1[\"smr\"][c1][0] + t_set2[\"smr\"][c2][0]\n",
        "                SUM = np.add(t_set1[\"smr\"][c1][1], t_set2[\"smr\"][c2][1])\n",
        "                SUMSQ = np.add(t_set1[\"smr\"][c1][2], t_set2[\"smr\"][c2][2])\n",
        "\n",
        "                ctr = SUM / n\n",
        "                var = np.sqrt(np.subtract(SUMSQ / n, np.square(ctr)))\n",
        "\n",
        "                t_set2[\"smr\"][c2] = [n, SUM, SUMSQ]\n",
        "                t_set2[\"ctr\"][c2] = ctr\n",
        "                t_set2[\"var\"][c2] = var\n",
        "                t_set2[\"pts\"][c2].extend(t_set1[\"pts\"][c1])\n",
        "\n",
        "                cluster_to_pop = c1 if inverse else c2\n",
        "\n",
        "                t_set1[\"smr\"].pop(cluster_to_pop)\n",
        "                t_set1[\"ctr\"].pop(cluster_to_pop)\n",
        "                t_set1[\"var\"].pop(cluster_to_pop)\n",
        "                t_set1[\"pts\"].pop(cluster_to_pop)\n",
        "\n",
        "    return t_set1, t_set2\n",
        "\n",
        "\n",
        "def update_round_results(\n",
        "    round_res:list, round_num: int, ds_smr: dict, cs_smr: dict, r_set: set\n",
        "):\n",
        "    \"\"\"\n",
        "    Write intermediate clustering results to a file.\n",
        "\n",
        "    Parameters:\n",
        "        output_path (str): The path to the output file.\n",
        "        round_num (int): The current round number of clustering.\n",
        "        ds_smr (dict): A dictionary containing parameters for the Discard Set clusters.\n",
        "        cs_params_dict (dict): A dictionary containing parameters for the Compression Set clusters.\n",
        "        r_set (set): A set containing isolated points in the Retained Set.\n",
        "    \"\"\"\n",
        "    num_ds = sum(value[0] for value in ds_smr.values())\n",
        "    num_cs = sum(value[0] for value in cs_smr.values())\n",
        "\n",
        "    result_str = f\"Round {round_num}: {num_ds},{len(cs_smr)},{num_cs},{len(r_set)}\\n\"\n",
        "    round_res.append(result_str)\n",
        "\n",
        "    return round_res\n",
        "\n",
        "\n",
        "def save_results(round_results, final_results, output_path):\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.write('The intermediate results:\\n')\n",
        "        f.writelines(round_results)\n",
        "\n",
        "        f.write('\\n')\n",
        "        f.write('The clustering results:\\n')\n",
        "        for k,v in final_results.items():\n",
        "            f.write(f\"{int(k)},{int(v)}\\n\")\n",
        "\n",
        "\n",
        "def BFR(data: np.array, n_cluster: int, output_path: str):\n",
        "    # Create Dictionaries to keep track of data\n",
        "    # \"smr\": Summary statistics, \"ctr\": Centroids, \"var\": Variances, \"pts\": Data points indexes.\n",
        "    d_set = {\"smr\": {}, \"ctr\": {}, \"var\": {}, \"pts\": {}} # Discarded Set\n",
        "    c_set = {\"smr\": {}, \"ctr\": {}, \"var\": {}, \"pts\": {}} # Compressed Set\n",
        "\n",
        "    # Store results\n",
        "    round_res = []\n",
        "    final_res = {}\n",
        "    flag = False\n",
        "\n",
        "    # Step 1: Load 20% of the data randomly\n",
        "    data = get_random_splits(data)\n",
        "    split1 = data[0]\n",
        "\n",
        "    # Step 2: Run K-Means (e.g., from sklearn) with a large K (e.g., 5 times of the number of the input clusters)\n",
        "    # on the data in memory using the Euclidean distance as the similarity measurement\n",
        "    clusters = cluster_points(split1, n_cluster, LARGE_K_FACTOR)\n",
        "\n",
        "    # Step 3: In the K-Means result from Step 2, move all the clusters that contain only one point to RS (outliers).\n",
        "    r_set = {idx[0] for idx in clusters.values() if len(idx) == 1}\n",
        "    ds_data = np.delete(split1, list(r_set), axis=0)\n",
        "\n",
        "    # Step 4: Run K-Means again to cluster the rest of the data points with K = the number of input clusters.\n",
        "    clusters = cluster_points(ds_data, n_cluster, 1)\n",
        "\n",
        "    # Step 5: Use the K-Means result from Step 4 to generate the DS clusters (i.e., discard their points and\n",
        "    # generate statistics).\n",
        "    d_set = process_cluster_data(clusters, ds_data, d_set)\n",
        "\n",
        "    # The initialization of DS has finished, so far, you have K numbers of DS clusters (from Step 5) and some\n",
        "    # numbers of RS (from Step 3).\n",
        "    rs_data = split1[list(r_set), :]\n",
        "    if len(r_set) >= LARGE_K_FACTOR * n_cluster:\n",
        "        clusters = cluster_points(rs_data, n_cluster, LARGE_K_FACTOR)\n",
        "        r_set = {idx[0] for idx in clusters.values() if len(idx) == 1}\n",
        "        c_set = process_cluster_data(clusters, rs_data, c_set, check_len=True)\n",
        "\n",
        "    # Store intermediate results for 1st round\n",
        "    round_res = update_round_results(round_res, 1, d_set[\"smr\"], c_set[\"smr\"], r_set)\n",
        "\n",
        "    THRESHOLD_DIST = 2 * np.sqrt(split1.shape[1] - 2)\n",
        "\n",
        "    # Repeat Step 7 - 12\n",
        "    for round in range(2,6):\n",
        "        # Step 7: Load another 20% of the data randomly.\n",
        "        split = data[round - 1]\n",
        "\n",
        "        for idx, value in enumerate(split):\n",
        "            row_id = int(value[0])\n",
        "            feat = value[2:]\n",
        "\n",
        "            # Step 8: For the new points, compare them to each of the DS using the Mahalanobis Distance and assign them\n",
        "            # to the nearest DS clusters if the distance is < 2 * sqrt(ð‘‘).\n",
        "            max_dist, cluster = reassign_cluster(feat, d_set)\n",
        "            if max_dist < THRESHOLD_DIST and cluster != -1:\n",
        "                d_set = update_cluster_summary(feat, d_set, cluster, row_id)\n",
        "\n",
        "            # Step 9: For the new points that are not assigned to DS clusters, using the Mahalanobis Distance and assign\n",
        "            # the points to the nearest CS clusters if the distance is < 2 * sqrt(ð‘‘)\n",
        "            else:\n",
        "                max_dist, cluster = reassign_cluster(feat, c_set)\n",
        "                if max_dist < THRESHOLD_DIST and cluster != -1:\n",
        "                    c_set = update_cluster_summary(feat, c_set, cluster, row_id)\n",
        "\n",
        "                # Step 10: For the new points that are not assigned to a DS cluster or a CS cluster, assign them to RS.\n",
        "                else:\n",
        "                    r_set.add(idx)\n",
        "\n",
        "        # Step 11: Run K-Means on the RS with a large K (e.g., 5 times of the number of the input clusters) to generate\n",
        "        # CS (clusters with more than one points) and RS (clusters with only one point).\n",
        "        rs_split = split[list(r_set), :]\n",
        "        if len(r_set) >= LARGE_K_FACTOR * n_cluster:\n",
        "            clusters = cluster_points(rs_split, n_cluster, LARGE_K_FACTOR)\n",
        "            r_set = {idx[0] for idx in clusters.values() if len(idx) == 1}\n",
        "            c_set = process_cluster_data(clusters, rs_split, c_set, check_len=True)\n",
        "\n",
        "        if flag:\n",
        "            # Step 12: Merge CS clusters that have a Mahalanobis Distance < 2 * sqrt(ð‘‘).\n",
        "            c_set, _ =  merge_clusters(c_set, c_set, THRESHOLD_DIST, inverse=False)\n",
        "\n",
        "            if round == 5:\n",
        "                c_set, d_set =  merge_clusters(c_set, d_set, THRESHOLD_DIST, inverse=True)\n",
        "\n",
        "        # Store intermediate results for every round\n",
        "        round_res = update_round_results(round_res, round, d_set[\"smr\"], c_set[\"smr\"], r_set)\n",
        "\n",
        "    if len(r_set) > 0:\n",
        "        rs_data = data[4][list(r_set), 0]\n",
        "        r_set = set([int(n) for n in rs_data])\n",
        "\n",
        "    # Process points in discarded set\n",
        "    for cluster_id in d_set[\"smr\"].keys():\n",
        "        for pt in d_set[\"pts\"][cluster_id]:\n",
        "            final_res[pt] = cluster_id\n",
        "\n",
        "    # Process points in compressed set\n",
        "    for cluster_id in c_set[\"smr\"].keys():\n",
        "        for pt in c_set[\"pts\"][cluster_id]:\n",
        "            final_res[pt] = -1\n",
        "\n",
        "    # Process points in retained set\n",
        "    for pt in r_set:\n",
        "        final_res[pt] = -1\n",
        "\n",
        "    return round_res, final_res\n",
        "\n",
        "\n",
        "def task(input_path: str, n_cluster: int, output_path: str):\n",
        "    np.random.seed(37)\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        data = read_data(input_path)\n",
        "\n",
        "        round_results, final_results = BFR(data, n_cluster, output_path)\n",
        "\n",
        "        save_results(round_results, final_results, output_path)\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "    print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if correct number of command-line arguments are provided\n",
        "    if len(sys.argv) != 4:\n",
        "        print(\"Usage: python task.py <input_filename> <n_cluster> <output_filename>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Parse command-line arguments\n",
        "    input_path = sys.argv[1]\n",
        "    n_cluster = int(sys.argv[2])\n",
        "    output_path = sys.argv[3]\n",
        "\n",
        "    # Call task1 function\n",
        "    task(input_path, n_cluster, output_path)\n",
        "\n",
        "# task(Path.input_csv_file, 5, Path.task_output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0MW1hHziX_G",
        "outputId": "8e2fee1c-9961-4d96-e7f9-22221f4a5194"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Round 1: 64462,0,0,1\\n',\n",
            " 'Round 2: 128904,0,0,22\\n',\n",
            " 'Round 3: 193341,10,32,15\\n',\n",
            " 'Round 4: 257793,10,32,25\\n',\n",
            " 'Round 5: 322253,6,15,19\\n']\n",
            "Duration: 29.49673867225647\n",
            "\n",
            "time: 29.5 s (started: 2024-04-21 09:53:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE END"
      ],
      "metadata": {
        "id": "dWoCHjwCsCV5"
      }
    }
  ]
}