{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PWWq0Er4FBuM",
        "cL97SCyDykKS",
        "KAj8s8GVuJPX",
        "It8sjYp1SGqG"
      ],
      "mount_file_id": "1IvAguEmMcTRmCA4131k0yKlwAwumXO7c",
      "authorship_tag": "ABX9TyPQUrs+Qy7l38EeZ7XJTYr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-3/notebooks/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Installation"
      ],
      "metadata": {
        "id": "5XnYG3M0Ek7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taGtoPV8D63T",
        "outputId": "e0531089-021a-41fa-86ec-809a887ea16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=532429dc724771aad94a04957806b1d63f4c0b1be3ce85e4e4b54c7f4a8dd824\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1 pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark ipython-autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "java --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IosF_LW0Erdl",
        "outputId": "194669b6-c03b-4e62-c774-25a17f79c26a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.22 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "K2XFckbdEtr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import math\n",
        "import statistics\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwhXtVnuEwIi",
        "outputId": "1f2975ef-9a02-411a-841a-db16cea297ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 447 µs (started: 2024-03-18 02:29:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3\")\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "f7f7NJOyEzwO",
        "outputId": "65b556b9-472b-40b8-926c-248d3a3b824d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 174 ms (started: 2024-03-18 02:29:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6fvZSlEE4OG",
        "outputId": "7d999372-21e3-4f71-99b8-d52176a88b80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HW3.ipynb\tt1.csv\t    t2_1.csv  t2_2-dryrun.csv  task1_ref.py  task2_1_ref.py\n",
            "HW3StudentData\tt1-ref.txt  t2_2.csv  t2-ref.txt       task2_1.py    task2_2.py\n",
            "time: 416 ms (started: 2024-03-18 02:29:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "Q7CV-QZhE7i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "PWWq0Er4FBuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from itertools import combinations\n",
        "import random\n",
        "import csv\n",
        "\n",
        "\n",
        "random.seed(37)\n",
        "\n",
        "# Define constants\n",
        "NUM_HASH_FUNCTIONS = 50\n",
        "PRIME_NUMBER = 15485863\n",
        "ROWS_PER_BAND = 2\n",
        "BANDS = NUM_HASH_FUNCTIONS // ROWS_PER_BAND\n",
        "\n",
        "\n",
        "def prepare_dataset(data):\n",
        "    # Remove the header\n",
        "    header = data.first()\n",
        "    data = (\n",
        "        data.filter(lambda row: row != header)\n",
        "        .map(lambda row: row.split(\",\"))\n",
        "    )\n",
        "\n",
        "    # Find unique users and map it to an index\n",
        "    usr_to_idx = (\n",
        "        data.map(lambda x: x[0])\n",
        "        .distinct()\n",
        "        .zipWithIndex()\n",
        "        .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # Group users that has reviewed a business\n",
        "    business_user = (\n",
        "        data.map(lambda row: (row[1], [row[0]]))\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "    return business_user, usr_to_idx\n",
        "\n",
        "\n",
        "def generate_hash_function_params(max_range, count):\n",
        "    \"\"\"Generate random hash function parameters within a specified range.\"\"\"\n",
        "    hash_funcs = []\n",
        "    for _ in range(count):\n",
        "        a = random.randint(1, max_range)  # Random coefficient 'a'\n",
        "        b = random.randint(0, max_range)  # Random intercept 'b'\n",
        "        hash_funcs.append((a, b))\n",
        "    return hash_funcs\n",
        "\n",
        "\n",
        "def hash_item(item, params, num_bins):\n",
        "    \"\"\"Hash an item using given hash function parameters.\n",
        "    Calculate hash value using the formula: ((a * item + b) % PRIME_NUMBER) % num_bins\n",
        "    \"\"\"\n",
        "    hash_val = ((params[0] * item + params[1]) % PRIME_NUMBER) % num_bins\n",
        "    return hash_val\n",
        "\n",
        "\n",
        "def build_minhash_signature_matrix(hash_funcs, users, num_bins):\n",
        "    \"\"\"Build the minhash signature matrix for a set of users.\"\"\"\n",
        "    mhs = []\n",
        "    for params in hash_funcs:\n",
        "        minhash = float(\"inf\")\n",
        "        for user in users:\n",
        "            # Hash each user and find the minimum hash value\n",
        "            hash_val = hash_item(user, params, num_bins)\n",
        "            minhash = min(minhash, hash_val)\n",
        "        mhs.append(minhash)\n",
        "    return mhs\n",
        "\n",
        "\n",
        "def jaccard_similarity(pair, bus_user_dict):\n",
        "    \"\"\"\n",
        "    Calculate Jaccard similarity for a candidate pair of businesses.\n",
        "\n",
        "    Args:\n",
        "        pair (tuple): A pair of business IDs.\n",
        "        bus_user_dict (dict): Dictionary mapping business IDs to sets of user IDs.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the business pair and their Jaccard similarity.\n",
        "    \"\"\"\n",
        "    # Extract business IDs from the pair\n",
        "    bus1, bus2 = pair\n",
        "\n",
        "    # Get sets of users who reviewed each business\n",
        "    user1 = set(bus_user_dict[bus1])\n",
        "    user2 = set(bus_user_dict[bus2])\n",
        "\n",
        "    # Calculate Jaccard similarity\n",
        "    intersection = len(user1 & user2)\n",
        "    union = len(user1 | user2)\n",
        "    similarity = intersection / union if union != 0 else 0\n",
        "\n",
        "    return (bus1, bus2), similarity\n",
        "\n",
        "\n",
        "def jaccard_based_lsh(prepared_data):\n",
        "    \"\"\"Perform Jaccard-based Locality Sensitive Hashing (LSH) on prepared data.\n",
        "\n",
        "    This function applies LSH to find candidate pairs of businesses with similar users,\n",
        "    based on the Jaccard similarity metric.\n",
        "\n",
        "    Algorithm Steps:\n",
        "    1. Unpack the prepared data containing the business-to-user mapping and user index mapping.\n",
        "    2. Generate a set of hash functions.\n",
        "    3. Compute the Minhash Signature for each business.\n",
        "    4. Divide the signature matrix into bands.\n",
        "    5. Group businesses into bands based on their Minhash Signature.\n",
        "    6. Find candidate pairs of businesses within each band.\n",
        "    7. Calculate the Jaccard similarity for candidate pairs.\n",
        "    8. Filter pairs with similarity above a threshold (e.g., 0.5).\n",
        "    9. Sort the results by business ID pairs.\n",
        "    10. Return the RDD containing the Jaccard similarity results for candidate business pairs.\n",
        "\n",
        "    Args:\n",
        "        prepared_data (tuple): A tuple containing the business-to-user mapping RDD and user index mapping dictionary.\n",
        "\n",
        "    Returns:\n",
        "        RDD: An RDD containing the Jaccard similarity results for candidate business pairs.\n",
        "    \"\"\"\n",
        "    # Unpack prepared data\n",
        "    business_to_user, usr_to_idx = prepared_data\n",
        "\n",
        "    # Generate Hash functions\n",
        "    NUM_BINS = len(usr_to_idx)\n",
        "    hash_func_params = generate_hash_function_params(NUM_BINS, NUM_HASH_FUNCTIONS)\n",
        "\n",
        "    # Compute Minhash Signature\n",
        "    minhash_sign = (\n",
        "        business_to_user.mapValues(lambda users: [usr_to_idx[user] for user in users])\n",
        "        .mapValues(lambda users: build_minhash_signature_matrix(hash_func_params, users, NUM_BINS))\n",
        "    )\n",
        "\n",
        "    # Divide signature matrix into bands\n",
        "    bands = (\n",
        "        minhash_sign.flatMap(\n",
        "            lambda x: [\n",
        "                (\n",
        "                    (i, tuple(x[1][i*ROWS_PER_BAND: (i+1)*ROWS_PER_BAND])), x[0]\n",
        "                )\n",
        "                for i in range(BANDS)\n",
        "            ]\n",
        "        )\n",
        "        .groupByKey()\n",
        "        .mapValues(list)\n",
        "        .filter(lambda x: len(x[1]) > 1)\n",
        "    )\n",
        "\n",
        "    # Find the business candidate pairs\n",
        "    candidates = (\n",
        "        bands.map(lambda x: sorted(x[1]))\n",
        "        .flatMap(lambda x: list(combinations(x, 2)))\n",
        "        .distinct()\n",
        "    )\n",
        "\n",
        "    # Calculate Jaccard Similirality for pairs\n",
        "    bus_to_user_dict = business_to_user.collectAsMap()\n",
        "\n",
        "    jaccard_sim_results = (\n",
        "        candidates.map(lambda x: jaccard_similarity(x, bus_to_user_dict))\n",
        "        .filter(lambda x: x[1] >= 0.5)\n",
        "        .sortByKey()\n",
        "        .map(lambda x: [x[0][0], x[0][1] ,x[1]])\n",
        "    )\n",
        "    return jaccard_sim_results\n",
        "\n",
        "\n",
        "def task1(input_file_name, output_file_name):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 1\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Read the input data\n",
        "        data = spark.textFile(input_file_name)\n",
        "        prepared_data = prepare_dataset(data)\n",
        "\n",
        "        # Compute Jaccard similarity using LSH\n",
        "        jaccard_sim_results = jaccard_based_lsh(prepared_data)\n",
        "\n",
        "        # Write header and results to a CSV file\n",
        "        header = [\"business_id_1\", \"business_id_2\", \"similarity\"]\n",
        "        with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "            writer.writerows(jaccard_sim_results.collect())\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if len(sys.argv) != 3:\n",
        "#         print(\n",
        "#             \"Usage: spark-submit task1.py <input_file_name> <output_file_name>\"\n",
        "#         )\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Read input parameters\n",
        "#     input_file_path = sys.argv[1]\n",
        "#     output_file_path = sys.argv[2]\n",
        "\n",
        "#     task1(input_file_path, output_file_path)\n",
        "\n",
        "\n",
        "task1(\"HW3StudentData/yelp_train.csv\", \"t1.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8RUcZwKK4_T",
        "outputId": "70b232cc-19a3-45d3-ae1b-c77bbfa58c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 53.42814826965332\n",
            "\n",
            "time: 54.7 s (started: 2024-03-13 09:12:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ref"
      ],
      "metadata": {
        "id": "cL97SCyDykKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Reference Code Task 1 { vertical-output: true, form-width: \"30%\" }\n",
        "%%writefile task1_ref.py\n",
        "from pyspark import SparkContext\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "from itertools import combinations\n",
        "import operator\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_path = sys.argv[1]\n",
        "    output_path = sys.argv[2]\n",
        "\n",
        "    s_t = time.time()\n",
        "\n",
        "    spark = SparkContext(appName= \"task1\")\n",
        "    lines = spark.textFile(input_path)\n",
        "    first = lines.first()\n",
        "    lines = lines.filter(lambda row: row != first).map(lambda row: row.split(\",\"))\n",
        "    #print(raw_rdd.take(10))\n",
        "\n",
        "    bus_user = lines.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set)\n",
        "    #print(bus_user.take(10))\n",
        "    bus_user_dict = {}\n",
        "    for bus, users in bus_user.collect():\n",
        "        bus_user_dict[bus] = users\n",
        "    users = lines.map(lambda row: row[0]).distinct()\n",
        "    users_dict = {}\n",
        "    i = 0\n",
        "    for user in users.collect():\n",
        "        users_dict[user] = i\n",
        "        i += 1\n",
        "\n",
        "    n = 60\n",
        "    m = i\n",
        "    p = 1e9 + 7\n",
        "    hash_funcs = [] #[a, b]\n",
        "    a = random.sample(range(1, m), n)\n",
        "    hash_funcs.append(a)\n",
        "    b = random.sample(range(1, m), n)\n",
        "    hash_funcs.append(b)\n",
        "    #print(hash_funcs)\n",
        "\n",
        "    sign_dict = {}\n",
        "    for bus, user_list in bus_user.collect():\n",
        "        minhash_sign_list = []\n",
        "        for i in range(n):\n",
        "            minhash = float(\"inf\")\n",
        "            for user in user_list:\n",
        "                minhash = min(minhash, (((hash_funcs[0][i] * users_dict[user] + hash_funcs[1][i]) % p) % m))\n",
        "            minhash_sign_list.append(int(minhash))\n",
        "        sign_dict[bus] = minhash_sign_list\n",
        "    #print(sign_dict)\n",
        "\n",
        "    r = 2\n",
        "    b = n // r\n",
        "    bands_dict = {}\n",
        "    for bus, minhash_sign in sign_dict.items():\n",
        "        for i in range(0, b):\n",
        "            #print(s[1][i*r: i*r+r])\n",
        "            idx = (i, tuple(minhash_sign[i*r: i*r+r]))\n",
        "            if idx not in bands_dict.keys():\n",
        "                   bands_dict[idx] = []\n",
        "                   bands_dict[idx].append(bus)\n",
        "            else:\n",
        "                   bands_dict[idx].append(bus)\n",
        "    #print(bands_dict)\n",
        "    bands_dict_fi = {}\n",
        "    for key, values in bands_dict.items():\n",
        "        if len(values) > 1:\n",
        "            bands_dict_fi[key] = values\n",
        "    #print(bands_dict_fi)\n",
        "    #418426\n",
        "    candidates = set()\n",
        "    for values in bands_dict_fi.values():\n",
        "        comb_list = combinations(sorted(values), 2)\n",
        "        for item in comb_list:\n",
        "            candidates.add(item)\n",
        "    #print(candidates)\n",
        "\n",
        "    result = {}\n",
        "    for bus1, bus2 in candidates:\n",
        "        user1 = bus_user_dict[bus1]\n",
        "        user2 = bus_user_dict[bus2]\n",
        "        js = len(user1 & user2) / len(user1 | user2)\n",
        "        if js >= 0.5:\n",
        "            result[str(bus1) + \",\" + str(bus2)] = js\n",
        "    result = dict(sorted(result.items(), key=operator.itemgetter(0)))\n",
        "    result_str = \"business_id_1, business_id_2, similarity\\n\"\n",
        "    for key, values in result.items():\n",
        "        result_str += key + \",\" + str(values) + \"\\n\"\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.writelines(result_str)\n",
        "\n",
        "    e_t = time.time()\n",
        "    print('Duration: ', e_t - s_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "GXsY5d1-uQAl",
        "outputId": "29c59c8a-30d7-4245-f9d9-66c88a708981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1_ref.py\n",
            "time: 13.6 ms (started: 2024-03-13 03:07:25 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit task1_ref.py HW3StudentData/yelp_train.csv t1-ref.txt --executor-memory 4G --driver-memory 4G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMGzjLZxuhZE",
        "outputId": "163753f9-b4ea-40dd-b1b9-0ae3dd26de0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/03/13 03:07:40 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/03/13 03:07:40 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "24/03/13 03:07:40 INFO SparkContext: Java version 11.0.22\n",
            "24/03/13 03:07:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/03/13 03:07:40 INFO ResourceUtils: ==============================================================\n",
            "24/03/13 03:07:40 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/03/13 03:07:40 INFO ResourceUtils: ==============================================================\n",
            "24/03/13 03:07:40 INFO SparkContext: Submitted application: task1\n",
            "24/03/13 03:07:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/03/13 03:07:41 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/03/13 03:07:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing view acls to: root\n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing modify acls to: root\n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing view acls groups to: \n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/03/13 03:07:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/03/13 03:07:41 INFO Utils: Successfully started service 'sparkDriver' on port 37875.\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/03/13 03:07:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/03/13 03:07:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/03/13 03:07:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b943d84d-3e85-4eda-96b6-323c9bed1828\n",
            "24/03/13 03:07:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/03/13 03:07:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/03/13 03:07:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/03/13 03:07:42 INFO Executor: Starting executor ID driver on host ebb69f3ce07d\n",
            "24/03/13 03:07:42 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "24/03/13 03:07:42 INFO Executor: Java version 11.0.22\n",
            "24/03/13 03:07:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/03/13 03:07:42 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7376c046 for default.\n",
            "24/03/13 03:07:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45743.\n",
            "24/03/13 03:07:42 INFO NettyBlockTransferService: Server created on ebb69f3ce07d:45743\n",
            "24/03/13 03:07:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/03/13 03:07:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:42 INFO BlockManagerMasterEndpoint: Registering block manager ebb69f3ce07d:45743 with 434.4 MiB RAM, BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
            "24/03/13 03:07:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
            "24/03/13 03:07:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ebb69f3ce07d:45743 (size: 32.6 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:44 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/03/13 03:07:44 INFO FileInputFormat: Total input files to process : 1\n",
            "24/03/13 03:07:44 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:181)\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Missing parents: List()\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "24/03/13 03:07:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ebb69f3ce07d:45743 (size: 4.8 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "24/03/13 03:07:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "24/03/13 03:07:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ebb69f3ce07d, executor driver, partition 0, PROCESS_LOCAL, 7716 bytes) \n",
            "24/03/13 03:07:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/03/13 03:07:45 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/13 03:07:47 INFO PythonRunner: Times: total = 1909, boot = 1270, init = 639, finish = 0\n",
            "24/03/13 03:07:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1478 bytes result sent to driver\n",
            "24/03/13 03:07:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2728 ms on ebb69f3ce07d (executor driver) (1/1)\n",
            "24/03/13 03:07:47 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33057\n",
            "24/03/13 03:07:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:47 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:181) finished in 3.018 s\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:181, took 3.269329 s\n",
            "24/03/13 03:07:48 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Registering RDD 4 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20) as input to shuffle 0\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Got job 1 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) with 2 output partitions\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23)\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20), which has no missing parents\n",
            "24/03/13 03:07:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.4 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ebb69f3ce07d:45743 (size: 7.9 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ebb69f3ce07d, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:48 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (ebb69f3ce07d, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "24/03/13 03:07:48 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "24/03/13 03:07:48 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/13 03:07:48 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/13 03:07:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ebb69f3ce07d:45743 in memory (size: 4.8 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:52 INFO PythonRunner: Times: total = 4065, boot = 22, init = 982, finish = 3061\n",
            "24/03/13 03:07:52 INFO PythonRunner: Times: total = 4071, boot = 48, init = 979, finish = 3044\n",
            "24/03/13 03:07:53 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1624 bytes result sent to driver\n",
            "24/03/13 03:07:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1667 bytes result sent to driver\n",
            "24/03/13 03:07:53 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 4540 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4578 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:53 INFO DAGScheduler: ShuffleMapStage 1 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20) finished in 4.698 s\n",
            "24/03/13 03:07:53 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/13 03:07:53 INFO DAGScheduler: running: Set()\n",
            "24/03/13 03:07:53 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "24/03/13 03:07:53 INFO DAGScheduler: failed: Set()\n",
            "24/03/13 03:07:53 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23), which has no missing parents\n",
            "24/03/13 03:07:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ebb69f3ce07d:45743 (size: 6.4 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (ebb69f3ce07d, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:53 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (ebb69f3ce07d, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)\n",
            "24/03/13 03:07:53 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 31 ms\n",
            "24/03/13 03:07:54 INFO PythonRunner: Times: total = 873, boot = -320, init = 691, finish = 502\n",
            "24/03/13 03:07:54 INFO MemoryStore: Block taskresult_4 stored as bytes in memory (estimated size 5.9 MiB, free 428.2 MiB)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Added taskresult_4 in memory on ebb69f3ce07d:45743 (size: 5.9 MiB, free: 428.5 MiB)\n",
            "24/03/13 03:07:54 INFO PythonRunner: Times: total = 937, boot = -280, init = 631, finish = 586\n",
            "24/03/13 03:07:54 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 6166238 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:54 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 5.8 MiB, free 422.4 MiB)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Added taskresult_3 in memory on ebb69f3ce07d:45743 (size: 5.8 MiB, free: 422.7 MiB)\n",
            "24/03/13 03:07:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 6065056 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:54 INFO TransportClientFactory: Successfully created connection to ebb69f3ce07d/172.28.0.12:45743 after 54 ms (0 ms spent in bootstraps)\n",
            "24/03/13 03:07:54 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 1410 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ebb69f3ce07d:45743 in memory (size: 7.9 KiB, free: 422.7 MiB)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Removed taskresult_4 on ebb69f3ce07d:45743 in memory (size: 5.9 MiB, free: 428.6 MiB)\n",
            "24/03/13 03:07:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 1509 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Removed taskresult_3 on ebb69f3ce07d:45743 in memory (size: 5.8 MiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:54 INFO DAGScheduler: ResultStage 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) finished in 1.550 s\n",
            "24/03/13 03:07:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "24/03/13 03:07:54 INFO DAGScheduler: Job 1 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23, took 6.378695 s\n",
            "24/03/13 03:07:55 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Registering RDD 9 (distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25) as input to shuffle 1\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Got job 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28) with 2 output partitions\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28)\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Submitting ShuffleMapStage 3 (PairwiseRDD[9] at distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25), which has no missing parents\n",
            "24/03/13 03:07:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ebb69f3ce07d:45743 (size: 7.9 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (PairwiseRDD[9] at distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (ebb69f3ce07d, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:55 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6) (ebb69f3ce07d, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:55 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "24/03/13 03:07:55 INFO Executor: Running task 1.0 in stage 3.0 (TID 6)\n",
            "24/03/13 03:07:55 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/13 03:07:55 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/13 03:07:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ebb69f3ce07d:45743 in memory (size: 6.4 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 2251, boot = -831, init = 1390, finish = 1692\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1667 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 2419 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 2362, boot = -894, init = 1312, finish = 1944\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 1.0 in stage 3.0 (TID 6). 1624 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 2474 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:57 INFO DAGScheduler: ShuffleMapStage 3 (distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25) finished in 2.511 s\n",
            "24/03/13 03:07:57 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/13 03:07:57 INFO DAGScheduler: running: Set()\n",
            "24/03/13 03:07:57 INFO DAGScheduler: waiting: Set(ResultStage 4)\n",
            "24/03/13 03:07:57 INFO DAGScheduler: failed: Set()\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28), which has no missing parents\n",
            "24/03/13 03:07:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.3 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ebb69f3ce07d:45743 (size: 6.1 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (ebb69f3ce07d, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:57 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8) (ebb69f3ce07d, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:57 INFO Executor: Running task 0.0 in stage 4.0 (TID 7)\n",
            "24/03/13 03:07:57 INFO Executor: Running task 1.0 in stage 4.0 (TID 8)\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Getting 2 (291.6 KiB) non-empty blocks including 2 (291.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Getting 2 (291.6 KiB) non-empty blocks including 2 (291.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 242, boot = -107, init = 325, finish = 24\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 7). 143710 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 286 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 308, boot = -91, init = 385, finish = 14\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 1.0 in stage 4.0 (TID 8). 144205 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 390 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:57 INFO DAGScheduler: ResultStage 4 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28) finished in 0.420 s\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Job 2 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28, took 2.971443 s\n",
            "24/03/13 03:07:58 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Got job 3 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43) with 2 output partitions\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43)\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Missing parents: List()\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23), which has no missing parents\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.0 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ebb69f3ce07d:45743 (size: 6.4 KiB, free: 434.3 MiB)\n",
            "24/03/13 03:07:58 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:58 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:58 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 9) (ebb69f3ce07d, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:58 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 10) (ebb69f3ce07d, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:58 INFO Executor: Running task 0.0 in stage 6.0 (TID 9)\n",
            "24/03/13 03:07:58 INFO Executor: Running task 1.0 in stage 6.0 (TID 10)\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "24/03/13 03:07:58 INFO PythonRunner: Times: total = 612, boot = -205, init = 488, finish = 329\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block taskresult_9 stored as bytes in memory (estimated size 5.8 MiB, free 428.3 MiB)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Added taskresult_9 in memory on ebb69f3ce07d:45743 (size: 5.8 MiB, free: 428.6 MiB)\n",
            "24/03/13 03:07:58 INFO Executor: Finished task 0.0 in stage 6.0 (TID 9). 6065056 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:58 INFO PythonRunner: Times: total = 696, boot = -101, init = 417, finish = 380\n",
            "24/03/13 03:07:58 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 9) in 754 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Removed taskresult_9 on ebb69f3ce07d:45743 in memory (size: 5.8 MiB, free: 434.3 MiB)\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block taskresult_10 stored as bytes in memory (estimated size 5.9 MiB, free 428.2 MiB)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Added taskresult_10 in memory on ebb69f3ce07d:45743 (size: 5.9 MiB, free: 428.5 MiB)\n",
            "24/03/13 03:07:58 INFO Executor: Finished task 1.0 in stage 6.0 (TID 10). 6166238 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ebb69f3ce07d:45743 in memory (size: 6.1 KiB, free: 428.5 MiB)\n",
            "24/03/13 03:07:58 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 10) in 936 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:59 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:59 INFO DAGScheduler: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43) finished in 1.009 s\n",
            "24/03/13 03:07:59 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "24/03/13 03:07:59 INFO DAGScheduler: Job 3 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43, took 1.026754 s\n",
            "24/03/13 03:07:59 INFO BlockManagerInfo: Removed taskresult_10 on ebb69f3ce07d:45743 in memory (size: 5.9 MiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ebb69f3ce07d:45743 in memory (size: 7.9 KiB, free: 434.4 MiB)\n",
            "Duration:  123.35331583023071\n",
            "24/03/13 03:09:51 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/03/13 03:09:51 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/03/13 03:09:51 INFO SparkUI: Stopped Spark web UI at http://ebb69f3ce07d:4040\n",
            "24/03/13 03:09:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/03/13 03:09:51 INFO MemoryStore: MemoryStore cleared\n",
            "24/03/13 03:09:51 INFO BlockManager: BlockManager stopped\n",
            "24/03/13 03:09:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/03/13 03:09:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/03/13 03:09:51 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-65db0161-2f70-4ea5-91b2-e826cde10906\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-efc7c3eb-9748-43d4-882f-442588c3c146/pyspark-9ce456fe-7700-4444-84c6-1018d55b2604\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-efc7c3eb-9748-43d4-882f-442588c3c146\n",
            "time: 2min 16s (started: 2024-03-13 03:07:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "KAj8s8GVuJPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "((7 * 9098 + 147) % 15485863) % 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcXz7TK3psjm",
        "outputId": "25be2363-da18-4d45-c4ad-4bf389f2de7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.36 ms (started: 2024-03-13 00:29:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(\n",
        "    combinations(\n",
        "        sorted(\n",
        "            (\n",
        "                (\"b1\",[2,3,4]),\n",
        "                (\"b2\",[3,4,77]),\n",
        "                (\"b3\", [-1, 67, 0]),\n",
        "                (\"b4\",[-2,3,4]),\n",
        "                (\"b5\",[3,-42,77]),\n",
        "                (\"b6\", [0, 7, -103])\n",
        "            )\n",
        "        ),\n",
        "    2)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY2KnIlnhbJp",
        "outputId": "dd344a2f-17a3-46b5-c6cc-799e1c36bc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('b1', [2, 3, 4]), ('b2', [3, 4, 77])),\n",
              " (('b1', [2, 3, 4]), ('b3', [-1, 67, 0])),\n",
              " (('b1', [2, 3, 4]), ('b4', [-2, 3, 4])),\n",
              " (('b1', [2, 3, 4]), ('b5', [3, -42, 77])),\n",
              " (('b1', [2, 3, 4]), ('b6', [0, 7, -103])),\n",
              " (('b2', [3, 4, 77]), ('b3', [-1, 67, 0])),\n",
              " (('b2', [3, 4, 77]), ('b4', [-2, 3, 4])),\n",
              " (('b2', [3, 4, 77]), ('b5', [3, -42, 77])),\n",
              " (('b2', [3, 4, 77]), ('b6', [0, 7, -103])),\n",
              " (('b3', [-1, 67, 0]), ('b4', [-2, 3, 4])),\n",
              " (('b3', [-1, 67, 0]), ('b5', [3, -42, 77])),\n",
              " (('b3', [-1, 67, 0]), ('b6', [0, 7, -103])),\n",
              " (('b4', [-2, 3, 4]), ('b5', [3, -42, 77])),\n",
              " (('b4', [-2, 3, 4]), ('b6', [0, 7, -103])),\n",
              " (('b5', [3, -42, 77]), ('b6', [0, 7, -103]))]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 16.5 ms (started: 2024-03-13 02:38:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1"
      ],
      "metadata": {
        "id": "T9XrGZweFJ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\n",
        "def prepare_dataset(data, split=\"train\"):\n",
        "    # Remove the header\n",
        "    header = data.first()\n",
        "    data = (\n",
        "        data.filter(lambda row: row != header)\n",
        "        .map(lambda row: row.split(\",\"))\n",
        "        .map(lambda row: (row[0], row[1], row[2]) if split == \"train\" else (row[0], row[1]))\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_data(data, output_file_name):\n",
        "    header = [\"user_id\", \"business_id\", \"prediction\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "\n",
        "\n",
        "def get_bus_to_usr_map(train_data):\n",
        "    # Group by business_id and collect the corresponding set of users\n",
        "    bus2user = (\n",
        "        train_data.map(lambda x: (x[1], (x[0], float(x[2]))))\n",
        "        .groupByKey()\n",
        "        .mapValues(lambda vals: {\"users\": dict(vals), \"avg_rating\": sum(val[1] for val in vals) / len(vals)})\n",
        "    )\n",
        "    return bus2user.collectAsMap()\n",
        "\n",
        "\n",
        "def get_usr_to_bus_map(train_data):\n",
        "    # Group by user_id and collect the corresponding set of businesses\n",
        "    user2bus = (\n",
        "        train_data.map(lambda x: (x[0], (x[1], float(x[2]))))\n",
        "        .groupByKey()\n",
        "        .mapValues(lambda vals: {\"business\": dict(vals)})\n",
        "    )\n",
        "    return user2bus.collectAsMap()\n",
        "\n",
        "\n",
        "def compute_pearson_similarity(data, item2user_dict):\n",
        "    \"\"\"\n",
        "    Formala: r = Σᵢ((xᵢ − mean(x))(yᵢ − mean(y))) (√Σᵢ(xᵢ − mean(x))² √Σᵢ(yᵢ − mean(y))²)⁻¹\n",
        "    \"\"\"\n",
        "    # Unpack the data\n",
        "    item1, item2 = data\n",
        "\n",
        "    # Find common user to calculate co-rated averages\n",
        "    users_item1 = set(item2user_dict[item1][\"users\"].keys())\n",
        "    users_item2 = set(item2user_dict[item2][\"users\"].keys())\n",
        "    common_users = users_item1.intersection(users_item2)\n",
        "\n",
        "    if len(common_users) <= 1:\n",
        "        similarity = (5 - abs(item2user_dict[item1][\"avg_rating\"] - item2user_dict[item2][\"avg_rating\"])) / 5\n",
        "    else:\n",
        "        r1 = []\n",
        "        r2 = []\n",
        "        # Get ratings of common users for both business\n",
        "        for usr in common_users:\n",
        "            r1.append(item2user_dict[item1][\"users\"][usr])\n",
        "            r2.append(item2user_dict[item2][\"users\"][usr])\n",
        "\n",
        "        # Center the ratings by subtracting the co-rated average rating\n",
        "        r1_bar = sum(r1) / len(r1)\n",
        "        r2_bar = sum(r2) / len(r2)\n",
        "        r1 = [r - r1_bar for r in r1]\n",
        "        r2 = [r - r2_bar for r in r2]\n",
        "\n",
        "        # Compute weight for the item pair\n",
        "        numer = sum([a * b for a, b in zip(r1, r2)])\n",
        "        denom = ((sum([a**2 for a in r1])) ** 0.5) * (sum([b**2 for b in r2]) ** 0.5)\n",
        "\n",
        "        similarity = 0 if denom == 0 else numer / denom\n",
        "\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def predict_rating(data, bus2user_dict, user2bus_dict, neighbours=15):\n",
        "    \"\"\"Perform Item-based Collaborative filtering on prepared data.\"\"\"\n",
        "    # Unpack the data\n",
        "    user, business = data\n",
        "\n",
        "    # Return avg rating if user or business is not present in the dataset\n",
        "    if user not in user2bus_dict or business not in bus2user_dict:\n",
        "        return 3.0\n",
        "\n",
        "    # Pearson similarities for rating prediction\n",
        "    pc = []\n",
        "\n",
        "    for item in user2bus_dict[user][\"business\"].keys():\n",
        "        # Compute pearson similarity for each business pair\n",
        "        similarity = compute_pearson_similarity((business, item), bus2user_dict)\n",
        "        pc.append((similarity, bus2user_dict[item][\"users\"][user]))\n",
        "\n",
        "    # Calculate the predicted rating\n",
        "    top_pc = sorted(pc, key=lambda x: -x[0])[:neighbours]\n",
        "    x, y = 0, 0\n",
        "    for p, r in top_pc:\n",
        "        x += p * r\n",
        "        y += abs(p)\n",
        "    predicted_rating = 3.5 if y == 0 else x / y\n",
        "\n",
        "    return predicted_rating\n",
        "\n",
        "\n",
        "def task2_1(train_file_name, test_file_name, output_file_name):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 2.1: Item-Based Collaborative Filtering\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Read and process the train data\n",
        "        train_data = spark.textFile(train_file_name)\n",
        "        train_data = prepare_dataset(train_data, split=\"train\")\n",
        "\n",
        "        # Preprocess train data to get mapping dictionaries\n",
        "        bus2user_dict = get_bus_to_usr_map(train_data)\n",
        "        user2bus_dict = get_usr_to_bus_map(train_data)\n",
        "\n",
        "        # Read and prepare validation data\n",
        "        val_data = spark.textFile(test_file_name)\n",
        "        val_data = prepare_dataset(val_data, split=\"valid\").cache()\n",
        "\n",
        "        val_data = val_data.map(lambda x: [x[0], x[1], predict_rating(x, bus2user_dict, user2bus_dict)]).cache()\n",
        "\n",
        "        save_data(val_data.collect(), output_file_name)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 4:\n",
        "        print(\"Usage: spark-submit task2_1.py <train_file_name> <test_file_name> <output_file_name>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Read input parameters\n",
        "    train_file_name = sys.argv[1]\n",
        "    test_file_name = sys.argv[2]\n",
        "    output_file_name = sys.argv[3]\n",
        "\n",
        "    task2_1(train_file_name, test_file_name, output_file_name)\n",
        "\n",
        "# task2_1(\"HW3StudentData/yelp_train.csv\", \"HW3StudentData/yelp_val.csv\", \"t2_1.csv\")\n"
      ],
      "metadata": {
        "id": "xL_vyguilLgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff86e11c-5afb-47fe-9c9b-54eb24d096e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 142.80510330200195\n",
            "\n",
            "time: 2min 24s (started: 2024-03-18 04:15:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ref"
      ],
      "metadata": {
        "id": "It8sjYp1SGqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Reference Code Task 2.1 { vertical-output: true, form-width: \"30%\" }\n",
        "%%writefile task2_1_ref.py\n",
        "from pyspark import SparkContext\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def item_based(bus, user):\n",
        "    if user not in user_bus_dict.keys():\n",
        "        return 3.5\n",
        "    if bus not in bus_user_dict.keys():\n",
        "        return user_avg_dict[user]\n",
        "\n",
        "    w_list = []\n",
        "\n",
        "    for bus1 in user_bus_dict[user]:\n",
        "        temp = tuple(sorted((bus1, bus)))\n",
        "        if temp in w_dict.keys():\n",
        "            w = w_dict[temp]\n",
        "        else:\n",
        "            #co-rated\n",
        "            user_inter = bus_user_dict[bus] & bus_user_dict[bus1]\n",
        "\n",
        "            if len(user_inter) <= 1:\n",
        "                w = (5.0 - abs(bus_avg_dict[bus] - bus_avg_dict[bus1])) / 5\n",
        "            elif len(user_inter) == 2:\n",
        "                user_inter = list(user_inter)\n",
        "                w1 = (5.0 - abs(float(bus_user_r_dict[bus][user_inter[0]]) - float(bus_user_r_dict[bus1][user_inter[0]]))) / 5\n",
        "                w2 = (5.0 - abs(float(bus_user_r_dict[bus][user_inter[1]]) - float(bus_user_r_dict[bus1][user_inter[1]]))) / 5\n",
        "                w = (w1 + w2) / 2\n",
        "            else:\n",
        "                r1 = []\n",
        "                r2 = []\n",
        "                for user1 in user_inter:\n",
        "                    r1.append(float(bus_user_r_dict[bus][user1]))\n",
        "                    r2.append(float(bus_user_r_dict[bus1][user1]))\n",
        "                avg1 = sum(r1) / len(r1)\n",
        "                avg2 = sum(r2) / len(r2)\n",
        "                temp1 = [x - avg1 for x in r1]\n",
        "                temp2 = [x - avg2 for x in r2]\n",
        "                X = (sum([x * y for x,y in zip(temp1, temp2)]))\n",
        "                Y = ((sum([x ** 2 for x in temp1])**(1/2)) * (sum([x ** 2 for x in temp2])**(1/2)))\n",
        "                if Y == 0:\n",
        "                    w = 0\n",
        "                else:\n",
        "                    w = X / Y\n",
        "            w_dict[temp] = w\n",
        "        w_list.append((w, float(bus_user_r_dict[bus1][user])))\n",
        "    w_list_can = sorted(w_list, key=lambda x: -x[0])[:15]\n",
        "    X = 0\n",
        "    Y = 0\n",
        "    for w, r in w_list_can:\n",
        "        X += (w * r)\n",
        "        Y += abs(w)\n",
        "    if Y == 0:\n",
        "        return 3.5\n",
        "    else:\n",
        "        return X / Y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_path = sys.argv[1]\n",
        "    val_path = sys.argv[2]\n",
        "    output_path = sys.argv[3]\n",
        "\n",
        "    s_t = time.time()\n",
        "\n",
        "    spark = SparkContext(appName= \"task2_1\")\n",
        "    #train data\n",
        "    lines_train = spark.textFile(train_path)\n",
        "    first_train = lines_train.first()\n",
        "    lines_train = lines_train.filter(lambda row: row != first_train).map(lambda row: row.split(\",\")).map(lambda row: (row[1], row[0], row[2]))\n",
        "\n",
        "    bus_user_train = lines_train.map(lambda row: (row[0], row[1])).groupByKey().mapValues(set)\n",
        "    bus_user_dict = {}\n",
        "    for bus, users in bus_user_train.collect():\n",
        "        bus_user_dict[bus] = users\n",
        "\n",
        "    user_bus_train = lines_train.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set)\n",
        "    user_bus_dict = {}\n",
        "    for user, bus in user_bus_train.collect():\n",
        "        user_bus_dict[user] = bus\n",
        "\n",
        "    #bus_mid = lines_train.map(lambda row: (row[0], float(row[2]))).groupByKey().mapValues(list).map(lambda x: (x[0], sorted(x[1]))).map(lambda x: (x[0], x[1][len(x[1]) // 2]))\n",
        "    bus_avg = lines_train.map(lambda row: (row[0], float(row[2]))).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]) / len(x[1])))\n",
        "    bus_avg_dict = {}\n",
        "    for bus, rating in bus_avg.collect():\n",
        "        bus_avg_dict[bus] = rating\n",
        "\n",
        "    #user_mid = lines_train.map(lambda row: (row[1], float(row[2]))).groupByKey().mapValues(list).map(lambda x: (x[0], sorted(x[1]))).map(lambda x: (x[0], x[1][len(x[1]) // 2]))\n",
        "    user_avg = lines_train.map(lambda row: (row[1], float(row[2]))).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]) / len(x[1])))\n",
        "    user_avg_dict = {}\n",
        "    for user, rating in user_avg.collect():\n",
        "        user_avg_dict[user] = rating\n",
        "\n",
        "    bus_user_r = lines_train.map(lambda row: (row[0], (row[1], row[2]))).groupByKey().mapValues(set)\n",
        "    bus_user_r_dict = {}\n",
        "    for bus, user_r_set in bus_user_r.collect():\n",
        "        temp = {}\n",
        "        for user_r in user_r_set:\n",
        "            temp[user_r[0]] = user_r[1]\n",
        "        bus_user_r_dict[bus] = temp\n",
        "    #print(bus_user_r_dict)\n",
        "\n",
        "    #val data\n",
        "    lines_val = spark.textFile(val_path)\n",
        "    first_val = lines_val.first()\n",
        "    lines_val = lines_val.filter(lambda row: row != first_val).map(lambda row: row.split(\",\")).map(lambda row: (row[1], row[0]))\n",
        "    # (bus1, bus2): {simi}\n",
        "    w_dict = {}\n",
        "\n",
        "    result_str = \"user_id, business_id, prediction\\n\"\n",
        "    for row in lines_val.collect():\n",
        "        prediction = item_based(row[0], row[1])\n",
        "        result_str += row[1] + \",\" + row[0] + \",\" + str(prediction) + \"\\n\"\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.writelines(result_str)\n",
        "\n",
        "    e_t = time.time()\n",
        "    print('Duration: ', e_t - s_t)\n",
        "\n",
        "\n",
        "    #RMSE: 1.0475857031155809"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "gFuww353SJD4",
        "outputId": "327b0c14-1a26-47da-f1b1-9124b0034663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task2_1_ref.py\n",
            "time: 14.4 ms (started: 2024-03-15 08:45:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit task2_1_ref.py HW3StudentData/yelp_train.csv HW3StudentData/yelp_val.csv t2-ref.txt --executor-memory 4G --driver-memory 4G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FktHQ_ZFSZBZ",
        "outputId": "9128a7f6-616b-492a-b032-f8c8a57fce18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/03/16 01:52:26 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/03/16 01:52:26 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "24/03/16 01:52:26 INFO SparkContext: Java version 11.0.22\n",
            "24/03/16 01:52:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/03/16 01:52:26 INFO ResourceUtils: ==============================================================\n",
            "24/03/16 01:52:26 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/03/16 01:52:26 INFO ResourceUtils: ==============================================================\n",
            "24/03/16 01:52:26 INFO SparkContext: Submitted application: task2_1\n",
            "24/03/16 01:52:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/03/16 01:52:26 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/03/16 01:52:26 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/03/16 01:52:26 INFO SecurityManager: Changing view acls to: root\n",
            "24/03/16 01:52:26 INFO SecurityManager: Changing modify acls to: root\n",
            "24/03/16 01:52:26 INFO SecurityManager: Changing view acls groups to: \n",
            "24/03/16 01:52:26 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/03/16 01:52:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/03/16 01:52:27 INFO Utils: Successfully started service 'sparkDriver' on port 46639.\n",
            "24/03/16 01:52:28 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/03/16 01:52:28 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/03/16 01:52:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/03/16 01:52:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/03/16 01:52:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/03/16 01:52:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6230cefb-c038-4cdc-813e-93af5650c218\n",
            "24/03/16 01:52:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/03/16 01:52:28 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/03/16 01:52:28 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/03/16 01:52:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/03/16 01:52:28 INFO Executor: Starting executor ID driver on host c0f3f75979b7\n",
            "24/03/16 01:52:28 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "24/03/16 01:52:28 INFO Executor: Java version 11.0.22\n",
            "24/03/16 01:52:28 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/03/16 01:52:28 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3b2145e7 for default.\n",
            "24/03/16 01:52:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33081.\n",
            "24/03/16 01:52:28 INFO NettyBlockTransferService: Server created on c0f3f75979b7:33081\n",
            "24/03/16 01:52:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/03/16 01:52:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c0f3f75979b7, 33081, None)\n",
            "24/03/16 01:52:28 INFO BlockManagerMasterEndpoint: Registering block manager c0f3f75979b7:33081 with 434.4 MiB RAM, BlockManagerId(driver, c0f3f75979b7, 33081, None)\n",
            "24/03/16 01:52:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c0f3f75979b7, 33081, None)\n",
            "24/03/16 01:52:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c0f3f75979b7, 33081, None)\n",
            "24/03/16 01:52:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
            "24/03/16 01:52:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
            "24/03/16 01:52:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c0f3f75979b7:33081 (size: 32.6 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:30 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/03/16 01:52:30 INFO FileInputFormat: Total input files to process : 1\n",
            "24/03/16 01:52:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
            "24/03/16 01:52:30 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
            "24/03/16 01:52:30 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:181)\n",
            "24/03/16 01:52:30 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/03/16 01:52:30 INFO DAGScheduler: Missing parents: List()\n",
            "24/03/16 01:52:30 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "24/03/16 01:52:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c0f3f75979b7:33081 (size: 4.8 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "24/03/16 01:52:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "24/03/16 01:52:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7716 bytes) \n",
            "24/03/16 01:52:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/03/16 01:52:31 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/16 01:52:32 INFO PythonRunner: Times: total = 1232, boot = 861, init = 370, finish = 1\n",
            "24/03/16 01:52:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1521 bytes result sent to driver\n",
            "24/03/16 01:52:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1750 ms on c0f3f75979b7 (executor driver) (1/1)\n",
            "24/03/16 01:52:32 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36097\n",
            "24/03/16 01:52:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:32 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:181) finished in 2.041 s\n",
            "24/03/16 01:52:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:52:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "24/03/16 01:52:32 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:181, took 2.159952 s\n",
            "24/03/16 01:52:33 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:73\n",
            "24/03/16 01:52:33 INFO DAGScheduler: Registering RDD 4 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:71) as input to shuffle 0\n",
            "24/03/16 01:52:33 INFO DAGScheduler: Got job 1 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:73) with 2 output partitions\n",
            "24/03/16 01:52:33 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:73)\n",
            "24/03/16 01:52:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "24/03/16 01:52:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "24/03/16 01:52:33 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:71), which has no missing parents\n",
            "24/03/16 01:52:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c0f3f75979b7:33081 (size: 8.0 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:33 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:71) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:33 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (c0f3f75979b7, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "24/03/16 01:52:33 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "24/03/16 01:52:33 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/16 01:52:33 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/16 01:52:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c0f3f75979b7:33081 in memory (size: 4.8 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:36 INFO PythonRunner: Times: total = 3508, boot = 20, init = 662, finish = 2826\n",
            "24/03/16 01:52:36 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1667 bytes result sent to driver\n",
            "24/03/16 01:52:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3680 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:36 INFO PythonRunner: Times: total = 3622, boot = 23, init = 671, finish = 2928\n",
            "24/03/16 01:52:37 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1667 bytes result sent to driver\n",
            "24/03/16 01:52:37 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 3754 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:37 INFO DAGScheduler: ShuffleMapStage 1 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:71) finished in 3.838 s\n",
            "24/03/16 01:52:37 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/16 01:52:37 INFO DAGScheduler: running: Set()\n",
            "24/03/16 01:52:37 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "24/03/16 01:52:37 INFO DAGScheduler: failed: Set()\n",
            "24/03/16 01:52:37 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:73), which has no missing parents\n",
            "24/03/16 01:52:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c0f3f75979b7:33081 (size: 6.4 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:37 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:73) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (c0f3f75979b7, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:37 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (c0f3f75979b7, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)\n",
            "24/03/16 01:52:37 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)\n",
            "24/03/16 01:52:37 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:37 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "24/03/16 01:52:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/03/16 01:52:37 INFO PythonRunner: Times: total = 568, boot = -256, init = 539, finish = 285\n",
            "24/03/16 01:52:37 INFO PythonRunner: Times: total = 600, boot = -143, init = 451, finish = 292\n",
            "24/03/16 01:52:37 INFO MemoryStore: Block taskresult_4 stored as bytes in memory (estimated size 5.9 MiB, free 428.2 MiB)\n",
            "24/03/16 01:52:37 INFO BlockManagerInfo: Added taskresult_4 in memory on c0f3f75979b7:33081 (size: 5.9 MiB, free: 428.5 MiB)\n",
            "24/03/16 01:52:37 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 6166238 bytes result sent via BlockManager)\n",
            "24/03/16 01:52:37 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 5.8 MiB, free 422.4 MiB)\n",
            "24/03/16 01:52:37 INFO BlockManagerInfo: Added taskresult_3 in memory on c0f3f75979b7:33081 (size: 5.8 MiB, free: 422.7 MiB)\n",
            "24/03/16 01:52:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 6065056 bytes result sent via BlockManager)\n",
            "24/03/16 01:52:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on c0f3f75979b7:33081 in memory (size: 8.0 KiB, free: 422.7 MiB)\n",
            "24/03/16 01:52:38 INFO TransportClientFactory: Successfully created connection to c0f3f75979b7/172.28.0.12:33081 after 95 ms (0 ms spent in bootstraps)\n",
            "24/03/16 01:52:38 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 1150 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:38 INFO BlockManagerInfo: Removed taskresult_4 on c0f3f75979b7:33081 in memory (size: 5.9 MiB, free: 428.6 MiB)\n",
            "24/03/16 01:52:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 1235 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:38 INFO DAGScheduler: ResultStage 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:73) finished in 1.262 s\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:52:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Job 1 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:73, took 5.187108 s\n",
            "24/03/16 01:52:38 INFO BlockManagerInfo: Removed taskresult_3 on c0f3f75979b7:33081 in memory (size: 5.8 MiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:38 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:78\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Registering RDD 9 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:76) as input to shuffle 1\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Got job 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:78) with 2 output partitions\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:78)\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Submitting ShuffleMapStage 3 (PairwiseRDD[9] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:76), which has no missing parents\n",
            "24/03/16 01:52:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:38 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c0f3f75979b7:33081 (size: 8.0 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:38 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (PairwiseRDD[9] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:76) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:39 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6) (c0f3f75979b7, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:39 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "24/03/16 01:52:39 INFO Executor: Running task 1.0 in stage 3.0 (TID 6)\n",
            "24/03/16 01:52:39 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/16 01:52:39 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/16 01:52:43 INFO PythonRunner: Times: total = 4362, boot = -1306, init = 2225, finish = 3443\n",
            "24/03/16 01:52:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1667 bytes result sent to driver\n",
            "24/03/16 01:52:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 4559 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:43 INFO PythonRunner: Times: total = 4549, boot = -1319, init = 2251, finish = 3617\n",
            "24/03/16 01:52:43 INFO Executor: Finished task 1.0 in stage 3.0 (TID 6). 1624 bytes result sent to driver\n",
            "24/03/16 01:52:43 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 4694 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:43 INFO DAGScheduler: ShuffleMapStage 3 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:76) finished in 4.737 s\n",
            "24/03/16 01:52:43 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/16 01:52:43 INFO DAGScheduler: running: Set()\n",
            "24/03/16 01:52:43 INFO DAGScheduler: waiting: Set(ResultStage 4)\n",
            "24/03/16 01:52:43 INFO DAGScheduler: failed: Set()\n",
            "24/03/16 01:52:43 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:78), which has no missing parents\n",
            "24/03/16 01:52:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 11.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c0f3f75979b7:33081 (size: 6.4 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:52:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:78) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:43 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (c0f3f75979b7, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:43 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8) (c0f3f75979b7, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:43 INFO Executor: Running task 0.0 in stage 4.0 (TID 7)\n",
            "24/03/16 01:52:43 INFO Executor: Running task 1.0 in stage 4.0 (TID 8)\n",
            "24/03/16 01:52:43 INFO ShuffleBlockFetcherIterator: Getting 2 (5.5 MiB) non-empty blocks including 2 (5.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "24/03/16 01:52:43 INFO ShuffleBlockFetcherIterator: Getting 2 (5.5 MiB) non-empty blocks including 2 (5.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
            "24/03/16 01:52:44 INFO PythonRunner: Times: total = 515, boot = -74, init = 362, finish = 227\n",
            "24/03/16 01:52:44 INFO PythonRunner: Times: total = 521, boot = -172, init = 458, finish = 235\n",
            "24/03/16 01:52:44 INFO MemoryStore: Block taskresult_8 stored as bytes in memory (estimated size 5.7 MiB, free 428.4 MiB)\n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Added taskresult_8 in memory on c0f3f75979b7:33081 (size: 5.7 MiB, free: 428.7 MiB)\n",
            "24/03/16 01:52:44 INFO MemoryStore: Block taskresult_7 stored as bytes in memory (estimated size 5.6 MiB, free 422.8 MiB)\n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Added taskresult_7 in memory on c0f3f75979b7:33081 (size: 5.6 MiB, free: 423.1 MiB)\n",
            "24/03/16 01:52:44 INFO Executor: Finished task 0.0 in stage 4.0 (TID 7). 5844075 bytes result sent via BlockManager)\n",
            "24/03/16 01:52:44 INFO Executor: Finished task 1.0 in stage 4.0 (TID 8). 5967915 bytes result sent via BlockManager)\n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on c0f3f75979b7:33081 in memory (size: 8.0 KiB, free: 423.1 MiB)\n",
            "24/03/16 01:52:44 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 710 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Removed taskresult_8 on c0f3f75979b7:33081 in memory (size: 5.7 MiB, free: 428.8 MiB)\n",
            "24/03/16 01:52:44 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 724 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:44 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Removed taskresult_7 on c0f3f75979b7:33081 in memory (size: 5.6 MiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:44 INFO DAGScheduler: ResultStage 4 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:78) finished in 0.744 s\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:52:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Job 2 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:78, took 5.521663 s\n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on c0f3f75979b7:33081 in memory (size: 6.4 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:44 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:84\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Registering RDD 14 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:82) as input to shuffle 2\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Got job 3 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:84) with 2 output partitions\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:84)\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[14] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:82), which has no missing parents\n",
            "24/03/16 01:52:44 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:44 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c0f3f75979b7:33081 (size: 8.0 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:44 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:44 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[14] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:82) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:44 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:44 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 9) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:44 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 10) (c0f3f75979b7, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:44 INFO Executor: Running task 1.0 in stage 5.0 (TID 10)\n",
            "24/03/16 01:52:44 INFO Executor: Running task 0.0 in stage 5.0 (TID 9)\n",
            "24/03/16 01:52:44 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/16 01:52:44 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/16 01:52:44 INFO BlockManagerInfo: Removed broadcast_5_piece0 on c0f3f75979b7:33081 in memory (size: 6.4 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:47 INFO PythonRunner: Times: total = 2341, boot = -599, init = 1069, finish = 1871\n",
            "24/03/16 01:52:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 9). 1624 bytes result sent to driver\n",
            "24/03/16 01:52:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 9) in 2465 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:47 INFO PythonRunner: Times: total = 2406, boot = -614, init = 1006, finish = 2014\n",
            "24/03/16 01:52:47 INFO Executor: Finished task 1.0 in stage 5.0 (TID 10). 1667 bytes result sent to driver\n",
            "24/03/16 01:52:47 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 10) in 2534 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:47 INFO DAGScheduler: ShuffleMapStage 5 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:82) finished in 2.561 s\n",
            "24/03/16 01:52:47 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/16 01:52:47 INFO DAGScheduler: running: Set()\n",
            "24/03/16 01:52:47 INFO DAGScheduler: waiting: Set(ResultStage 6)\n",
            "24/03/16 01:52:47 INFO DAGScheduler: failed: Set()\n",
            "24/03/16 01:52:47 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[17] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:84), which has no missing parents\n",
            "24/03/16 01:52:47 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.6 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:47 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:47 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on c0f3f75979b7:33081 (size: 6.6 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:47 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[17] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:84) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:47 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:47 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 11) (c0f3f75979b7, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:47 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 12) (c0f3f75979b7, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:47 INFO Executor: Running task 0.0 in stage 6.0 (TID 11)\n",
            "24/03/16 01:52:47 INFO Executor: Running task 1.0 in stage 6.0 (TID 12)\n",
            "24/03/16 01:52:47 INFO ShuffleBlockFetcherIterator: Getting 2 (1218.2 KiB) non-empty blocks including 2 (1218.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "24/03/16 01:52:47 INFO ShuffleBlockFetcherIterator: Getting 2 (1218.2 KiB) non-empty blocks including 2 (1218.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "24/03/16 01:52:48 INFO PythonRunner: Times: total = 727, boot = -129, init = 526, finish = 330\n",
            "24/03/16 01:52:48 INFO Executor: Finished task 0.0 in stage 6.0 (TID 11). 446106 bytes result sent to driver\n",
            "24/03/16 01:52:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 11) in 772 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:48 INFO PythonRunner: Times: total = 736, boot = -80, init = 457, finish = 359\n",
            "24/03/16 01:52:48 INFO Executor: Finished task 1.0 in stage 6.0 (TID 12). 453727 bytes result sent to driver\n",
            "24/03/16 01:52:48 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 12) in 815 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:48 INFO DAGScheduler: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:84) finished in 0.852 s\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:52:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Job 3 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:84, took 3.453761 s\n",
            "24/03/16 01:52:48 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:90\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Registering RDD 19 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:88) as input to shuffle 3\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Got job 4 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:90) with 2 output partitions\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Final stage: ResultStage 8 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:90)\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Submitting ShuffleMapStage 7 (PairwiseRDD[19] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:88), which has no missing parents\n",
            "24/03/16 01:52:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on c0f3f75979b7:33081 (size: 8.0 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:52:48 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (PairwiseRDD[19] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:88) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:48 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:48 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 13) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:48 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 14) (c0f3f75979b7, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:48 INFO Executor: Running task 0.0 in stage 7.0 (TID 13)\n",
            "24/03/16 01:52:48 INFO Executor: Running task 1.0 in stage 7.0 (TID 14)\n",
            "24/03/16 01:52:48 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/16 01:52:48 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/16 01:52:49 INFO BlockManagerInfo: Removed broadcast_7_piece0 on c0f3f75979b7:33081 in memory (size: 6.6 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:52 INFO PythonRunner: Times: total = 3881, boot = -339, init = 1289, finish = 2931\n",
            "24/03/16 01:52:52 INFO Executor: Finished task 1.0 in stage 7.0 (TID 14). 1624 bytes result sent to driver\n",
            "24/03/16 01:52:52 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 14) in 4022 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:52 INFO PythonRunner: Times: total = 3956, boot = -390, init = 1308, finish = 3038\n",
            "24/03/16 01:52:52 INFO Executor: Finished task 0.0 in stage 7.0 (TID 13). 1624 bytes result sent to driver\n",
            "24/03/16 01:52:52 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 13) in 4088 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:52 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:52 INFO DAGScheduler: ShuffleMapStage 7 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:88) finished in 4.116 s\n",
            "24/03/16 01:52:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/16 01:52:52 INFO DAGScheduler: running: Set()\n",
            "24/03/16 01:52:52 INFO DAGScheduler: waiting: Set(ResultStage 8)\n",
            "24/03/16 01:52:52 INFO DAGScheduler: failed: Set()\n",
            "24/03/16 01:52:52 INFO DAGScheduler: Submitting ResultStage 8 (PythonRDD[22] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:90), which has no missing parents\n",
            "24/03/16 01:52:52 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.6 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:52 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:52 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on c0f3f75979b7:33081 (size: 6.6 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:52:52 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (PythonRDD[22] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:90) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:52 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:52 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15) (c0f3f75979b7, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:52 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 16) (c0f3f75979b7, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:52 INFO Executor: Running task 0.0 in stage 8.0 (TID 15)\n",
            "24/03/16 01:52:52 INFO Executor: Running task 1.0 in stage 8.0 (TID 16)\n",
            "24/03/16 01:52:52 INFO ShuffleBlockFetcherIterator: Getting 2 (832.0 KiB) non-empty blocks including 2 (832.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "24/03/16 01:52:52 INFO ShuffleBlockFetcherIterator: Getting 2 (832.0 KiB) non-empty blocks including 2 (832.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/03/16 01:52:53 INFO PythonRunner: Times: total = 336, boot = -34, init = 277, finish = 93\n",
            "24/03/16 01:52:53 INFO Executor: Finished task 0.0 in stage 8.0 (TID 15). 205907 bytes result sent to driver\n",
            "24/03/16 01:52:53 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 378 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:53 INFO PythonRunner: Times: total = 378, boot = -104, init = 371, finish = 111\n",
            "24/03/16 01:52:53 INFO Executor: Finished task 1.0 in stage 8.0 (TID 16). 206558 bytes result sent to driver\n",
            "24/03/16 01:52:53 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 16) in 411 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:53 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:53 INFO DAGScheduler: ResultStage 8 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:90) finished in 0.430 s\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:52:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Job 4 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:90, took 4.566167 s\n",
            "24/03/16 01:52:53 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:95\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Registering RDD 24 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:93) as input to shuffle 4\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Got job 5 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:95) with 2 output partitions\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Final stage: ResultStage 10 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:95)\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Submitting ShuffleMapStage 9 (PairwiseRDD[24] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:93), which has no missing parents\n",
            "24/03/16 01:52:53 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:53 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:53 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on c0f3f75979b7:33081 (size: 8.0 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:52:53 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:53 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (PairwiseRDD[24] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:93) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:53 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:53 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 17) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:53 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 18) (c0f3f75979b7, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/16 01:52:53 INFO Executor: Running task 1.0 in stage 9.0 (TID 18)\n",
            "24/03/16 01:52:53 INFO Executor: Running task 0.0 in stage 9.0 (TID 17)\n",
            "24/03/16 01:52:53 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/16 01:52:53 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/16 01:52:53 INFO BlockManagerInfo: Removed broadcast_8_piece0 on c0f3f75979b7:33081 in memory (size: 8.0 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:52:53 INFO BlockManagerInfo: Removed broadcast_9_piece0 on c0f3f75979b7:33081 in memory (size: 6.6 KiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:57 INFO PythonRunner: Times: total = 4183, boot = -119, init = 699, finish = 3603\n",
            "24/03/16 01:52:57 INFO Executor: Finished task 0.0 in stage 9.0 (TID 17). 1624 bytes result sent to driver\n",
            "24/03/16 01:52:57 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 17) in 4286 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:57 INFO PythonRunner: Times: total = 4277, boot = -172, init = 732, finish = 3717\n",
            "24/03/16 01:52:57 INFO Executor: Finished task 1.0 in stage 9.0 (TID 18). 1624 bytes result sent to driver\n",
            "24/03/16 01:52:57 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 18) in 4364 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:57 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:57 INFO DAGScheduler: ShuffleMapStage 9 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:93) finished in 4.380 s\n",
            "24/03/16 01:52:57 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/16 01:52:57 INFO DAGScheduler: running: Set()\n",
            "24/03/16 01:52:57 INFO DAGScheduler: waiting: Set(ResultStage 10)\n",
            "24/03/16 01:52:57 INFO DAGScheduler: failed: Set()\n",
            "24/03/16 01:52:57 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[27] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:95), which has no missing parents\n",
            "24/03/16 01:52:57 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.0 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:57 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "24/03/16 01:52:57 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on c0f3f75979b7:33081 (size: 6.4 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:52:57 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:52:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (PythonRDD[27] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:95) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:52:57 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:52:57 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 19) (c0f3f75979b7, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:57 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 20) (c0f3f75979b7, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/16 01:52:57 INFO Executor: Running task 1.0 in stage 10.0 (TID 20)\n",
            "24/03/16 01:52:57 INFO Executor: Running task 0.0 in stage 10.0 (TID 19)\n",
            "24/03/16 01:52:57 INFO ShuffleBlockFetcherIterator: Getting 2 (6.6 MiB) non-empty blocks including 2 (6.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms\n",
            "24/03/16 01:52:57 INFO ShuffleBlockFetcherIterator: Getting 2 (6.6 MiB) non-empty blocks including 2 (6.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/16 01:52:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
            "24/03/16 01:52:58 INFO PythonRunner: Times: total = 1058, boot = -57, init = 530, finish = 585\n",
            "24/03/16 01:52:58 INFO MemoryStore: Block taskresult_19 stored as bytes in memory (estimated size 7.5 MiB, free 426.6 MiB)\n",
            "24/03/16 01:52:58 INFO BlockManagerInfo: Added taskresult_19 in memory on c0f3f75979b7:33081 (size: 7.5 MiB, free: 426.8 MiB)\n",
            "24/03/16 01:52:58 INFO Executor: Finished task 0.0 in stage 10.0 (TID 19). 7882932 bytes result sent via BlockManager)\n",
            "24/03/16 01:52:58 INFO BlockManagerInfo: Removed broadcast_10_piece0 on c0f3f75979b7:33081 in memory (size: 8.0 KiB, free: 426.8 MiB)\n",
            "24/03/16 01:52:58 INFO PythonRunner: Times: total = 1122, boot = -138, init = 615, finish = 645\n",
            "24/03/16 01:52:58 INFO MemoryStore: Block taskresult_20 stored as bytes in memory (estimated size 7.6 MiB, free 419.0 MiB)\n",
            "24/03/16 01:52:58 INFO BlockManagerInfo: Added taskresult_20 in memory on c0f3f75979b7:33081 (size: 7.6 MiB, free: 419.2 MiB)\n",
            "24/03/16 01:52:58 INFO Executor: Finished task 1.0 in stage 10.0 (TID 20). 8014441 bytes result sent via BlockManager)\n",
            "24/03/16 01:52:58 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 19) in 1256 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:52:58 INFO BlockManagerInfo: Removed taskresult_19 on c0f3f75979b7:33081 in memory (size: 7.5 MiB, free: 426.7 MiB)\n",
            "24/03/16 01:52:58 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 20) in 1342 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:52:58 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:52:58 INFO DAGScheduler: ResultStage 10 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:95) finished in 1.362 s\n",
            "24/03/16 01:52:58 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:52:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "24/03/16 01:52:58 INFO DAGScheduler: Job 5 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:95, took 5.764721 s\n",
            "24/03/16 01:52:58 INFO BlockManagerInfo: Removed taskresult_20 on c0f3f75979b7:33081 in memory (size: 7.6 MiB, free: 434.4 MiB)\n",
            "24/03/16 01:52:59 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 221.5 KiB, free 433.9 MiB)\n",
            "24/03/16 01:52:59 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
            "24/03/16 01:52:59 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on c0f3f75979b7:33081 (size: 32.6 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:52:59 INFO SparkContext: Created broadcast 12 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/03/16 01:53:00 INFO FileInputFormat: Total input files to process : 1\n",
            "24/03/16 01:53:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Got job 6 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Final stage: ResultStage 11 (runJob at PythonRDD.scala:181)\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Missing parents: List()\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[30] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "24/03/16 01:53:00 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
            "24/03/16 01:53:00 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 433.9 MiB)\n",
            "24/03/16 01:53:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on c0f3f75979b7:33081 (size: 4.8 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:53:00 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (PythonRDD[30] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "24/03/16 01:53:00 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "24/03/16 01:53:00 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 21) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes) \n",
            "24/03/16 01:53:00 INFO Executor: Running task 0.0 in stage 11.0 (TID 21)\n",
            "24/03/16 01:53:00 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_val.csv:0+3551113\n",
            "24/03/16 01:53:00 INFO BlockManagerInfo: Removed broadcast_11_piece0 on c0f3f75979b7:33081 in memory (size: 6.4 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:53:00 INFO BlockManagerInfo: Removed broadcast_6_piece0 on c0f3f75979b7:33081 in memory (size: 8.0 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:53:00 INFO PythonRunner: Times: total = 213, boot = -1413, init = 1626, finish = 0\n",
            "24/03/16 01:53:00 INFO Executor: Finished task 0.0 in stage 11.0 (TID 21). 1435 bytes result sent to driver\n",
            "24/03/16 01:53:00 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 21) in 293 ms on c0f3f75979b7 (executor driver) (1/1)\n",
            "24/03/16 01:53:00 INFO DAGScheduler: ResultStage 11 (runJob at PythonRDD.scala:181) finished in 0.316 s\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:53:00 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:53:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:181, took 0.325856 s\n",
            "24/03/16 01:53:00 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:110\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Got job 7 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:110) with 2 output partitions\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Final stage: ResultStage 12 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:110)\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Missing parents: List()\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Submitting ResultStage 12 (PythonRDD[31] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:110), which has no missing parents\n",
            "24/03/16 01:53:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.7 KiB, free 433.9 MiB)\n",
            "24/03/16 01:53:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.9 MiB)\n",
            "24/03/16 01:53:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on c0f3f75979b7:33081 (size: 5.2 KiB, free: 434.3 MiB)\n",
            "24/03/16 01:53:00 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/16 01:53:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (PythonRDD[31] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:110) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/16 01:53:00 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0\n",
            "24/03/16 01:53:00 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 22) (c0f3f75979b7, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes) \n",
            "24/03/16 01:53:00 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 23) (c0f3f75979b7, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes) \n",
            "24/03/16 01:53:00 INFO Executor: Running task 1.0 in stage 12.0 (TID 23)\n",
            "24/03/16 01:53:00 INFO Executor: Running task 0.0 in stage 12.0 (TID 22)\n",
            "24/03/16 01:53:00 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_val.csv:3551113+3551113\n",
            "24/03/16 01:53:00 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_val.csv:0+3551113\n",
            "24/03/16 01:53:01 INFO PythonRunner: Times: total = 880, boot = -1611, init = 1916, finish = 575\n",
            "24/03/16 01:53:01 INFO MemoryStore: Block taskresult_23 stored as bytes in memory (estimated size 3.5 MiB, free 430.3 MiB)\n",
            "24/03/16 01:53:01 INFO BlockManagerInfo: Added taskresult_23 in memory on c0f3f75979b7:33081 (size: 3.5 MiB, free: 430.8 MiB)\n",
            "24/03/16 01:53:01 INFO Executor: Finished task 1.0 in stage 12.0 (TID 23). 3714232 bytes result sent via BlockManager)\n",
            "24/03/16 01:53:01 INFO PythonRunner: Times: total = 857, boot = 16, init = 423, finish = 418\n",
            "24/03/16 01:53:01 INFO MemoryStore: Block taskresult_22 stored as bytes in memory (estimated size 3.5 MiB, free 426.8 MiB)\n",
            "24/03/16 01:53:01 INFO BlockManagerInfo: Added taskresult_22 in memory on c0f3f75979b7:33081 (size: 3.5 MiB, free: 427.2 MiB)\n",
            "24/03/16 01:53:01 INFO Executor: Finished task 0.0 in stage 12.0 (TID 22). 3714232 bytes result sent via BlockManager)\n",
            "24/03/16 01:53:01 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 23) in 963 ms on c0f3f75979b7 (executor driver) (1/2)\n",
            "24/03/16 01:53:01 INFO BlockManagerInfo: Removed broadcast_13_piece0 on c0f3f75979b7:33081 in memory (size: 4.8 KiB, free: 427.2 MiB)\n",
            "24/03/16 01:53:01 INFO BlockManagerInfo: Removed taskresult_23 on c0f3f75979b7:33081 in memory (size: 3.5 MiB, free: 430.8 MiB)\n",
            "24/03/16 01:53:01 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 22) in 996 ms on c0f3f75979b7 (executor driver) (2/2)\n",
            "24/03/16 01:53:01 INFO DAGScheduler: ResultStage 12 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:110) finished in 1.006 s\n",
            "24/03/16 01:53:01 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/16 01:53:01 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "24/03/16 01:53:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "24/03/16 01:53:01 INFO DAGScheduler: Job 7 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task2_1_ref.py:110, took 1.015338 s\n",
            "24/03/16 01:53:01 INFO BlockManagerInfo: Removed taskresult_22 on c0f3f75979b7:33081 in memory (size: 3.5 MiB, free: 434.3 MiB)\n",
            "Duration:  115.69928741455078\n",
            "24/03/16 01:54:22 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/03/16 01:54:22 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/03/16 01:54:22 INFO SparkUI: Stopped Spark web UI at http://c0f3f75979b7:4040\n",
            "24/03/16 01:54:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/03/16 01:54:22 INFO MemoryStore: MemoryStore cleared\n",
            "24/03/16 01:54:22 INFO BlockManager: BlockManager stopped\n",
            "24/03/16 01:54:22 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/03/16 01:54:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/03/16 01:54:22 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/03/16 01:54:22 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/03/16 01:54:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-05455b2c-51b0-4596-935e-e09f935b81c4\n",
            "24/03/16 01:54:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-05455b2c-51b0-4596-935e-e09f935b81c4/pyspark-aff5c838-f441-45ee-b837-e3a639636bb0\n",
            "24/03/16 01:54:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-5548fadd-fa01-4967-9518-86dff81439fe\n",
            "time: 2min 3s (started: 2024-03-16 01:52:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "3wWEhSPDH_px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"HW3StudentData/yelp_val.csv\")\n",
        "# df.loc[df[\"user_id\"]==\"wf1GqnKQuvH-V3QN80UOOQ\"]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "J2ez1ccXIBmd",
        "outputId": "4d0aabc2-8edb-43e6-bae9-7654c756fe3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       user_id             business_id  stars\n",
              "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg    5.0\n",
              "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ    5.0\n",
              "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA    4.0\n",
              "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA    5.0\n",
              "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw    3.0\n",
              "...                        ...                     ...    ...\n",
              "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q    2.0\n",
              "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA    4.0\n",
              "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg    5.0\n",
              "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA    3.0\n",
              "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg    4.0\n",
              "\n",
              "[142044 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dab7a405-37e2-4610-b6b2-1da07fc9af82\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
              "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
              "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
              "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
              "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
              "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142039</th>\n",
              "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
              "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142040</th>\n",
              "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
              "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142041</th>\n",
              "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
              "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142042</th>\n",
              "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
              "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142043</th>\n",
              "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
              "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142044 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dab7a405-37e2-4610-b6b2-1da07fc9af82')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dab7a405-37e2-4610-b6b2-1da07fc9af82 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dab7a405-37e2-4610-b6b2-1da07fc9af82');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-447aff39-0d1a-4177-ad0e-2682635daa0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-447aff39-0d1a-4177-ad0e-2682635daa0c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-447aff39-0d1a-4177-ad0e-2682635daa0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 255 ms (started: 2024-03-17 22:42:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"t2_1.csv\")\n",
        "# df.loc[df[\"user_id\"]==\"wf1GqnKQuvH-V3QN80UOOQ\"]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "prZyWWZawLYr",
        "outputId": "12587df3-e526-42fa-c770-f73afd17c824"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       user_id             business_id  prediction\n",
              "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg    4.467539\n",
              "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ    4.731460\n",
              "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA    4.344990\n",
              "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA    4.746280\n",
              "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw    2.996730\n",
              "...                        ...                     ...         ...\n",
              "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q    3.336156\n",
              "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA    2.887312\n",
              "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg    3.662905\n",
              "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA    3.800393\n",
              "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg    3.999940\n",
              "\n",
              "[142044 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c72ee0e6-db05-4704-860c-c0aed50fba14\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
              "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
              "      <td>4.467539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
              "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
              "      <td>4.731460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
              "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
              "      <td>4.344990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
              "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
              "      <td>4.746280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
              "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
              "      <td>2.996730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142039</th>\n",
              "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
              "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
              "      <td>3.336156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142040</th>\n",
              "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
              "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
              "      <td>2.887312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142041</th>\n",
              "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
              "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
              "      <td>3.662905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142042</th>\n",
              "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
              "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
              "      <td>3.800393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142043</th>\n",
              "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
              "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
              "      <td>3.999940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142044 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c72ee0e6-db05-4704-860c-c0aed50fba14')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c72ee0e6-db05-4704-860c-c0aed50fba14 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c72ee0e6-db05-4704-860c-c0aed50fba14');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-99f94263-516d-4224-8169-01f72366fa14\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99f94263-516d-4224-8169-01f72366fa14')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-99f94263-516d-4224-8169-01f72366fa14 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 421 ms (started: 2024-03-18 04:11:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_dict = {\n",
        "    \"key1\": \"value1\",\n",
        "    \"key2\": \"value2\",\n",
        "    \"key3\": \"value3\",\n",
        "    \"key4\": \"value4\",\n",
        "    \"key5\": \"value5\"\n",
        "}\n",
        "\n",
        "set(example_dict.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMy5YJUaZ6Kb",
        "outputId": "37d72ddd-a08b-4b4d-9892-26b888a2637c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'key1', 'key2', 'key3', 'key4', 'key5'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.03 ms (started: 2024-03-18 02:34:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2"
      ],
      "metadata": {
        "id": "ltvKIGMJFPfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile task2_2.py\n",
        "import csv\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "def save_data(data, output_file_name):\n",
        "    header = [\"user_id\", \"business_id\", \"prediction\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "\n",
        "\n",
        "def read_csv_spark(path, sc):\n",
        "    rdd = sc.textFile(path)\n",
        "    header = rdd.first()\n",
        "    rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
        "    return rdd\n",
        "\n",
        "\n",
        "def read_json_spark(path, sc):\n",
        "    return sc.textFile(path).map(lambda row: json.loads(row))\n",
        "\n",
        "\n",
        "def process_reviews(review_rdd):\n",
        "    review_rdd = (\n",
        "        review_rdd.map(\n",
        "            lambda row: (row[\"business_id\"], (float(row[\"useful\"]), float(row[\"funny\"]), float(row[\"cool\"])))\n",
        "        )\n",
        "        .groupByKey()\n",
        "        .mapValues(lambda x: tuple(sum(col) / len(col) for col in zip(*x)))\n",
        "        .cache()\n",
        "    )\n",
        "    return review_rdd.collectAsMap()\n",
        "\n",
        "\n",
        "def process_user(usr_rdd):\n",
        "    usr_rdd = usr_rdd.map(\n",
        "        lambda row: (row[\"user_id\"], (float(row[\"average_stars\"]), float(row[\"review_count\"]), float(row[\"fans\"])))\n",
        "    ).cache()\n",
        "    return usr_rdd.collectAsMap()\n",
        "\n",
        "\n",
        "def process_bus(bus_rdd):\n",
        "    bus_rdd = bus_rdd.map(lambda row: (row[\"business_id\"], (float(row[\"stars\"]), float(row[\"review_count\"])))).cache()\n",
        "    return bus_rdd.collectAsMap()\n",
        "\n",
        "\n",
        "def process_train_data(row, review_dict, usr_dict, bus_dict):\n",
        "    if len(row)==3:\n",
        "        usr, bus, rating = row\n",
        "    else:\n",
        "        usr, bus = row\n",
        "        rating = 0\n",
        "\n",
        "    useful, funny, cool = review_dict.get(bus, (None, None, None))\n",
        "    usr_avg_star, usr_review_cnt, usr_fans = usr_dict.get(usr, (None, None, None))\n",
        "    bus_avg_star, bus_review_cnt = bus_dict.get(bus, (None, None))\n",
        "\n",
        "    return ([useful, funny, cool, usr_avg_star, usr_review_cnt, usr_fans, bus_avg_star, bus_review_cnt], rating)\n",
        "\n",
        "\n",
        "def task2_2(folder_path, test_file_name, output_file_name):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 2.2: : Model-based recommendation system\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Read and process the train data\n",
        "        train_rdd = read_csv_spark(folder_path + \"/yelp_train.csv\", spark)\n",
        "\n",
        "        review_rdd = read_json_spark(folder_path + \"/review_train.json\", spark)\n",
        "        review_rdd = process_reviews(review_rdd)\n",
        "\n",
        "        usr_rdd = read_json_spark(folder_path + \"/user.json\", spark)\n",
        "        usr_rdd = process_user(usr_rdd)\n",
        "\n",
        "        bus_rdd = read_json_spark(folder_path + \"/business.json\", spark)\n",
        "        bus_rdd = process_bus(bus_rdd)\n",
        "\n",
        "        # Read and process validation dataset\n",
        "        val_rdd = read_csv_spark(test_file_name, spark).cache()\n",
        "\n",
        "        # Train X and Y\n",
        "        train_rdd = train_rdd.map(lambda x: process_train_data(x, review_rdd, usr_rdd, bus_rdd))\n",
        "\n",
        "        # Valid x and Y\n",
        "        val_processed = val_rdd.map(lambda x: process_train_data(x, review_rdd, usr_rdd, bus_rdd))\n",
        "\n",
        "        # Extract X_train and Y_train\n",
        "        X_train = train_rdd.map(lambda x: x[0]).cache()\n",
        "        X_train = np.array(X_train.collect(), dtype=\"float32\")\n",
        "        Y_train = train_rdd.map(lambda x: x[1]).cache()\n",
        "        Y_train = np.array(Y_train.collect(), dtype=\"float32\")\n",
        "\n",
        "        # Extract X_train and Y_train\n",
        "        X_val = val_processed.map(lambda x: x[0]).cache()\n",
        "        X_val = np.array(X_val.collect(), dtype=\"float32\")\n",
        "        # Y_val = val_processed.map(lambda x: x[1]).cache()\n",
        "        # Y_val = np.array(Y_val.collect(), dtype='float32')\n",
        "\n",
        "        xgb = XGBRegressor(\n",
        "            colsample_bytree=0.5,\n",
        "            subsample=0.8,\n",
        "            learning_rate=0.02,\n",
        "            max_depth=17,\n",
        "            random_state=47,\n",
        "            min_child_weight=101,\n",
        "            n_estimators=40,\n",
        "        )\n",
        "        xgb.fit(X_train, Y_train)\n",
        "        Y_pred = xgb.predict(X_val)\n",
        "\n",
        "        pred_data = []\n",
        "        for i, row in enumerate(val_rdd.collect()):\n",
        "            pred_data.append([row[0], row[1], Y_pred[i]])\n",
        "\n",
        "        save_data(pred_data, output_file_name)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 4:\n",
        "        print(\"Usage: spark-submit task2_1.py <folder_path> <test_file_name> <output_file_name>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Read input parameters\n",
        "    folder_path = sys.argv[1]\n",
        "    test_file_name = sys.argv[2]\n",
        "    output_file_name = sys.argv[3]\n",
        "\n",
        "    task2_2(folder_path, test_file_name, output_file_name)\n",
        "\n",
        "# task2_2(\"HW3StudentData\", \"HW3StudentData/yelp_val.csv\", \"t2_2.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mkoeQrUCJKK",
        "outputId": "40b83406-94b3-44c2-b11e-b641909c2273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 89.79863214492798\n",
            "\n",
            "time: 1min 31s (started: 2024-03-17 02:54:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit task2_2.py HW3StudentData HW3StudentData/yelp_val.csv t2_2-dryrun.csv"
      ],
      "metadata": {
        "id": "ZUnNCb9zhsu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3"
      ],
      "metadata": {
        "id": "ew2iZuyHFPt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE END"
      ],
      "metadata": {
        "id": "YBynG38GE-7H"
      }
    }
  ]
}