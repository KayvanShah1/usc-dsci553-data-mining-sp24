{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PWWq0Er4FBuM",
        "cL97SCyDykKS",
        "KAj8s8GVuJPX",
        "It8sjYp1SGqG"
      ],
      "mount_file_id": "1IvAguEmMcTRmCA4131k0yKlwAwumXO7c",
      "authorship_tag": "ABX9TyMzxRvSQ3qhml34irYEMfZv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-3/notebooks/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Installation"
      ],
      "metadata": {
        "id": "5XnYG3M0Ek7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taGtoPV8D63T",
        "outputId": "c94b14ef-5018-4a2b-8e9f-b1c11d0d4736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark ipython-autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "java --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IosF_LW0Erdl",
        "outputId": "194669b6-c03b-4e62-c774-25a17f79c26a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.22 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "K2XFckbdEtr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import math\n",
        "import statistics\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwhXtVnuEwIi",
        "outputId": "92fb86e0-251e-4e7b-f1dc-46e41739ef6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 327 Âµs (started: 2024-03-18 07:33:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3\")\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "f7f7NJOyEzwO",
        "outputId": "c58db774-82bc-41a7-bfef-ac3a7320bc0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.93 ms (started: 2024-03-18 07:33:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6fvZSlEE4OG",
        "outputId": "164d3d76-c97f-4f94-e7dc-75e7a4a847a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HW3.ipynb\t__pycache__  t1-ref.txt  t2_2.csv\t  t2-ref.txt\ttask2_1.py\ttask2_2.py\n",
            "HW3StudentData\tt1.csv\t     t2_1.csv\t t2_2-dryrun.csv  task1_ref.py\ttask2_1_ref.py\n",
            "time: 106 ms (started: 2024-03-18 07:33:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "Q7CV-QZhE7i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "PWWq0Er4FBuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from itertools import combinations\n",
        "import random\n",
        "import csv\n",
        "\n",
        "\n",
        "random.seed(37)\n",
        "\n",
        "# Define constants\n",
        "NUM_HASH_FUNCTIONS = 50\n",
        "PRIME_NUMBER = 15485863\n",
        "ROWS_PER_BAND = 2\n",
        "BANDS = NUM_HASH_FUNCTIONS // ROWS_PER_BAND\n",
        "\n",
        "\n",
        "def prepare_dataset(data):\n",
        "    # Remove the header\n",
        "    header = data.first()\n",
        "    data = (\n",
        "        data.filter(lambda row: row != header)\n",
        "        .map(lambda row: row.split(\",\"))\n",
        "    )\n",
        "\n",
        "    # Find unique users and map it to an index\n",
        "    usr_to_idx = (\n",
        "        data.map(lambda x: x[0])\n",
        "        .distinct()\n",
        "        .zipWithIndex()\n",
        "        .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # Group users that has reviewed a business\n",
        "    business_user = (\n",
        "        data.map(lambda row: (row[1], [row[0]]))\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "    return business_user, usr_to_idx\n",
        "\n",
        "\n",
        "def generate_hash_function_params(max_range, count):\n",
        "    \"\"\"Generate random hash function parameters within a specified range.\"\"\"\n",
        "    hash_funcs = []\n",
        "    for _ in range(count):\n",
        "        a = random.randint(1, max_range)  # Random coefficient 'a'\n",
        "        b = random.randint(0, max_range)  # Random intercept 'b'\n",
        "        hash_funcs.append((a, b))\n",
        "    return hash_funcs\n",
        "\n",
        "\n",
        "def hash_item(item, params, num_bins):\n",
        "    \"\"\"Hash an item using given hash function parameters.\n",
        "    Calculate hash value using the formula: ((a * item + b) % PRIME_NUMBER) % num_bins\n",
        "    \"\"\"\n",
        "    hash_val = ((params[0] * item + params[1]) % PRIME_NUMBER) % num_bins\n",
        "    return hash_val\n",
        "\n",
        "\n",
        "def build_minhash_signature_matrix(hash_funcs, users, num_bins):\n",
        "    \"\"\"Build the minhash signature matrix for a set of users.\"\"\"\n",
        "    mhs = []\n",
        "    for params in hash_funcs:\n",
        "        minhash = float(\"inf\")\n",
        "        for user in users:\n",
        "            # Hash each user and find the minimum hash value\n",
        "            hash_val = hash_item(user, params, num_bins)\n",
        "            minhash = min(minhash, hash_val)\n",
        "        mhs.append(minhash)\n",
        "    return mhs\n",
        "\n",
        "\n",
        "def jaccard_similarity(pair, bus_user_dict):\n",
        "    \"\"\"\n",
        "    Calculate Jaccard similarity for a candidate pair of businesses.\n",
        "\n",
        "    Args:\n",
        "        pair (tuple): A pair of business IDs.\n",
        "        bus_user_dict (dict): Dictionary mapping business IDs to sets of user IDs.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the business pair and their Jaccard similarity.\n",
        "    \"\"\"\n",
        "    # Extract business IDs from the pair\n",
        "    bus1, bus2 = pair\n",
        "\n",
        "    # Get sets of users who reviewed each business\n",
        "    user1 = set(bus_user_dict[bus1])\n",
        "    user2 = set(bus_user_dict[bus2])\n",
        "\n",
        "    # Calculate Jaccard similarity\n",
        "    intersection = len(user1 & user2)\n",
        "    union = len(user1 | user2)\n",
        "    similarity = intersection / union if union != 0 else 0\n",
        "\n",
        "    return (bus1, bus2), similarity\n",
        "\n",
        "\n",
        "def jaccard_based_lsh(prepared_data):\n",
        "    \"\"\"Perform Jaccard-based Locality Sensitive Hashing (LSH) on prepared data.\n",
        "\n",
        "    This function applies LSH to find candidate pairs of businesses with similar users,\n",
        "    based on the Jaccard similarity metric.\n",
        "\n",
        "    Algorithm Steps:\n",
        "    1. Unpack the prepared data containing the business-to-user mapping and user index mapping.\n",
        "    2. Generate a set of hash functions.\n",
        "    3. Compute the Minhash Signature for each business.\n",
        "    4. Divide the signature matrix into bands.\n",
        "    5. Group businesses into bands based on their Minhash Signature.\n",
        "    6. Find candidate pairs of businesses within each band.\n",
        "    7. Calculate the Jaccard similarity for candidate pairs.\n",
        "    8. Filter pairs with similarity above a threshold (e.g., 0.5).\n",
        "    9. Sort the results by business ID pairs.\n",
        "    10. Return the RDD containing the Jaccard similarity results for candidate business pairs.\n",
        "\n",
        "    Args:\n",
        "        prepared_data (tuple): A tuple containing the business-to-user mapping RDD and user index mapping dictionary.\n",
        "\n",
        "    Returns:\n",
        "        RDD: An RDD containing the Jaccard similarity results for candidate business pairs.\n",
        "    \"\"\"\n",
        "    # Unpack prepared data\n",
        "    business_to_user, usr_to_idx = prepared_data\n",
        "\n",
        "    # Generate Hash functions\n",
        "    NUM_BINS = len(usr_to_idx)\n",
        "    hash_func_params = generate_hash_function_params(NUM_BINS, NUM_HASH_FUNCTIONS)\n",
        "\n",
        "    # Compute Minhash Signature\n",
        "    minhash_sign = (\n",
        "        business_to_user.mapValues(lambda users: [usr_to_idx[user] for user in users])\n",
        "        .mapValues(lambda users: build_minhash_signature_matrix(hash_func_params, users, NUM_BINS))\n",
        "    )\n",
        "\n",
        "    # Divide signature matrix into bands\n",
        "    bands = (\n",
        "        minhash_sign.flatMap(\n",
        "            lambda x: [\n",
        "                (\n",
        "                    (i, tuple(x[1][i*ROWS_PER_BAND: (i+1)*ROWS_PER_BAND])), x[0]\n",
        "                )\n",
        "                for i in range(BANDS)\n",
        "            ]\n",
        "        )\n",
        "        .groupByKey()\n",
        "        .mapValues(list)\n",
        "        .filter(lambda x: len(x[1]) > 1)\n",
        "    )\n",
        "\n",
        "    # Find the business candidate pairs\n",
        "    candidates = (\n",
        "        bands.map(lambda x: sorted(x[1]))\n",
        "        .flatMap(lambda x: list(combinations(x, 2)))\n",
        "        .distinct()\n",
        "    )\n",
        "\n",
        "    # Calculate Jaccard Similirality for pairs\n",
        "    bus_to_user_dict = business_to_user.collectAsMap()\n",
        "\n",
        "    jaccard_sim_results = (\n",
        "        candidates.map(lambda x: jaccard_similarity(x, bus_to_user_dict))\n",
        "        .filter(lambda x: x[1] >= 0.5)\n",
        "        .sortByKey()\n",
        "        .map(lambda x: [x[0][0], x[0][1] ,x[1]])\n",
        "    )\n",
        "    return jaccard_sim_results\n",
        "\n",
        "\n",
        "def task1(input_file_name, output_file_name):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 1\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Read the input data\n",
        "        data = spark.textFile(input_file_name)\n",
        "        prepared_data = prepare_dataset(data)\n",
        "\n",
        "        # Compute Jaccard similarity using LSH\n",
        "        jaccard_sim_results = jaccard_based_lsh(prepared_data)\n",
        "\n",
        "        # Write header and results to a CSV file\n",
        "        header = [\"business_id_1\", \"business_id_2\", \"similarity\"]\n",
        "        with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "            writer.writerows(jaccard_sim_results.collect())\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if len(sys.argv) != 3:\n",
        "#         print(\n",
        "#             \"Usage: spark-submit task1.py <input_file_name> <output_file_name>\"\n",
        "#         )\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Read input parameters\n",
        "#     input_file_path = sys.argv[1]\n",
        "#     output_file_path = sys.argv[2]\n",
        "\n",
        "#     task1(input_file_path, output_file_path)\n",
        "\n",
        "\n",
        "task1(\"HW3StudentData/yelp_train.csv\", \"t1.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8RUcZwKK4_T",
        "outputId": "70b232cc-19a3-45d3-ae1b-c77bbfa58c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 53.42814826965332\n",
            "\n",
            "time: 54.7 s (started: 2024-03-13 09:12:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ref"
      ],
      "metadata": {
        "id": "cL97SCyDykKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Reference Code Task 1 { vertical-output: true, form-width: \"30%\" }\n",
        "%%writefile task1_ref.py\n",
        "from pyspark import SparkContext\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "from itertools import combinations\n",
        "import operator\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_path = sys.argv[1]\n",
        "    output_path = sys.argv[2]\n",
        "\n",
        "    s_t = time.time()\n",
        "\n",
        "    spark = SparkContext(appName= \"task1\")\n",
        "    lines = spark.textFile(input_path)\n",
        "    first = lines.first()\n",
        "    lines = lines.filter(lambda row: row != first).map(lambda row: row.split(\",\"))\n",
        "    #print(raw_rdd.take(10))\n",
        "\n",
        "    bus_user = lines.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set)\n",
        "    #print(bus_user.take(10))\n",
        "    bus_user_dict = {}\n",
        "    for bus, users in bus_user.collect():\n",
        "        bus_user_dict[bus] = users\n",
        "    users = lines.map(lambda row: row[0]).distinct()\n",
        "    users_dict = {}\n",
        "    i = 0\n",
        "    for user in users.collect():\n",
        "        users_dict[user] = i\n",
        "        i += 1\n",
        "\n",
        "    n = 60\n",
        "    m = i\n",
        "    p = 1e9 + 7\n",
        "    hash_funcs = [] #[a, b]\n",
        "    a = random.sample(range(1, m), n)\n",
        "    hash_funcs.append(a)\n",
        "    b = random.sample(range(1, m), n)\n",
        "    hash_funcs.append(b)\n",
        "    #print(hash_funcs)\n",
        "\n",
        "    sign_dict = {}\n",
        "    for bus, user_list in bus_user.collect():\n",
        "        minhash_sign_list = []\n",
        "        for i in range(n):\n",
        "            minhash = float(\"inf\")\n",
        "            for user in user_list:\n",
        "                minhash = min(minhash, (((hash_funcs[0][i] * users_dict[user] + hash_funcs[1][i]) % p) % m))\n",
        "            minhash_sign_list.append(int(minhash))\n",
        "        sign_dict[bus] = minhash_sign_list\n",
        "    #print(sign_dict)\n",
        "\n",
        "    r = 2\n",
        "    b = n // r\n",
        "    bands_dict = {}\n",
        "    for bus, minhash_sign in sign_dict.items():\n",
        "        for i in range(0, b):\n",
        "            #print(s[1][i*r: i*r+r])\n",
        "            idx = (i, tuple(minhash_sign[i*r: i*r+r]))\n",
        "            if idx not in bands_dict.keys():\n",
        "                   bands_dict[idx] = []\n",
        "                   bands_dict[idx].append(bus)\n",
        "            else:\n",
        "                   bands_dict[idx].append(bus)\n",
        "    #print(bands_dict)\n",
        "    bands_dict_fi = {}\n",
        "    for key, values in bands_dict.items():\n",
        "        if len(values) > 1:\n",
        "            bands_dict_fi[key] = values\n",
        "    #print(bands_dict_fi)\n",
        "    #418426\n",
        "    candidates = set()\n",
        "    for values in bands_dict_fi.values():\n",
        "        comb_list = combinations(sorted(values), 2)\n",
        "        for item in comb_list:\n",
        "            candidates.add(item)\n",
        "    #print(candidates)\n",
        "\n",
        "    result = {}\n",
        "    for bus1, bus2 in candidates:\n",
        "        user1 = bus_user_dict[bus1]\n",
        "        user2 = bus_user_dict[bus2]\n",
        "        js = len(user1 & user2) / len(user1 | user2)\n",
        "        if js >= 0.5:\n",
        "            result[str(bus1) + \",\" + str(bus2)] = js\n",
        "    result = dict(sorted(result.items(), key=operator.itemgetter(0)))\n",
        "    result_str = \"business_id_1, business_id_2, similarity\\n\"\n",
        "    for key, values in result.items():\n",
        "        result_str += key + \",\" + str(values) + \"\\n\"\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.writelines(result_str)\n",
        "\n",
        "    e_t = time.time()\n",
        "    print('Duration: ', e_t - s_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "GXsY5d1-uQAl",
        "outputId": "29c59c8a-30d7-4245-f9d9-66c88a708981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1_ref.py\n",
            "time: 13.6 ms (started: 2024-03-13 03:07:25 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit task1_ref.py HW3StudentData/yelp_train.csv t1-ref.txt --executor-memory 4G --driver-memory 4G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMGzjLZxuhZE",
        "outputId": "163753f9-b4ea-40dd-b1b9-0ae3dd26de0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/03/13 03:07:40 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/03/13 03:07:40 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "24/03/13 03:07:40 INFO SparkContext: Java version 11.0.22\n",
            "24/03/13 03:07:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/03/13 03:07:40 INFO ResourceUtils: ==============================================================\n",
            "24/03/13 03:07:40 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/03/13 03:07:40 INFO ResourceUtils: ==============================================================\n",
            "24/03/13 03:07:40 INFO SparkContext: Submitted application: task1\n",
            "24/03/13 03:07:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/03/13 03:07:41 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/03/13 03:07:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing view acls to: root\n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing modify acls to: root\n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing view acls groups to: \n",
            "24/03/13 03:07:41 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/03/13 03:07:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/03/13 03:07:41 INFO Utils: Successfully started service 'sparkDriver' on port 37875.\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/03/13 03:07:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/03/13 03:07:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/03/13 03:07:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b943d84d-3e85-4eda-96b6-323c9bed1828\n",
            "24/03/13 03:07:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/03/13 03:07:41 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/03/13 03:07:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/03/13 03:07:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/03/13 03:07:42 INFO Executor: Starting executor ID driver on host ebb69f3ce07d\n",
            "24/03/13 03:07:42 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "24/03/13 03:07:42 INFO Executor: Java version 11.0.22\n",
            "24/03/13 03:07:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/03/13 03:07:42 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7376c046 for default.\n",
            "24/03/13 03:07:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45743.\n",
            "24/03/13 03:07:42 INFO NettyBlockTransferService: Server created on ebb69f3ce07d:45743\n",
            "24/03/13 03:07:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/03/13 03:07:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:42 INFO BlockManagerMasterEndpoint: Registering block manager ebb69f3ce07d:45743 with 434.4 MiB RAM, BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ebb69f3ce07d, 45743, None)\n",
            "24/03/13 03:07:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
            "24/03/13 03:07:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
            "24/03/13 03:07:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ebb69f3ce07d:45743 (size: 32.6 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:44 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/03/13 03:07:44 INFO FileInputFormat: Total input files to process : 1\n",
            "24/03/13 03:07:44 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:181)\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Missing parents: List()\n",
            "24/03/13 03:07:44 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "24/03/13 03:07:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ebb69f3ce07d:45743 (size: 4.8 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "24/03/13 03:07:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "24/03/13 03:07:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ebb69f3ce07d, executor driver, partition 0, PROCESS_LOCAL, 7716 bytes) \n",
            "24/03/13 03:07:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/03/13 03:07:45 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/13 03:07:47 INFO PythonRunner: Times: total = 1909, boot = 1270, init = 639, finish = 0\n",
            "24/03/13 03:07:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1478 bytes result sent to driver\n",
            "24/03/13 03:07:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2728 ms on ebb69f3ce07d (executor driver) (1/1)\n",
            "24/03/13 03:07:47 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33057\n",
            "24/03/13 03:07:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:47 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:181) finished in 3.018 s\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:181, took 3.269329 s\n",
            "24/03/13 03:07:48 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Registering RDD 4 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20) as input to shuffle 0\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Got job 1 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) with 2 output partitions\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23)\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20), which has no missing parents\n",
            "24/03/13 03:07:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.4 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ebb69f3ce07d:45743 (size: 7.9 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ebb69f3ce07d, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:48 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (ebb69f3ce07d, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "24/03/13 03:07:48 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "24/03/13 03:07:48 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/13 03:07:48 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/13 03:07:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ebb69f3ce07d:45743 in memory (size: 4.8 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:52 INFO PythonRunner: Times: total = 4065, boot = 22, init = 982, finish = 3061\n",
            "24/03/13 03:07:52 INFO PythonRunner: Times: total = 4071, boot = 48, init = 979, finish = 3044\n",
            "24/03/13 03:07:53 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1624 bytes result sent to driver\n",
            "24/03/13 03:07:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1667 bytes result sent to driver\n",
            "24/03/13 03:07:53 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 4540 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4578 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:53 INFO DAGScheduler: ShuffleMapStage 1 (groupByKey at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:20) finished in 4.698 s\n",
            "24/03/13 03:07:53 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/13 03:07:53 INFO DAGScheduler: running: Set()\n",
            "24/03/13 03:07:53 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "24/03/13 03:07:53 INFO DAGScheduler: failed: Set()\n",
            "24/03/13 03:07:53 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23), which has no missing parents\n",
            "24/03/13 03:07:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ebb69f3ce07d:45743 (size: 6.4 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (ebb69f3ce07d, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:53 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (ebb69f3ce07d, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)\n",
            "24/03/13 03:07:53 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 31 ms\n",
            "24/03/13 03:07:54 INFO PythonRunner: Times: total = 873, boot = -320, init = 691, finish = 502\n",
            "24/03/13 03:07:54 INFO MemoryStore: Block taskresult_4 stored as bytes in memory (estimated size 5.9 MiB, free 428.2 MiB)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Added taskresult_4 in memory on ebb69f3ce07d:45743 (size: 5.9 MiB, free: 428.5 MiB)\n",
            "24/03/13 03:07:54 INFO PythonRunner: Times: total = 937, boot = -280, init = 631, finish = 586\n",
            "24/03/13 03:07:54 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 6166238 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:54 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 5.8 MiB, free 422.4 MiB)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Added taskresult_3 in memory on ebb69f3ce07d:45743 (size: 5.8 MiB, free: 422.7 MiB)\n",
            "24/03/13 03:07:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 6065056 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:54 INFO TransportClientFactory: Successfully created connection to ebb69f3ce07d/172.28.0.12:45743 after 54 ms (0 ms spent in bootstraps)\n",
            "24/03/13 03:07:54 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 1410 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ebb69f3ce07d:45743 in memory (size: 7.9 KiB, free: 422.7 MiB)\n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Removed taskresult_4 on ebb69f3ce07d:45743 in memory (size: 5.9 MiB, free: 428.6 MiB)\n",
            "24/03/13 03:07:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 1509 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:54 INFO BlockManagerInfo: Removed taskresult_3 on ebb69f3ce07d:45743 in memory (size: 5.8 MiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:54 INFO DAGScheduler: ResultStage 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) finished in 1.550 s\n",
            "24/03/13 03:07:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "24/03/13 03:07:54 INFO DAGScheduler: Job 1 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23, took 6.378695 s\n",
            "24/03/13 03:07:55 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Registering RDD 9 (distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25) as input to shuffle 1\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Got job 2 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28) with 2 output partitions\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28)\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Submitting ShuffleMapStage 3 (PairwiseRDD[9] at distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25), which has no missing parents\n",
            "24/03/13 03:07:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ebb69f3ce07d:45743 (size: 7.9 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (PairwiseRDD[9] at distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (ebb69f3ce07d, executor driver, partition 0, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:55 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6) (ebb69f3ce07d, executor driver, partition 1, PROCESS_LOCAL, 7705 bytes) \n",
            "24/03/13 03:07:55 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "24/03/13 03:07:55 INFO Executor: Running task 1.0 in stage 3.0 (TID 6)\n",
            "24/03/13 03:07:55 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:0+11396363\n",
            "24/03/13 03:07:55 INFO HadoopRDD: Input split: file:/content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/HW3StudentData/yelp_train.csv:11396363+11396363\n",
            "24/03/13 03:07:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ebb69f3ce07d:45743 in memory (size: 6.4 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 2251, boot = -831, init = 1390, finish = 1692\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1667 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 2419 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 2362, boot = -894, init = 1312, finish = 1944\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 1.0 in stage 3.0 (TID 6). 1624 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 2474 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:57 INFO DAGScheduler: ShuffleMapStage 3 (distinct at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:25) finished in 2.511 s\n",
            "24/03/13 03:07:57 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/03/13 03:07:57 INFO DAGScheduler: running: Set()\n",
            "24/03/13 03:07:57 INFO DAGScheduler: waiting: Set(ResultStage 4)\n",
            "24/03/13 03:07:57 INFO DAGScheduler: failed: Set()\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28), which has no missing parents\n",
            "24/03/13 03:07:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.3 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ebb69f3ce07d:45743 (size: 6.1 KiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (PythonRDD[12] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (ebb69f3ce07d, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:57 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8) (ebb69f3ce07d, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:57 INFO Executor: Running task 0.0 in stage 4.0 (TID 7)\n",
            "24/03/13 03:07:57 INFO Executor: Running task 1.0 in stage 4.0 (TID 8)\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Getting 2 (291.6 KiB) non-empty blocks including 2 (291.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Getting 2 (291.6 KiB) non-empty blocks including 2 (291.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 242, boot = -107, init = 325, finish = 24\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 7). 143710 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 286 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:57 INFO PythonRunner: Times: total = 308, boot = -91, init = 385, finish = 14\n",
            "24/03/13 03:07:57 INFO Executor: Finished task 1.0 in stage 4.0 (TID 8). 144205 bytes result sent to driver\n",
            "24/03/13 03:07:57 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 390 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:57 INFO DAGScheduler: ResultStage 4 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28) finished in 0.420 s\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "24/03/13 03:07:57 INFO DAGScheduler: Job 2 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:28, took 2.971443 s\n",
            "24/03/13 03:07:58 INFO SparkContext: Starting job: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Got job 3 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43) with 2 output partitions\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43)\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Missing parents: List()\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23), which has no missing parents\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.0 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ebb69f3ce07d:45743 (size: 6.4 KiB, free: 434.3 MiB)\n",
            "24/03/13 03:07:58 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
            "24/03/13 03:07:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[7] at collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:23) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/03/13 03:07:58 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
            "24/03/13 03:07:58 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 9) (ebb69f3ce07d, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:58 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 10) (ebb69f3ce07d, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/03/13 03:07:58 INFO Executor: Running task 0.0 in stage 6.0 (TID 9)\n",
            "24/03/13 03:07:58 INFO Executor: Running task 1.0 in stage 6.0 (TID 10)\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Getting 2 (6.0 MiB) non-empty blocks including 2 (6.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/03/13 03:07:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "24/03/13 03:07:58 INFO PythonRunner: Times: total = 612, boot = -205, init = 488, finish = 329\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block taskresult_9 stored as bytes in memory (estimated size 5.8 MiB, free 428.3 MiB)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Added taskresult_9 in memory on ebb69f3ce07d:45743 (size: 5.8 MiB, free: 428.6 MiB)\n",
            "24/03/13 03:07:58 INFO Executor: Finished task 0.0 in stage 6.0 (TID 9). 6065056 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:58 INFO PythonRunner: Times: total = 696, boot = -101, init = 417, finish = 380\n",
            "24/03/13 03:07:58 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 9) in 754 ms on ebb69f3ce07d (executor driver) (1/2)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Removed taskresult_9 on ebb69f3ce07d:45743 in memory (size: 5.8 MiB, free: 434.3 MiB)\n",
            "24/03/13 03:07:58 INFO MemoryStore: Block taskresult_10 stored as bytes in memory (estimated size 5.9 MiB, free 428.2 MiB)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Added taskresult_10 in memory on ebb69f3ce07d:45743 (size: 5.9 MiB, free: 428.5 MiB)\n",
            "24/03/13 03:07:58 INFO Executor: Finished task 1.0 in stage 6.0 (TID 10). 6166238 bytes result sent via BlockManager)\n",
            "24/03/13 03:07:58 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ebb69f3ce07d:45743 in memory (size: 6.1 KiB, free: 428.5 MiB)\n",
            "24/03/13 03:07:58 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 10) in 936 ms on ebb69f3ce07d (executor driver) (2/2)\n",
            "24/03/13 03:07:59 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "24/03/13 03:07:59 INFO DAGScheduler: ResultStage 6 (collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43) finished in 1.009 s\n",
            "24/03/13 03:07:59 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/03/13 03:07:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "24/03/13 03:07:59 INFO DAGScheduler: Job 3 finished: collect at /content/drive/MyDrive/Colab Notebooks/DSCI553/hw3/task1_ref.py:43, took 1.026754 s\n",
            "24/03/13 03:07:59 INFO BlockManagerInfo: Removed taskresult_10 on ebb69f3ce07d:45743 in memory (size: 5.9 MiB, free: 434.4 MiB)\n",
            "24/03/13 03:07:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ebb69f3ce07d:45743 in memory (size: 7.9 KiB, free: 434.4 MiB)\n",
            "Duration:  123.35331583023071\n",
            "24/03/13 03:09:51 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/03/13 03:09:51 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/03/13 03:09:51 INFO SparkUI: Stopped Spark web UI at http://ebb69f3ce07d:4040\n",
            "24/03/13 03:09:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/03/13 03:09:51 INFO MemoryStore: MemoryStore cleared\n",
            "24/03/13 03:09:51 INFO BlockManager: BlockManager stopped\n",
            "24/03/13 03:09:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/03/13 03:09:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/03/13 03:09:51 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-65db0161-2f70-4ea5-91b2-e826cde10906\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-efc7c3eb-9748-43d4-882f-442588c3c146/pyspark-9ce456fe-7700-4444-84c6-1018d55b2604\n",
            "24/03/13 03:09:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-efc7c3eb-9748-43d4-882f-442588c3c146\n",
            "time: 2min 16s (started: 2024-03-13 03:07:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "KAj8s8GVuJPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "((7 * 9098 + 147) % 15485863) % 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcXz7TK3psjm",
        "outputId": "25be2363-da18-4d45-c4ad-4bf389f2de7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.36 ms (started: 2024-03-13 00:29:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(\n",
        "    combinations(\n",
        "        sorted(\n",
        "            (\n",
        "                (\"b1\",[2,3,4]),\n",
        "                (\"b2\",[3,4,77]),\n",
        "                (\"b3\", [-1, 67, 0]),\n",
        "                (\"b4\",[-2,3,4]),\n",
        "                (\"b5\",[3,-42,77]),\n",
        "                (\"b6\", [0, 7, -103])\n",
        "            )\n",
        "        ),\n",
        "    2)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY2KnIlnhbJp",
        "outputId": "dd344a2f-17a3-46b5-c6cc-799e1c36bc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('b1', [2, 3, 4]), ('b2', [3, 4, 77])),\n",
              " (('b1', [2, 3, 4]), ('b3', [-1, 67, 0])),\n",
              " (('b1', [2, 3, 4]), ('b4', [-2, 3, 4])),\n",
              " (('b1', [2, 3, 4]), ('b5', [3, -42, 77])),\n",
              " (('b1', [2, 3, 4]), ('b6', [0, 7, -103])),\n",
              " (('b2', [3, 4, 77]), ('b3', [-1, 67, 0])),\n",
              " (('b2', [3, 4, 77]), ('b4', [-2, 3, 4])),\n",
              " (('b2', [3, 4, 77]), ('b5', [3, -42, 77])),\n",
              " (('b2', [3, 4, 77]), ('b6', [0, 7, -103])),\n",
              " (('b3', [-1, 67, 0]), ('b4', [-2, 3, 4])),\n",
              " (('b3', [-1, 67, 0]), ('b5', [3, -42, 77])),\n",
              " (('b3', [-1, 67, 0]), ('b6', [0, 7, -103])),\n",
              " (('b4', [-2, 3, 4]), ('b5', [3, -42, 77])),\n",
              " (('b4', [-2, 3, 4]), ('b6', [0, 7, -103])),\n",
              " (('b5', [3, -42, 77]), ('b6', [0, 7, -103]))]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 16.5 ms (started: 2024-03-13 02:38:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1"
      ],
      "metadata": {
        "id": "T9XrGZweFJ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile task2_1.py\n",
        "import csv\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\n",
        "def prepare_dataset(data, split=\"train\"):\n",
        "    # Remove the header\n",
        "    header = data.first()\n",
        "    data = (\n",
        "        data.filter(lambda row: row != header)\n",
        "        .map(lambda row: row.split(\",\"))\n",
        "        .map(lambda row: (row[0], row[1], row[2]) if split == \"train\" else (row[0], row[1]))\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_data(data, output_file_name):\n",
        "    header = [\"user_id\", \"business_id\", \"prediction\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "\n",
        "\n",
        "def get_bus_to_usr_map(train_data):\n",
        "    # Group by business_id and collect the corresponding set of users\n",
        "    bus2user = (\n",
        "        train_data.map(lambda x: (x[1], (x[0], float(x[2]))))\n",
        "        .groupByKey()\n",
        "        .mapValues(lambda vals: {\"users\": dict(vals), \"avg_rating\": sum(val[1] for val in vals) / len(vals)})\n",
        "    )\n",
        "    return bus2user.collectAsMap()\n",
        "\n",
        "\n",
        "def get_usr_to_bus_map(train_data):\n",
        "    # Group by user_id and collect the corresponding set of businesses\n",
        "    user2bus = (\n",
        "        train_data.map(lambda x: (x[0], (x[1], float(x[2]))))\n",
        "        .groupByKey()\n",
        "        .mapValues(lambda vals: {\"business\": dict(vals)})\n",
        "    )\n",
        "    return user2bus.collectAsMap()\n",
        "\n",
        "\n",
        "def compute_pearson_similarity(data, item2user_dict):\n",
        "    \"\"\"\n",
        "    Formala: r = Î£áµ¢((xáµ¢ â mean(x))(yáµ¢ â mean(y))) (âÎ£áµ¢(xáµ¢ â mean(x))Â² âÎ£áµ¢(yáµ¢ â mean(y))Â²)â»Â¹\n",
        "    \"\"\"\n",
        "    # Unpack the data\n",
        "    item1, item2 = data\n",
        "\n",
        "    # Find common user to calculate co-rated averages\n",
        "    users_item1 = set(item2user_dict[item1][\"users\"].keys())\n",
        "    users_item2 = set(item2user_dict[item2][\"users\"].keys())\n",
        "    common_users = users_item1.intersection(users_item2)\n",
        "\n",
        "    if len(common_users) <= 1:\n",
        "        similarity = (5 - abs(item2user_dict[item1][\"avg_rating\"] - item2user_dict[item2][\"avg_rating\"])) / 5\n",
        "    else:\n",
        "        r1 = []\n",
        "        r2 = []\n",
        "        # Get ratings of common users for both business\n",
        "        for usr in common_users:\n",
        "            r1.append(item2user_dict[item1][\"users\"][usr])\n",
        "            r2.append(item2user_dict[item2][\"users\"][usr])\n",
        "\n",
        "        # Center the ratings by subtracting the co-rated average rating\n",
        "        r1_bar = sum(r1) / len(r1)\n",
        "        r2_bar = sum(r2) / len(r2)\n",
        "        r1 = [r - r1_bar for r in r1]\n",
        "        r2 = [r - r2_bar for r in r2]\n",
        "\n",
        "        # Compute weight for the item pair\n",
        "        numer = sum([a * b for a, b in zip(r1, r2)])\n",
        "        denom = ((sum([a**2 for a in r1])) ** 0.5) * (sum([b**2 for b in r2]) ** 0.5)\n",
        "\n",
        "        similarity = 0 if denom == 0 else numer / denom\n",
        "\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def predict_rating(data, bus2user_dict, user2bus_dict, neighbours=15):\n",
        "    \"\"\"Perform Item-based Collaborative filtering on prepared data.\"\"\"\n",
        "    # Unpack the data\n",
        "    user, business = data\n",
        "\n",
        "    # Return avg rating if user or business is not present in the dataset\n",
        "    if user not in user2bus_dict or business not in bus2user_dict:\n",
        "        return 3.0\n",
        "\n",
        "    # Pearson similarities for rating prediction\n",
        "    pc = []\n",
        "\n",
        "    for item in user2bus_dict[user][\"business\"].keys():\n",
        "        # Compute pearson similarity for each business pair\n",
        "        similarity = compute_pearson_similarity((business, item), bus2user_dict)\n",
        "        pc.append((similarity, bus2user_dict[item][\"users\"][user]))\n",
        "\n",
        "    # Calculate the predicted rating\n",
        "    top_pc = sorted(pc, key=lambda x: -x[0])[:neighbours]\n",
        "    x, y = 0, 0\n",
        "    for p, r in top_pc:\n",
        "        x += p * r\n",
        "        y += abs(p)\n",
        "    predicted_rating = 3.5 if y == 0 else x / y\n",
        "\n",
        "    return predicted_rating\n",
        "\n",
        "\n",
        "def task2_1(train_file_name, test_file_name, output_file_name):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 2.1: Item-Based Collaborative Filtering\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Read and process the train data\n",
        "        train_data = spark.textFile(train_file_name)\n",
        "        train_data = prepare_dataset(train_data, split=\"train\")\n",
        "\n",
        "        # Preprocess train data to get mapping dictionaries\n",
        "        bus2user_dict = get_bus_to_usr_map(train_data)\n",
        "        user2bus_dict = get_usr_to_bus_map(train_data)\n",
        "\n",
        "        # Read and prepare validation data\n",
        "        val_data = spark.textFile(test_file_name)\n",
        "        val_data = prepare_dataset(val_data, split=\"valid\").cache()\n",
        "\n",
        "        val_data = val_data.map(lambda x: [x[0], x[1], predict_rating(x, bus2user_dict, user2bus_dict)]).cache()\n",
        "\n",
        "        save_data(val_data.collect(), output_file_name)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 4:\n",
        "        print(\"Usage: spark-submit task2_1.py <train_file_name> <test_file_name> <output_file_name>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Read input parameters\n",
        "    train_file_name = sys.argv[1]\n",
        "    test_file_name = sys.argv[2]\n",
        "    output_file_name = sys.argv[3]\n",
        "\n",
        "    task2_1(train_file_name, test_file_name, output_file_name)\n",
        "\n",
        "# task2_1(\"HW3StudentData/yelp_train.csv\", \"HW3StudentData/yelp_val.csv\", \"t2_1.csv\")\n"
      ],
      "metadata": {
        "id": "xL_vyguilLgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d13652-2f5c-4ac7-f7f8-5d707f686b0e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2_1.py\n",
            "time: 357 ms (started: 2024-03-18 04:52:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "3wWEhSPDH_px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"HW3StudentData/yelp_val.csv\")\n",
        "# df.loc[df[\"user_id\"]==\"wf1GqnKQuvH-V3QN80UOOQ\"]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "J2ez1ccXIBmd",
        "outputId": "4d0aabc2-8edb-43e6-bae9-7654c756fe3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       user_id             business_id  stars\n",
              "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg    5.0\n",
              "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ    5.0\n",
              "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA    4.0\n",
              "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA    5.0\n",
              "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw    3.0\n",
              "...                        ...                     ...    ...\n",
              "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q    2.0\n",
              "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA    4.0\n",
              "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg    5.0\n",
              "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA    3.0\n",
              "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg    4.0\n",
              "\n",
              "[142044 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dab7a405-37e2-4610-b6b2-1da07fc9af82\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
              "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
              "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
              "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
              "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
              "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142039</th>\n",
              "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
              "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142040</th>\n",
              "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
              "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142041</th>\n",
              "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
              "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142042</th>\n",
              "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
              "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142043</th>\n",
              "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
              "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142044 rows Ã 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dab7a405-37e2-4610-b6b2-1da07fc9af82')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dab7a405-37e2-4610-b6b2-1da07fc9af82 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dab7a405-37e2-4610-b6b2-1da07fc9af82');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-447aff39-0d1a-4177-ad0e-2682635daa0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-447aff39-0d1a-4177-ad0e-2682635daa0c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-447aff39-0d1a-4177-ad0e-2682635daa0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 255 ms (started: 2024-03-17 22:42:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"t2_1.csv\")\n",
        "# df.loc[df[\"user_id\"]==\"wf1GqnKQuvH-V3QN80UOOQ\"]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "prZyWWZawLYr",
        "outputId": "f0de5690-75bd-482e-af22-6312a9e61b16"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       user_id             business_id  prediction\n",
              "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg    4.467539\n",
              "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ    4.731460\n",
              "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA    4.344990\n",
              "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA    4.746280\n",
              "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw    2.996730\n",
              "...                        ...                     ...         ...\n",
              "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q    3.336156\n",
              "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA    2.887312\n",
              "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg    3.662905\n",
              "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA    3.800393\n",
              "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg    3.999940\n",
              "\n",
              "[142044 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-adb8be3c-6af4-46e1-8ec5-9abc0b4a2669\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
              "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
              "      <td>4.467539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
              "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
              "      <td>4.731460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
              "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
              "      <td>4.344990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
              "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
              "      <td>4.746280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
              "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
              "      <td>2.996730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142039</th>\n",
              "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
              "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
              "      <td>3.336156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142040</th>\n",
              "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
              "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
              "      <td>2.887312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142041</th>\n",
              "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
              "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
              "      <td>3.662905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142042</th>\n",
              "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
              "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
              "      <td>3.800393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142043</th>\n",
              "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
              "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
              "      <td>3.999940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142044 rows Ã 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adb8be3c-6af4-46e1-8ec5-9abc0b4a2669')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-adb8be3c-6af4-46e1-8ec5-9abc0b4a2669 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-adb8be3c-6af4-46e1-8ec5-9abc0b4a2669');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-04fb31e1-a40f-4ee1-a1cd-9fafdd70b0dd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04fb31e1-a40f-4ee1-a1cd-9fafdd70b0dd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-04fb31e1-a40f-4ee1-a1cd-9fafdd70b0dd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 208 ms (started: 2024-03-18 05:13:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2"
      ],
      "metadata": {
        "id": "ltvKIGMJFPfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile task2_2.py\n",
        "import csv\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "def save_data(data, output_file_name):\n",
        "    header = [\"user_id\", \"business_id\", \"prediction\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "\n",
        "\n",
        "def read_csv_spark(path, sc):\n",
        "    rdd = sc.textFile(path)\n",
        "    header = rdd.first()\n",
        "    rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
        "    return rdd\n",
        "\n",
        "\n",
        "def read_json_spark(path, sc):\n",
        "    return sc.textFile(path).map(lambda row: json.loads(row))\n",
        "\n",
        "\n",
        "def process_reviews(review_rdd):\n",
        "    review_rdd = (\n",
        "        review_rdd.map(\n",
        "            lambda row: (row[\"business_id\"], (float(row[\"useful\"]), float(row[\"funny\"]), float(row[\"cool\"])))\n",
        "        )\n",
        "        .groupByKey()\n",
        "        .mapValues(lambda x: tuple(sum(col) / len(col) for col in zip(*x)))\n",
        "        .cache()\n",
        "    )\n",
        "    return review_rdd.collectAsMap()\n",
        "\n",
        "\n",
        "def process_user(usr_rdd):\n",
        "    usr_rdd = usr_rdd.map(\n",
        "        lambda row: (row[\"user_id\"], (float(row[\"average_stars\"]), float(row[\"review_count\"]), float(row[\"fans\"])))\n",
        "    ).cache()\n",
        "    return usr_rdd.collectAsMap()\n",
        "\n",
        "\n",
        "def process_bus(bus_rdd):\n",
        "    bus_rdd = bus_rdd.map(lambda row: (row[\"business_id\"], (float(row[\"stars\"]), float(row[\"review_count\"])))).cache()\n",
        "    return bus_rdd.collectAsMap()\n",
        "\n",
        "\n",
        "def process_train_data(row, review_dict, usr_dict, bus_dict):\n",
        "    if len(row)==3:\n",
        "        usr, bus, rating = row\n",
        "    else:\n",
        "        usr, bus = row\n",
        "        rating = 0\n",
        "\n",
        "    useful, funny, cool = review_dict.get(bus, (None, None, None))\n",
        "    usr_avg_star, usr_review_cnt, usr_fans = usr_dict.get(usr, (None, None, None))\n",
        "    bus_avg_star, bus_review_cnt = bus_dict.get(bus, (None, None))\n",
        "\n",
        "    return ([useful, funny, cool, usr_avg_star, usr_review_cnt, usr_fans, bus_avg_star, bus_review_cnt], rating)\n",
        "\n",
        "\n",
        "def task2_2(folder_path, test_file_name, output_file_name):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 2.2: : Model-based recommendation system\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Read and process the train data\n",
        "        train_rdd = read_csv_spark(folder_path + \"/yelp_train.csv\", spark)\n",
        "\n",
        "        review_rdd = read_json_spark(folder_path + \"/review_train.json\", spark)\n",
        "        review_rdd = process_reviews(review_rdd)\n",
        "\n",
        "        usr_rdd = read_json_spark(folder_path + \"/user.json\", spark)\n",
        "        usr_rdd = process_user(usr_rdd)\n",
        "\n",
        "        bus_rdd = read_json_spark(folder_path + \"/business.json\", spark)\n",
        "        bus_rdd = process_bus(bus_rdd)\n",
        "\n",
        "        # Read and process validation dataset\n",
        "        val_rdd = read_csv_spark(test_file_name, spark).cache()\n",
        "\n",
        "        # Train X and Y\n",
        "        train_rdd = train_rdd.map(lambda x: process_train_data(x, review_rdd, usr_rdd, bus_rdd))\n",
        "\n",
        "        # Valid x and Y\n",
        "        val_processed = val_rdd.map(lambda x: process_train_data(x, review_rdd, usr_rdd, bus_rdd))\n",
        "\n",
        "        # Extract X_train and Y_train\n",
        "        X_train = train_rdd.map(lambda x: x[0]).cache()\n",
        "        X_train = np.array(X_train.collect(), dtype=\"float32\")\n",
        "        Y_train = train_rdd.map(lambda x: x[1]).cache()\n",
        "        Y_train = np.array(Y_train.collect(), dtype=\"float32\")\n",
        "\n",
        "        # Extract X_train and Y_train\n",
        "        X_val = val_processed.map(lambda x: x[0]).cache()\n",
        "        X_val = np.array(X_val.collect(), dtype=\"float32\")\n",
        "        # Y_val = val_processed.map(lambda x: x[1]).cache()\n",
        "        # Y_val = np.array(Y_val.collect(), dtype='float32')\n",
        "\n",
        "        xgb = XGBRegressor(\n",
        "            colsample_bytree=0.5,\n",
        "            subsample=0.8,\n",
        "            learning_rate=0.02,\n",
        "            max_depth=17,\n",
        "            random_state=47,\n",
        "            min_child_weight=101,\n",
        "            n_estimators=40,\n",
        "        )\n",
        "        xgb.fit(X_train, Y_train)\n",
        "        Y_pred = xgb.predict(X_val)\n",
        "\n",
        "        pred_data = []\n",
        "        for i, row in enumerate(val_rdd.collect()):\n",
        "            pred_data.append([row[0], row[1], Y_pred[i]])\n",
        "\n",
        "        save_data(pred_data, output_file_name)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 4:\n",
        "        print(\"Usage: spark-submit task2_1.py <folder_path> <test_file_name> <output_file_name>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Read input parameters\n",
        "    folder_path = sys.argv[1]\n",
        "    test_file_name = sys.argv[2]\n",
        "    output_file_name = sys.argv[3]\n",
        "\n",
        "    task2_2(folder_path, test_file_name, output_file_name)\n",
        "\n",
        "# task2_2(\"HW3StudentData\", \"HW3StudentData/yelp_val.csv\", \"t2_2.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mkoeQrUCJKK",
        "outputId": "40b83406-94b3-44c2-b11e-b641909c2273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 89.79863214492798\n",
            "\n",
            "time: 1min 31s (started: 2024-03-17 02:54:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit task2_2.py HW3StudentData HW3StudentData/yelp_val.csv t2_2-dryrun.csv"
      ],
      "metadata": {
        "id": "ZUnNCb9zhsu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3"
      ],
      "metadata": {
        "id": "ew2iZuyHFPt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "def save_data(data, output_file_name):\n",
        "    header = [\"user_id\", \"business_id\", \"prediction\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "\n",
        "\n",
        "class ItemBasedCF:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_dataset(data, split=\"train\"):\n",
        "        # Remove the header\n",
        "        header = data.first()\n",
        "        data = (\n",
        "            data.filter(lambda row: row != header)\n",
        "            .map(lambda row: row.split(\",\"))\n",
        "            .map(lambda row: (row[0], row[1], row[2]) if split == \"train\" else (row[0], row[1]))\n",
        "        )\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def get_bus_to_usr_map(train_data):\n",
        "        # Group by business_id and collect the corresponding set of users\n",
        "        bus2user = (\n",
        "            train_data.map(lambda x: (x[1], (x[0], float(x[2]))))\n",
        "            .groupByKey()\n",
        "            .mapValues(lambda vals: {\"users\": dict(vals), \"avg_rating\": sum(val[1] for val in vals) / len(vals)})\n",
        "        )\n",
        "        return bus2user.collectAsMap()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_usr_to_bus_map(train_data):\n",
        "        # Group by user_id and collect the corresponding set of businesses\n",
        "        user2bus = (\n",
        "            train_data.map(lambda x: (x[0], (x[1], float(x[2]))))\n",
        "            .groupByKey()\n",
        "            .mapValues(lambda vals: {\"business\": dict(vals)})\n",
        "        )\n",
        "        return user2bus.collectAsMap()\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_pearson_similarity(data, item2user_dict):\n",
        "        \"\"\"\n",
        "        Formala: r = Î£áµ¢((xáµ¢ â mean(x))(yáµ¢ â mean(y))) (âÎ£áµ¢(xáµ¢ â mean(x))Â² âÎ£áµ¢(yáµ¢ â mean(y))Â²)â»Â¹\n",
        "        \"\"\"\n",
        "        # Unpack the data\n",
        "        item1, item2 = data\n",
        "\n",
        "        # Find common user to calculate co-rated averages\n",
        "        users_item1 = set(item2user_dict[item1][\"users\"].keys())\n",
        "        users_item2 = set(item2user_dict[item2][\"users\"].keys())\n",
        "        common_users = users_item1.intersection(users_item2)\n",
        "\n",
        "        if len(common_users) <= 1:\n",
        "            similarity = (5 - abs(item2user_dict[item1][\"avg_rating\"] - item2user_dict[item2][\"avg_rating\"])) / 5\n",
        "        else:\n",
        "            r1 = []\n",
        "            r2 = []\n",
        "            # Get ratings of common users for both business\n",
        "            for usr in common_users:\n",
        "                r1.append(item2user_dict[item1][\"users\"][usr])\n",
        "                r2.append(item2user_dict[item2][\"users\"][usr])\n",
        "\n",
        "            # Center the ratings by subtracting the co-rated average rating\n",
        "            r1_bar = sum(r1) / len(r1)\n",
        "            r2_bar = sum(r2) / len(r2)\n",
        "            r1 = [r - r1_bar for r in r1]\n",
        "            r2 = [r - r2_bar for r in r2]\n",
        "\n",
        "            # Compute weight for the item pair\n",
        "            numer = sum([a * b for a, b in zip(r1, r2)])\n",
        "            denom = ((sum([a**2 for a in r1])) ** 0.5) * (sum([b**2 for b in r2]) ** 0.5)\n",
        "\n",
        "            similarity = 0 if denom == 0 else numer / denom\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    @staticmethod\n",
        "    def predict_rating(data, bus2user_dict, user2bus_dict, neighbours=15):\n",
        "        \"\"\"Perform Item-based Collaborative filtering on prepared data.\"\"\"\n",
        "        # Unpack the data\n",
        "        user, business = data\n",
        "\n",
        "        # Return avg rating if user or business is not present in the dataset\n",
        "        if user not in user2bus_dict or business not in bus2user_dict:\n",
        "            return 3.0\n",
        "\n",
        "        # Pearson similarities for rating prediction\n",
        "        pc = []\n",
        "\n",
        "        for item in user2bus_dict[user][\"business\"].keys():\n",
        "            # Compute pearson similarity for each business pair\n",
        "            similarity = ItemBasedCF.compute_pearson_similarity((business, item), bus2user_dict)\n",
        "            pc.append((similarity, bus2user_dict[item][\"users\"][user]))\n",
        "\n",
        "        # Calculate the predicted rating\n",
        "        top_pc = sorted(pc, key=lambda x: -x[0])[:neighbours]\n",
        "        x, y = 0, 0\n",
        "        for p, r in top_pc:\n",
        "            x += p * r\n",
        "            y += abs(p)\n",
        "        predicted_rating = 3.5 if y == 0 else x / y\n",
        "\n",
        "        return predicted_rating\n",
        "\n",
        "    def run(self, spark, train_file_name, test_file_name):\n",
        "        # Read and process the train data\n",
        "        train_data = spark.textFile(train_file_name)\n",
        "        train_data = ItemBasedCF.prepare_dataset(train_data, split=\"train\")\n",
        "\n",
        "        # Preprocess train data to get mapping dictionaries\n",
        "        bus2user_dict = ItemBasedCF.get_bus_to_usr_map(train_data)\n",
        "        user2bus_dict = ItemBasedCF.get_usr_to_bus_map(train_data)\n",
        "\n",
        "        # Read and prepare validation data\n",
        "        val_data = spark.textFile(test_file_name)\n",
        "        val_data = ItemBasedCF.prepare_dataset(val_data, split=\"valid\").cache()\n",
        "\n",
        "        val_data = val_data.map(\n",
        "            lambda x: [x[0], x[1], ItemBasedCF.predict_rating(x, bus2user_dict, user2bus_dict)]\n",
        "        ).cache()\n",
        "\n",
        "        return val_data\n",
        "\n",
        "\n",
        "class ModelBased:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def read_csv_spark(path, sc):\n",
        "        rdd = sc.textFile(path)\n",
        "        header = rdd.first()\n",
        "        rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
        "        return rdd\n",
        "\n",
        "    @staticmethod\n",
        "    def read_json_spark(path, sc):\n",
        "        return sc.textFile(path).map(lambda row: json.loads(row))\n",
        "\n",
        "    @staticmethod\n",
        "    def process_reviews(review_rdd):\n",
        "        review_rdd = (\n",
        "            review_rdd.map(\n",
        "                lambda row: (row[\"business_id\"], (float(row[\"useful\"]), float(row[\"funny\"]), float(row[\"cool\"])))\n",
        "            )\n",
        "            .groupByKey()\n",
        "            .mapValues(lambda x: tuple(sum(col) / len(col) for col in zip(*x)))\n",
        "            .cache()\n",
        "        )\n",
        "        return review_rdd.collectAsMap()\n",
        "\n",
        "    @staticmethod\n",
        "    def process_user(usr_rdd):\n",
        "        usr_rdd = usr_rdd.map(\n",
        "            lambda row: (row[\"user_id\"], (float(row[\"average_stars\"]), float(row[\"review_count\"]), float(row[\"fans\"])))\n",
        "        ).cache()\n",
        "        return usr_rdd.collectAsMap()\n",
        "\n",
        "    @staticmethod\n",
        "    def process_bus(bus_rdd):\n",
        "        bus_rdd = bus_rdd.map(\n",
        "            lambda row: (row[\"business_id\"], (float(row[\"stars\"]), float(row[\"review_count\"])))\n",
        "        ).cache()\n",
        "        return bus_rdd.collectAsMap()\n",
        "\n",
        "    @staticmethod\n",
        "    def process_train_data(row, review_dict, usr_dict, bus_dict):\n",
        "        if len(row) == 3:\n",
        "            usr, bus, rating = row\n",
        "        else:\n",
        "            usr, bus = row\n",
        "            rating = None\n",
        "\n",
        "        useful, funny, cool = review_dict.get(bus, (None, None, None))\n",
        "        usr_avg_star, usr_review_cnt, usr_fans = usr_dict.get(usr, (None, None, None))\n",
        "        bus_avg_star, bus_review_cnt = bus_dict.get(bus, (None, None))\n",
        "\n",
        "        return ([useful, funny, cool, usr_avg_star, usr_review_cnt, usr_fans, bus_avg_star, bus_review_cnt], rating)\n",
        "\n",
        "    def run(self, spark, folder_path, test_file_name):\n",
        "        # Read and process the train data\n",
        "        train_rdd = ModelBased.read_csv_spark(folder_path + \"/yelp_train.csv\", spark)\n",
        "\n",
        "        review_rdd = ModelBased.read_json_spark(folder_path + \"/review_train.json\", spark)\n",
        "        review_rdd = ModelBased.process_reviews(review_rdd)\n",
        "\n",
        "        usr_rdd = ModelBased.read_json_spark(folder_path + \"/user.json\", spark)\n",
        "        usr_rdd = ModelBased.process_user(usr_rdd)\n",
        "\n",
        "        bus_rdd = ModelBased.read_json_spark(folder_path + \"/business.json\", spark)\n",
        "        bus_rdd = ModelBased.process_bus(bus_rdd)\n",
        "\n",
        "        # Read and process validation dataset\n",
        "        val_rdd = ModelBased.read_csv_spark(test_file_name, spark).cache()\n",
        "\n",
        "        # Train X and Y\n",
        "        train_rdd = train_rdd.map(lambda x: ModelBased.process_train_data(x, review_rdd, usr_rdd, bus_rdd))\n",
        "\n",
        "        # Valid x and Y\n",
        "        val_processed = val_rdd.map(lambda x: ModelBased.process_train_data(x, review_rdd, usr_rdd, bus_rdd))\n",
        "\n",
        "        # Extract X_train and Y_train\n",
        "        X_train = train_rdd.map(lambda x: x[0]).cache()\n",
        "        X_train = np.array(X_train.collect(), dtype=\"float32\")\n",
        "        Y_train = train_rdd.map(lambda x: x[1]).cache()\n",
        "        Y_train = np.array(Y_train.collect(), dtype=\"float32\")\n",
        "\n",
        "        # Extract X_train and Y_train\n",
        "        X_val = val_processed.map(lambda x: x[0]).cache()\n",
        "        X_val = np.array(X_val.collect(), dtype=\"float32\")\n",
        "\n",
        "        xgb = XGBRegressor()\n",
        "        xgb.fit(X_train, Y_train)\n",
        "        Y_pred = xgb.predict(X_val)\n",
        "\n",
        "        pred_data = []\n",
        "        for i, row in enumerate(val_rdd.collect()):\n",
        "            pred_data.append([row[0], row[1], Y_pred[i]])\n",
        "\n",
        "        return spark.parallelize(pred_data)\n",
        "\n",
        "\n",
        "def hybrid_pred(preds, factor=0.5):\n",
        "    wieghted_pred = factor * preds[0] + (1 - factor) * preds[1]\n",
        "    return wieghted_pred\n",
        "\n",
        "\n",
        "def task2_3(folder_path, test_file_name, output_file_name):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 2.3: Hybrid recommendation system\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the item-based collaborative recommendation system\n",
        "        item_based = ItemBasedCF()\n",
        "        item_based_pred = item_based.run(\n",
        "            spark=spark, train_file_name=f\"{folder_path}/yelp_train.csv\", test_file_name=test_file_name\n",
        "        )\n",
        "        item_based_pred = item_based_pred.map(lambda x: ((x[0], x[1]), x[2])).persist()\n",
        "\n",
        "        # Train the item-based collaborative recommendation system\n",
        "        model_based = ModelBased()\n",
        "        model_based_pred = model_based.run(spark=spark, folder_path=folder_path, test_file_name=test_file_name)\n",
        "        model_based_pred = model_based_pred.map(lambda x: ((x[0], x[1]), x[2])).persist()\n",
        "\n",
        "        FACTOR = 0.05222\n",
        "\n",
        "        joined_preds = (\n",
        "            item_based_pred.join(model_based_pred)\n",
        "            .map(lambda x: [x[0][0], x[0][1], hybrid_pred(x[1], factor=FACTOR)])\n",
        "            .cache()\n",
        "        )\n",
        "\n",
        "        save_data(joined_preds.collect(), output_file_name)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if len(sys.argv) != 4:\n",
        "#         print(\"Usage: spark-submit task2_1.py <folder_path> <test_file_name> <output_file_name>\")\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Read input parameters\n",
        "#     folder_path = sys.argv[1]\n",
        "#     test_file_name = sys.argv[2]\n",
        "#     output_file_name = sys.argv[3]\n",
        "\n",
        "#     task2_3(folder_path, test_file_name, output_file_name)\n",
        "\n",
        "task2_3(\"HW3StudentData\", \"HW3StudentData/yelp_val.csv\", \"t2_3.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Dr_U-54OTe",
        "outputId": "a5f57c7d-f411-42d8-bb5b-d2550cf21f5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x-8ZMKKNycT3782Kqf9loA', 'jgtWfJCJZty_Nctqpdtp3g', 4.339137756524084]\n",
            "['0FVcoJko1kfZCrJRfssfIA', 'JVK8szNDoy9MNiYSz_MiAA', 2.6283899840360982]\n",
            "['C__1BHWTGBNA5s2ZPH289g', 'h_UvnQfe1cuVICly_kIqHg', 4.217598720355673]\n",
            "['zDBOdWtl2PsNY38IeoE5cQ', 'gy-HBIeJGlQHs4RRYDLuHw', 4.29041363721337]\n",
            "['CMu9FmdK8xpiawJowJuGQg', '364hhL5st0LV16UcBHRJ3A', 4.311573210538574]\n",
            "['aOseJnydZYD8Og00vWylqg', 'fNc1WuGwiT7RhqXUIe4S8A', 4.221355593590737]\n",
            "['I8_iXLcpYHAb_xi2vShgOg', 'LR0qF0FEVsCOhYWUOiH26A', 2.781415633708503]\n",
            "['renPzRDqMZpMaHiCD_e1_A', 'xTlmLL2xZZ0xhZ2J16zXQQ', 3.92118768756853]\n",
            "['_VTEyUzzH92X3w-IpGaXVA', 'X-b4-QvZLENnf3yFwhpSXQ', 4.522463311630964]\n",
            "['Nhk0jTP2gkU12G4LHJVR3A', 'FaHADZARwnY4yvlvpnsfGA', 4.109731298778561]\n",
            "Duration: 317.4028687477112\n",
            "\n",
            "time: 5min 19s (started: 2024-03-18 08:30:54 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "MHUCyI22-RZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Read the data\n",
        "df = pd.read_csv(\"HW3StudentData/yelp_val.csv\")\n",
        "df[\"item_based\"] = pd.read_csv(\"t2_1.csv\")[\"prediction\"]\n",
        "df[\"model_based\"] = pd.read_csv(\"t2_2.csv\")[\"prediction\"]\n",
        "df = df.loc[:, [\"item_based\", \"model_based\", \"stars\"]]\n",
        "\n",
        "# Initialize lists to store w and corresponding RMSE values\n",
        "w_values = []\n",
        "rmse_values = []\n",
        "\n",
        "# Iterate over different values of w\n",
        "for w in np.arange(0.049, 0.06, 0.00001):\n",
        "    # Calculate hybrid predictions\n",
        "    df[\"hybrid\"] = (w) * df[\"item_based\"] + (1 - w) * df[\"model_based\"]\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(df[\"stars\"], df[\"hybrid\"]))\n",
        "\n",
        "    # Append w and RMSE values to the lists\n",
        "    w_values.append(w)\n",
        "    rmse_values.append(rmse)\n",
        "\n",
        "# Create a DataFrame to store w and RMSE values\n",
        "rmse_df = pd.DataFrame({\"w\": w_values, \"RMSE\": rmse_values}).sort_values(by=\"RMSE\")\n",
        "\n",
        "# Find the row with the minimum RMSE\n",
        "best_w_row = rmse_df.loc[rmse_df['RMSE'].idxmin()]\n",
        "\n",
        "# Extract the best w value\n",
        "best_w = best_w_row['w']\n",
        "best_rmse = best_w_row['RMSE']\n",
        "\n",
        "print(\"Best w:\", best_w)\n",
        "print(\"Corresponding RMSE:\", best_rmse)\n",
        "\n",
        "# Print the DataFrame\n",
        "rmse_df.head(10)\n",
        "\n",
        "# w=0.05222"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "UR2bzuvY-ThW",
        "outputId": "d2c2dea1-39e0-40a8-c8ef-7f053820ede3"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best w: 0.05222000000000099\n",
            "Corresponding RMSE: 0.9823398391952476\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           w     RMSE\n",
              "322  0.05222  0.98234\n",
              "323  0.05223  0.98234\n",
              "321  0.05221  0.98234\n",
              "324  0.05224  0.98234\n",
              "320  0.05220  0.98234\n",
              "325  0.05225  0.98234\n",
              "319  0.05219  0.98234\n",
              "326  0.05226  0.98234\n",
              "318  0.05218  0.98234\n",
              "327  0.05227  0.98234"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6da2188b-0252-4211-a5b9-08771dcf79e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>w</th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>0.05222</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>0.05223</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>0.05221</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>0.05224</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>0.05220</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>0.05225</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0.05219</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>0.05226</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>0.05218</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>0.05227</td>\n",
              "      <td>0.98234</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6da2188b-0252-4211-a5b9-08771dcf79e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6da2188b-0252-4211-a5b9-08771dcf79e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6da2188b-0252-4211-a5b9-08771dcf79e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9d656fa1-ad5c-4c6e-a23b-209571093203\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d656fa1-ad5c-4c6e-a23b-209571093203')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9d656fa1-ad5c-4c6e-a23b-209571093203 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# w=0\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.027650354098419e-05,\n        \"min\": 0.052180000000000976,\n        \"max\": 0.052270000000001,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.052180000000000976,\n          0.05223000000000099,\n          0.052250000000001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.55600328803396e-11,\n        \"min\": 0.9823398391952476,\n        \"max\": 0.9823398393851367,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.982339839344803,\n          0.9823398391997289,\n          0.9823398392589364\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 103
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.65 s (started: 2024-03-18 07:30:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Read the data\n",
        "df = pd.read_csv(\"HW3StudentData/yelp_val.csv\")\n",
        "df[\"item_based\"] = pd.read_csv(\"t2_1.csv\")[\"prediction\"]\n",
        "df[\"model_based\"] = pd.read_csv(\"t2_2.csv\")[\"prediction\"]\n",
        "df = df.loc[:, [\"item_based\", \"model_based\", \"stars\"]]\n",
        "\n",
        "w = 0.05222\n",
        "df[\"hybrid\"] = (w) * df[\"item_based\"] + (1 - w) * df[\"model_based\"]\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(df[\"stars\"], df[\"hybrid\"]))\n",
        "print(rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ox2a_r8ZpTB",
        "outputId": "973b8c3e-afdd-40e2-e4a1-a93e8ba3b364"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9823398391952476\n",
            "time: 931 ms (started: 2024-03-18 07:30:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Initialize Spark\n",
        "conf = SparkConf().setAppName(\"RDD Join Example\")\n",
        "sc = SparkContext(conf=conf)\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "try:\n",
        "    # Sample data\n",
        "    item_based_data = [(\"A\", \"X\", 1), (\"B\", \"Y\", 2), (\"C\", \"Z\", 3)]\n",
        "    model_based_data = [(\"A\", \"X\", 10), (\"B\", \"Y\", 20), (\"C\", \"Z\", 30)]\n",
        "\n",
        "    # Create RDDs\n",
        "    item_based_rdd = sc.parallelize(item_based_data)\n",
        "    model_based_rdd = sc.parallelize(model_based_data)\n",
        "\n",
        "    # Transform RDDs into key-value pairs with composite key\n",
        "    item_based_keyed = item_based_rdd.map(lambda x: ((x[0], x[1]), x[2]))\n",
        "    model_based_keyed = model_based_rdd.map(lambda x: ((x[0], x[1]), x[2]))\n",
        "\n",
        "    # Join RDDs based on composite keys\n",
        "    joined_rdd = item_based_keyed.join(model_based_keyed)\n",
        "\n",
        "    # Output the joined data\n",
        "    for i in joined_rdd.collect():\n",
        "        print(i)\n",
        "\n",
        "finally:\n",
        "    # Stop Spark\n",
        "    sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QltN2A0KpV1v",
        "outputId": "ece7498e-9979-4b0e-ead3-b22583a5d332"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('C', 'Z'), (3, 30))\n",
            "(('A', 'X'), (1, 10))\n",
            "(('B', 'Y'), (2, 20))\n",
            "time: 5.42 s (started: 2024-03-18 08:20:43 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE END"
      ],
      "metadata": {
        "id": "YBynG38GE-7H"
      }
    }
  ]
}