{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1W97TXodSpBKOEESu3olsgSDMHcHYxmTr",
      "authorship_tag": "ABX9TyOkZQSqvcoM2sCCKQaxfsfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-5/notebooks/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Setup"
      ],
      "metadata": {
        "id": "IVI0kpoudqfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2y4EFAwchR4",
        "outputId": "8157b806-095d-439c-b586-afa5a3057731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=c527dd7cc52d3154f7065f26a608359f1865b46fc67d3f458596103573421ee5\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1 pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark ipython-autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "java --version\n",
        "pyspark --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-r2C6r2dywH",
        "outputId": "5c7de2f2-f60d-4acb-f1fc-1ed4c4f75d74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.22 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.22\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "QioWHKdLeKzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import math\n",
        "import statistics\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBlLSh8Ud_r4",
        "outputId": "6dbe7ac5-0955-4ac7-8972-637cf70748cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 431 µs (started: 2024-04-12 23:50:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "i_w3C5-aeNRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DSCI553/hw5\")\n",
        "\n",
        "\n",
        "class Path:\n",
        "    current_dir = os.getcwd()\n",
        "    data_dir = os.path.join(current_dir, \"data\")\n",
        "    input_csv_file = os.path.join(data_dir, \"users.txt\")\n",
        "\n",
        "    output_dir = os.path.join(current_dir, \"output\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    task1_output = os.path.join(output_dir, \"task1_op.txt\")\n",
        "    task2_output = os.path.join(output_dir, \"task2_op.txt\")\n",
        "    task3_output = os.path.join(output_dir, \"task3_op.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xle1WnnWeMmL",
        "outputId": "86ee4591-e991-4f8c-cb54-8fb4003f9546"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 716 ms (started: 2024-04-12 23:50:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "tKi22UTkewG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import binascii\n",
        "import csv\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from blackbox import BlackBox\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "FILTER_ARRAY_LENGTH = 69997\n",
        "ENCODING = \"utf-8\"\n",
        "NUM_HASHES = 50\n",
        "PRIME_NUMBER = 1e9 + 7\n",
        "\n",
        "\n",
        "def generate_hash_function_params(max_range, count):\n",
        "    \"\"\"Generate random hash function parameters within a specified range.\"\"\"\n",
        "    a = random.sample(range(1, max_range), count)  # Random coefficient 'a'\n",
        "    b = random.sample(range(1, max_range), count)  # Random intercept 'b'\n",
        "    return list(zip(a, b))\n",
        "\n",
        "\n",
        "def hash_user(user, params):\n",
        "    \"\"\"Hash an item using given hash function parameters.\n",
        "    Calculate hash value using the formula: ((a * item + b) % PRIME_NUMBER) % num_bins\n",
        "    \"\"\"\n",
        "    user = int(binascii.hexlify(user.encode(\"utf8\")), 16)\n",
        "    hash_val = ((params[0] * user + params[1]) % PRIME_NUMBER) % FILTER_ARRAY_LENGTH\n",
        "    return hash_val\n",
        "\n",
        "\n",
        "def myhashs(user):\n",
        "    hash_funcs = generate_hash_function_params(FILTER_ARRAY_LENGTH, NUM_HASHES)\n",
        "    return [hash_user(user, hash_funcs[i]) for i in range(NUM_HASHES)]\n",
        "\n",
        "\n",
        "def bloom_filter(input_path: str, blackbox: BlackBox, num_of_asks: int, stream_size: int):\n",
        "    results = []\n",
        "    exist_user = set()\n",
        "    exist_hash = []\n",
        "\n",
        "    # Fetch stream and perform Bloom Filtering for each batch\n",
        "    for i in range(num_of_asks):\n",
        "        stream_users = blackbox.ask(input_path, stream_size)\n",
        "        false_positives = 0\n",
        "        for user in stream_users:\n",
        "            usr_hashes = myhashs(user)\n",
        "\n",
        "            if usr_hashes in exist_hash and user not in exist_user:\n",
        "                false_positives += 1\n",
        "\n",
        "            exist_hash.append(usr_hashes)\n",
        "            exist_user.add(user)\n",
        "\n",
        "        results.append([i, false_positives / stream_size])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_output(output_file_name, results):\n",
        "    header = [\"Time\", \"FPR\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(results)\n",
        "\n",
        "\n",
        "def task1(input_path: str, stream_size: int, num_of_asks: int, output_path: str):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 1: Bloom Filter\").setMaster(\"local[*]\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize BlackBox\n",
        "        blackbox = BlackBox()\n",
        "\n",
        "        # Apply bloom filter on stream of users\n",
        "        results = bloom_filter(input_path, blackbox, num_of_asks, stream_size)\n",
        "\n",
        "        # Write results to output file\n",
        "        save_output(output_path, results)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Check if correct number of command-line arguments are provided\n",
        "#     if len(sys.argv) != 5:\n",
        "#         print(\"Usage: python task1.py <input_filename> <stream_size> <num_of_asks> <output_filename>\")\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Parse command-line arguments\n",
        "#     input_path = sys.argv[1]\n",
        "#     stream_size = int(sys.argv[2])\n",
        "#     num_of_asks = int(sys.argv[3])\n",
        "#     output_path = sys.argv[4]\n",
        "\n",
        "#     # Call task1 function\n",
        "#     task1(input_path, stream_size, num_of_asks, output_path)\n",
        "\n",
        "task1(Path.input_csv_file, 100, 30, Path.task1_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq0KJj8QezpL",
        "outputId": "ba8f8910-bf63-4ebf-a2af-68719efff77e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 16.860059022903442\n",
            "\n",
            "time: 29.1 s (started: 2024-04-12 23:50:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "CLEcAF5ve20o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import binascii\n",
        "import csv\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from blackbox import BlackBox\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "FILTER_ARRAY_LENGTH = 997\n",
        "ENCODING = \"utf-8\"\n",
        "NUM_HASHES = 50\n",
        "PRIME_NUMBER = 1e9 + 7\n",
        "\n",
        "\n",
        "def generate_hash_function_params(max_range, count):\n",
        "    \"\"\"Generate random hash function parameters within a specified range.\"\"\"\n",
        "    a = random.sample(range(1, max_range), count)  # Random coefficient 'a'\n",
        "    b = random.sample(range(1, max_range), count)  # Random intercept 'b'\n",
        "    return list(zip(a, b))\n",
        "\n",
        "\n",
        "def hash_user(user, params):\n",
        "    \"\"\"Hash an item using given hash function parameters.\n",
        "    Calculate hash value using the formula: ((a * item + b) % PRIME_NUMBER) % num_bins\n",
        "    \"\"\"\n",
        "    user = int(binascii.hexlify(user.encode(\"utf8\")), 16)\n",
        "    hash_val = ((params[0] * user + params[1]) % PRIME_NUMBER) % FILTER_ARRAY_LENGTH\n",
        "    return hash_val\n",
        "\n",
        "\n",
        "def myhashs(user):\n",
        "    hash_funcs = generate_hash_function_params(FILTER_ARRAY_LENGTH, NUM_HASHES)\n",
        "    return [hash_user(user, hash_funcs[i]) for i in range(NUM_HASHES)]\n",
        "\n",
        "\n",
        "def calculate_ground_truth(stream_users, ground_truth):\n",
        "    for user in stream_users:\n",
        "        ground_truth.add(user)\n",
        "\n",
        "    return len(ground_truth)\n",
        "\n",
        "\n",
        "def calculate_estimation(usr_hash):\n",
        "    sum_estimate = 0\n",
        "\n",
        "    for h in range(NUM_HASHES):\n",
        "        temp = [int(value[h]) for value in usr_hash.values()]\n",
        "\n",
        "        max_t_zero = 0\n",
        "        for value in temp:\n",
        "            tmp_str = bin(value)[2:]\n",
        "            wo_zero = tmp_str.rstrip('0')\n",
        "            if max_t_zero < len(tmp_str) - len(wo_zero):\n",
        "                max_t_zero = len(tmp_str) - len(wo_zero)\n",
        "\n",
        "        sum_estimate += 2 ** max_t_zero\n",
        "    return sum_estimate // NUM_HASHES\n",
        "\n",
        "\n",
        "def flajolet_martin(input_path: str, blackbox: BlackBox, num_of_asks: int, stream_size: int):\n",
        "    results = []\n",
        "\n",
        "    # Fetch stream and perform Bloom Filtering for each batch\n",
        "    for i in range(num_of_asks):\n",
        "        ground_truth = set()\n",
        "        usr_hash = dict()\n",
        "\n",
        "        stream_usrs = blackbox.ask(input_path, stream_size)\n",
        "        len_ground_truth = calculate_ground_truth(stream_usrs, ground_truth)\n",
        "\n",
        "        for user in stream_usrs:\n",
        "            usr_hash[user] = myhashs(user)\n",
        "\n",
        "        estimate = calculate_estimation(usr_hash)\n",
        "        results.append([i, len_ground_truth, estimate])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_output(output_file_name, results):\n",
        "    header = [\"Time\", \"Ground Truth\", \"Estimation\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(results)\n",
        "\n",
        "\n",
        "def task2(input_path: str, stream_size: int, num_of_asks: int, output_path: str):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize BlackBox\n",
        "        blackbox = BlackBox()\n",
        "\n",
        "        # Apply bloom filter on stream of users\n",
        "        results = flajolet_martin(input_path, blackbox, num_of_asks, stream_size)\n",
        "\n",
        "        # Write results to output file\n",
        "        save_output(output_path, results)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Check if correct number of command-line arguments are provided\n",
        "#     if len(sys.argv) != 5:\n",
        "#         print(\"Usage: python task2.py <input_filename> <stream_size> <num_of_asks> <output_filename>\")\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Parse command-line arguments\n",
        "#     input_path = sys.argv[1]\n",
        "#     stream_size = int(sys.argv[2])\n",
        "#     num_of_asks = int(sys.argv[3])\n",
        "#     output_path = sys.argv[4]\n",
        "\n",
        "#     # Call task1 function\n",
        "#     task2(input_path, stream_size, num_of_asks, output_path)\n",
        "\n",
        "task2(Path.input_csv_file, 300, 30, Path.task2_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbRmZIO8e20p",
        "outputId": "8ed18456-bd70-4954-d45b-08d8ef6498df"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 10.805005550384521\n",
            "\n",
            "time: 10.8 s (started: 2024-04-13 01:43:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "AdT_uhnRe2_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from blackbox import BlackBox\n",
        "\n",
        "def reservoir_sampling(input_path: str, blackbox: BlackBox, num_of_asks: int, stream_size: int):\n",
        "    results = []\n",
        "\n",
        "    users_found = []\n",
        "    seq_num = 0\n",
        "\n",
        "    for i in range(num_of_asks):\n",
        "        stream_usrs = blackbox.ask(input_path, stream_size)\n",
        "\n",
        "        for usr in stream_usrs:\n",
        "            seq_num += 1\n",
        "\n",
        "            # For the first 100 users, add them directly to the reservoir\n",
        "            if len(users_found) < 100:\n",
        "                users_found.append(usr)\n",
        "            elif random.random() < 100 / seq_num:\n",
        "                replace_index = random.randint(0, 99)\n",
        "                users_found[replace_index] = usr\n",
        "\n",
        "            # Output the current stage of the reservoir after every 100 users\n",
        "            if seq_num % 100 == 0:\n",
        "                results.append([seq_num] + users_found[::20])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_output(output_file_name, results):\n",
        "    header = [\"seqnum\", \"0_id\", \"20_id\", \"40_id\", \"60_id\", \"80_id\"]\n",
        "\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(results)\n",
        "\n",
        "\n",
        "def task3(input_path: str, stream_size: int, num_of_asks: int, output_path: str):\n",
        "    random.seed(553)\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize BlackBox\n",
        "        blackbox = BlackBox()\n",
        "\n",
        "        # Apply algorithm on stream of users\n",
        "        results = reservoir_sampling(input_path, blackbox, num_of_asks, stream_size)\n",
        "\n",
        "        # Write results to output file\n",
        "        save_output(output_path, results)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Check if correct number of command-line arguments are provided\n",
        "#     if len(sys.argv) != 5:\n",
        "#         print(\"Usage: python task3.py <input_filename> <stream_size> <num_of_asks> <output_filename>\")\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Parse command-line arguments\n",
        "#     input_path = sys.argv[1]\n",
        "#     stream_size = int(sys.argv[2])\n",
        "#     num_of_asks = int(sys.argv[3])\n",
        "#     output_path = sys.argv[4]\n",
        "\n",
        "#     # Call task1 function\n",
        "#     task3(input_path, stream_size, num_of_asks, output_path)\n",
        "\n",
        "task3(Path.input_csv_file, 100, 30, Path.task3_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIgAkoYDe2_R",
        "outputId": "b31debc7-43e8-4184-91b7-d19de44a1146"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 8.900791883468628\n",
            "\n",
            "time: 8.91 s (started: 2024-04-13 02:20:39 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "HxyEdjl8e32W"
      }
    }
  ]
}