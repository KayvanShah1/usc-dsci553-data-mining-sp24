{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1W97TXodSpBKOEESu3olsgSDMHcHYxmTr",
      "authorship_tag": "ABX9TyPWkra53rzD9mODSQbgiD1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-dsci553-data-mining-sp24/blob/main/assignment-5/notebooks/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Setup"
      ],
      "metadata": {
        "id": "IVI0kpoudqfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2y4EFAwchR4",
        "outputId": "309886ec-584e-4c37-cf90-cd9c0aa9ffd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark ipython-autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "java --version\n",
        "pyspark --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-r2C6r2dywH",
        "outputId": "bf957fa3-5952-46b5-9b9d-4fbe4c074d4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.22 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.22\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "QioWHKdLeKzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import math\n",
        "import statistics\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBlLSh8Ud_r4",
        "outputId": "fb44a466-0df6-4a6d-ce1d-7fb7fc8aadfb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 456 Âµs (started: 2024-04-12 05:43:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "i_w3C5-aeNRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DSCI553/hw5\")\n",
        "\n",
        "\n",
        "class Path:\n",
        "    current_dir = os.getcwd()\n",
        "    data_dir = os.path.join(current_dir, \"data\")\n",
        "    input_csv_file = os.path.join(data_dir, \"users.txt\")\n",
        "\n",
        "    output_dir = os.path.join(current_dir, \"output\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    task1_output = os.path.join(output_dir, \"task1_op.txt\")\n",
        "    task2_output = os.path.join(output_dir, \"task2_op.txt\")\n",
        "    task3_output = os.path.join(output_dir, \"task3_op.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xle1WnnWeMmL",
        "outputId": "778652e0-7675-4c9d-8b35-b48ce35c6b54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.08 s (started: 2024-04-12 05:43:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "tKi22UTkewG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import binascii\n",
        "import csv\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from blackbox import BlackBox\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "FILTER_ARRAY_LENGTH = 69997\n",
        "ENCODING = \"utf-8\"\n",
        "NUM_HASHES = 50\n",
        "PRIME_NUMBER = 1e9 + 7\n",
        "\n",
        "\n",
        "def generate_hash_function_params(max_range, count):\n",
        "    \"\"\"Generate random hash function parameters within a specified range.\"\"\"\n",
        "    a = random.sample(range(1, max_range), count)  # Random coefficient 'a'\n",
        "    b = random.sample(range(1, max_range), count)  # Random intercept 'b'\n",
        "    return list(zip(a, b))\n",
        "\n",
        "\n",
        "def hash_user(user, params):\n",
        "    \"\"\"Hash an item using given hash function parameters.\n",
        "    Calculate hash value using the formula: ((a * item + b) % PRIME_NUMBER) % num_bins\n",
        "    \"\"\"\n",
        "    user = int(binascii.hexlify(user.encode(\"utf8\")), 16)\n",
        "    hash_val = ((params[0] * user + params[1]) % PRIME_NUMBER) % FILTER_ARRAY_LENGTH\n",
        "    return hash_val\n",
        "\n",
        "\n",
        "def myhashs(user):\n",
        "    hash_funcs = generate_hash_function_params(FILTER_ARRAY_LENGTH, NUM_HASHES)\n",
        "    return [hash_user(user, hash_funcs[i]) for i in range(NUM_HASHES)]\n",
        "\n",
        "\n",
        "def bloom_filter(input_path: str, blackbox: BlackBox, num_of_asks: int, stream_size: int):\n",
        "    results = []\n",
        "    exist_user = set()\n",
        "    exist_hash = []\n",
        "\n",
        "    # Fetch stream and perform Bloom Filtering for each batch\n",
        "    for i in range(num_of_asks):\n",
        "        stream_users = blackbox.ask(input_path, stream_size)\n",
        "        false_positives = 0\n",
        "        for user in stream_users:\n",
        "            usr_hashes = myhashs(user)\n",
        "\n",
        "            if usr_hashes in exist_hash and user not in exist_user:\n",
        "                false_positives += 1\n",
        "\n",
        "            exist_hash.append(usr_hashes)\n",
        "            exist_user.add(user)\n",
        "\n",
        "        results.append([i, false_positives / stream_size])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_output(output_file_name, results):\n",
        "    header = [\"Time\", \"FPR\"]\n",
        "    with open(output_file_name, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(results)\n",
        "\n",
        "\n",
        "def task1(input_path: str, stream_size: int, num_of_asks: int, output_path: str):\n",
        "    # Initialize Spark\n",
        "    conf = SparkConf().setAppName(\"Task 1: Bloom Filter\").setMaster(\"local[*]\")\n",
        "    spark = SparkContext(conf=conf).getOrCreate()\n",
        "    spark.setLogLevel(\"ERROR\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize BlackBox\n",
        "        blackbox = BlackBox()\n",
        "\n",
        "        # Apply bloom filter on stream of users\n",
        "        results = bloom_filter(input_path, blackbox, num_of_asks, stream_size)\n",
        "\n",
        "        # Write results to output file\n",
        "        save_output(output_path, results)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Duration: {execution_time}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark\n",
        "        spark.stop()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Check if correct number of command-line arguments are provided\n",
        "#     if len(sys.argv) != 5:\n",
        "#         print(\"Usage: python task1.py <input_filename> <stream_size> <num_of_asks> <output_filename>\")\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # Parse command-line arguments\n",
        "#     input_path = sys.argv[1]\n",
        "#     stream_size = int(sys.argv[2])\n",
        "#     num_of_asks = int(sys.argv[3])\n",
        "#     output_path = sys.argv[4]\n",
        "\n",
        "#     # Call task1 function\n",
        "#     task1(input_path, stream_size, num_of_asks, output_path)\n",
        "\n",
        "task1(Path.input_csv_file, 100, 30, Path.task1_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq0KJj8QezpL",
        "outputId": "cc1d1c78-77f6-4f78-f4cf-27a82bc03604"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 9.474308013916016\n",
            "\n",
            "time: 10.6 s (started: 2024-04-12 06:44:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "CLEcAF5ve20o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbRmZIO8e20p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "AdT_uhnRe2_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIgAkoYDe2_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "HxyEdjl8e32W"
      }
    }
  ]
}